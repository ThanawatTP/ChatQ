{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from googletrans import Translator\n",
    "from tqdm import tqdm\n",
    "from dotenv import load_dotenv\n",
    "from pathlib import Path\n",
    "import json, os\n",
    "\n",
    "translator = Translator()\n",
    "folder_path = \"src/pointx\"\n",
    "change_type = { \"string\" : \"text\",\n",
    "                \"int\" : \"number\",\n",
    "                \"bigint\": \"number\",\n",
    "                \"decimal(27,2)\" : \"number\",\n",
    "                \"double\" : \"number\",\n",
    "                \"timestamp\" : \"text\",\n",
    "                \"date\" : \"text\"\n",
    "}\n",
    "schema_desc_path = os.path.join(folder_path,\"ETL Mapping & Data Dict - PointX (1).xlsx\")\n",
    "# dotenv_path = Path('.env')\n",
    "# load_dotenv(dotenv_path=dotenv_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing Tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pointx_keymatrix_dly\tTable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(schema_desc_path, sheet_name='14')\n",
    "df.columns = df.iloc[17,:]\n",
    "df = df.iloc[18:,:].reset_index(drop=True)\n",
    "df.columns.name = None\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_types = {}\n",
    "col_descs = {}\n",
    "table_name = df['Table'].unique().tolist()[0]\n",
    "table_desc = \"\"\"The Key Matrix Dashboard Design table provides a detailed overview of dashboard-related database columns, \n",
    "including data types, status indicators, descriptions, conditions, business logic, and sample data, \n",
    "enabling a comprehensive understanding of the data structure for effective dashboard design.\"\"\"\n",
    "\n",
    "for i, row in tqdm(df.iterrows()):\n",
    "    col_name = row['Column']\n",
    "    data_type = change_type[row['Data Type'].lower()]\n",
    "    desc = translator.translate(row['Description'], dest='en').text\n",
    "\n",
    "    col_types[col_name] = data_type\n",
    "    col_descs[col_name] = desc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema_desc = {\n",
    "    \"table\": table_name,\n",
    "    \"description\": table_desc,\n",
    "    \"columns\": col_descs\n",
    "}\n",
    "\n",
    "# with open(os.path.join(folder_path, \"pointx_keymatrix_dly_schema_description.json\"),'w') as f:\n",
    "#     json.dump(schema_desc, f, indent=4)\n",
    "\n",
    "# with open(os.path.join(folder_path, \"pointx_keymatrix_dly_columns_type.json\"),'w') as f:\n",
    "#     json.dump(col_types, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(os.path.join(folder_path, \"pointx_keymatrix_dly_schema_description.json\"),'r') as f:\n",
    "#     col_descs = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pointx_cust_mly Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"src/pointx/schema/pointx_cust_mly_type.json\") as f:\n",
    "    col_type = json.load(f)\n",
    "col_names = set(col_type.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(\"src/pointx/Business Glossary 1.xlsx\")\n",
    "df = df[['col_name', 'descriptions']]\n",
    "df = df[df.applymap(lambda x: isinstance(x, str) and x.strip() != '')].dropna()\n",
    "df['descriptions'] = df['descriptions'].apply(lambda desc : translator.translate(desc, dest='en').text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_name = \"pointx_cust_mly\"\n",
    "table_desc = \"\"\"The table provides a comprehensive monthly overview of customer engagement within the app, \n",
    "capturing data related to accumulated points, usage patterns, and relevant metrics, \n",
    "facilitating in-depth analysis of user behavior and app performance.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_descs = df.set_index('col_name')['descriptions'].to_dict()\n",
    "for col in col_descs:\n",
    "    if col not in col_descs:\n",
    "        del col_descs[col]\n",
    "\n",
    "schema_desc = {\n",
    "    \"table\": table_name,\n",
    "    \"description\": table_desc,\n",
    "    \"columns\": col_descs\n",
    "}\n",
    "with open(os.path.join(folder_path, \"schema/pointx_cust_mly_schema_description.json\"),'w') as f:\n",
    "    json.dump(schema_desc, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pointx_fbs_rpt_dly Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_name = \"pointx_fbs_rpt_dly\"\n",
    "table_desc = \"\"\"Table records user interactions with the PointX app daily, capturing events such as app opens and deletions, \n",
    "providing key insights into user behavior, app version usage, and device characteristics \"\"\"\n",
    "\n",
    "df = pd.read_csv(\"src/pointx/pointx_fbs_rpt_dly_description.csv\")\n",
    "col_descs = df.set_index('Column')['Description'].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema_desc = {\n",
    "    \"table\": table_name,\n",
    "    \"description\": table_desc,\n",
    "    \"columns\": col_descs\n",
    "}\n",
    "\n",
    "# with open(os.path.join(folder_path, \"pointx_fbs_rpt_dly_description.json\"),'w') as f:\n",
    "#     json.dump(schema_desc, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from transformers import AutoTokenizer\n",
    "# from sql_metadata import Parser\n",
    "\n",
    "model = SentenceTransformer('models/all-MiniLM-L6-v2')\n",
    "tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/all-MiniLM-L6-v2')\n",
    "lemmanizer = WordNetLemmatizer()\n",
    "\n",
    "table_desc_vectors = {}     # { table1: vector , ...}\n",
    "schema_desc_vectors = {}    # { table1: { column1: vector, ...}}\n",
    "schema_datatypes = {}       # { table1: { column1: datatype, ...}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_schema(schema_description_path:str, schema_datatype_path:str):\n",
    "    with open(schema_description_path) as jsonfile:\n",
    "        new_schema_description = json.load(jsonfile)\n",
    "    with open(schema_datatype_path) as jsonfile:\n",
    "        new_schema_datatype = json.load(jsonfile)\n",
    "    \n",
    "    table_name = new_schema_description['table']\n",
    "    table_vector = model.encode(new_schema_description['description'])\n",
    "    table_desc_vectors[table_name] = table_vector\n",
    "\n",
    "    schema_datatypes[table_name] = new_schema_datatype\n",
    "    column_vectors = {}\n",
    "    for col, desc in new_schema_description[\"columns\"].items():\n",
    "        column_vectors[col] = model.encode(desc)\n",
    "    schema_desc_vectors[table_name] = column_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "join_schema(\"src/pointx/schema/pointx_fbs_rpt_dly_schema_description.json\",\n",
    "            \"src/pointx/schema/pointx_fbs_rpt_dly_columns_type.json\")\n",
    "\n",
    "join_schema(\"src/pointx/schema/pointx_cust_mly_schema_description.json\",\n",
    "            \"src/pointx/schema/pointx_cust_mly_columns_type.json\")\n",
    "\n",
    "join_schema(\"src/pointx/schema/pointx_keymatrix_dly_schema_description.json\",\n",
    "            \"src/pointx/schema/pointx_keymatrix_dly_columns_type.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_schema(question:str, column_threshold:float = 0.4, table_threshold:float = 0.2, filter_tables:bool = True):\n",
    "    question_emb = model.encode(question)\n",
    "    used_schemas = {}\n",
    "    found_table = []\n",
    "\n",
    "    # string matching with table, coumn and question tokens\n",
    "    for token in question.split():\n",
    "        found_columns = []\n",
    "        if token in schema_desc_vectors.keys():\n",
    "            print(\"Table offset  ---->\", token)\n",
    "            found_table.append(token)\n",
    "        for table, column in schema_desc_vectors.items():\n",
    "            if token in column.keys(): \n",
    "                found_columns.append(token)\n",
    "                print(\"Column matching  --->\",token)\n",
    "    \n",
    "    if filter_tables:       #filter table before\n",
    "        used_tables = []\n",
    "        for table_name, table_vector in table_desc_vectors.items():\n",
    "            if util.cos_sim(table_vector, question_emb) >= table_threshold: \n",
    "                used_tables.append(table_name)\n",
    "    else: used_tables = list(table_desc_vectors.keys())     # filtering schema all columns\n",
    "\n",
    "    for table in used_tables:\n",
    "        if table in found_table: table_offset = 0.1\n",
    "        else: table_offset = 0\n",
    "        used_schemas[table] = []\n",
    "        for column, column_vector in schema_desc_vectors[table].items():\n",
    "            if (util.cos_sim(column_vector, question_emb) >= (column_threshold - table_offset)\n",
    "                or column in found_columns):\n",
    "                used_schemas[table].append(column)\n",
    "    \n",
    "    return used_schemas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column matching ---> event_date\n",
      "Table offset  ----> pointx_fbs_rpt_dly\n",
      "Table : pointx_fbs_rpt_dly\n",
      "Selected columns : ['event_date', 'event_month', 'event_bundle_sequence_id', 'event_timestamp', 'event_name', 'event_previous_timestamp', 'device_web_info_hostname', 'geo_region', 'event_id', 'transaction_status', 'transaction_type', '_date']\n",
      "Table : pointx_cust_mly\n",
      "Selected columns : ['date_from_last_financial', 'date_last_financial', 'days_from_last_financial']\n",
      "Table : pointx_keymatrix_dly\n",
      "Selected columns : ['mtd1_ncust_visit', 'mtd1_ncust_pointx_visit', 'mtd1_ncust_pointx_financial', 'mtd1_ncust_guest_visit', 'mtd1_amt_point_topup', 'mtd1_amt_point_transfer_out', 'mtd1_amt_point_pay', 'mtd1_amt_point_pay_sku', 'mtd1_amt_point_pay_pyw', 'mtd1_n_topup_point', 'mtd1_n_topup_point_onboard', 'mtd1_n_purchase', 'mtd1_n_point_payment_p_only', 'mtd1_amt_point_transfer_out_extnl', '_date']\n"
     ]
    }
   ],
   "source": [
    "question = \"Which event_date has largest transaction of pointx_fbs_rpt_dly table\"\n",
    "result = filter_schema(question, column_threshold=0.4, table_threshold=0.3,filter_tables=False)\n",
    "for table, columns in result.items():\n",
    "    print(f\"Table : {table}\")\n",
    "    print(f\"Selected columns : {columns}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
