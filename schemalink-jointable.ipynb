{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from googletrans import Translator\n",
    "from tqdm import tqdm\n",
    "from dotenv import load_dotenv\n",
    "from pathlib import Path\n",
    "import json, os\n",
    "\n",
    "translator = Translator()\n",
    "folder_path = \"src/pointx\"\n",
    "change_type = { \"string\" : \"text\",\n",
    "                \"int\" : \"number\",\n",
    "                \"bigint\": \"number\",\n",
    "                \"decimal(27,2)\" : \"number\",\n",
    "                \"double\" : \"number\",\n",
    "                \"timestamp\" : \"text\",\n",
    "                \"date\" : \"text\"\n",
    "}\n",
    "schema_desc_path = os.path.join(folder_path,\"ETL Mapping & Data Dict - PointX (1).xlsx\")\n",
    "# dotenv_path = Path('.env')\n",
    "# load_dotenv(dotenv_path=dotenv_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pointx_keymatrix_dly\tTable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(schema_desc_path, sheet_name='14')\n",
    "df.columns = df.iloc[17,:]\n",
    "df = df.iloc[18:,:].reset_index(drop=True)\n",
    "df.columns.name = None\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_types = {}\n",
    "col_descs = {}\n",
    "table_name = df['Table'].unique().tolist()[0]\n",
    "table_desc = \"\"\"The Key Matrix Dashboard Design table provides a detailed overview of dashboard-related database columns, \n",
    "including data types, status indicators, descriptions, conditions, business logic, and sample data, \n",
    "enabling a comprehensive understanding of the data structure for effective dashboard design.\"\"\"\n",
    "\n",
    "for i, row in tqdm(df.iterrows()):\n",
    "    col_name = row['Column']\n",
    "    data_type = change_type[row['Data Type'].lower()]\n",
    "    desc = translator.translate(row['Description'], dest='en').text\n",
    "\n",
    "    col_types[col_name] = data_type\n",
    "    col_descs[col_name] = desc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema_desc = {\n",
    "    \"table\": table_name,\n",
    "    \"description\": table_desc,\n",
    "    \"columns\": col_descs\n",
    "}\n",
    "\n",
    "# with open(os.path.join(folder_path, \"pointx_keymatrix_dly_schema_description.json\"),'w') as f:\n",
    "#     json.dump(schema_desc, f, indent=4)\n",
    "\n",
    "# with open(os.path.join(folder_path, \"pointx_keymatrix_dly_columns_type.json\"),'w') as f:\n",
    "#     json.dump(col_types, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(os.path.join(folder_path, \"pointx_keymatrix_dly_schema_description.json\"),'r') as f:\n",
    "#     col_descs = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pointx_cust_mly Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"src/pointx/pointx_cust_mly_datatype.csv\")\n",
    "col_types = df[['col_name', 'data_type']].set_index('col_name')['data_type'].to_dict()\n",
    "\n",
    "for c in col_types:\n",
    "    col_types[c] = change_type[col_types[c]]\n",
    "    \n",
    "with open(os.path.join(folder_path, \"pointx_cust_mly_type.json\"),'w') as f:\n",
    "    json.dump(col_types, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.rea"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pointx_fbs_rpt_dly Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_name = \"pointx_fbs_rpt_dly\"\n",
    "table_desc = \"\"\"Table records user interactions with the PointX app daily, capturing events such as app opens and deletions, \n",
    "providing key insights into user behavior, app version usage, and device characteristics \"\"\"\n",
    "\n",
    "df = pd.read_csv(\"src/pointx/pointx_fbs_rpt_dly_description.csv\")\n",
    "col_descs = df.set_index('Column')['Description'].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema_desc = {\n",
    "    \"table\": table_name,\n",
    "    \"description\": table_desc,\n",
    "    \"columns\": col_descs\n",
    "}\n",
    "\n",
    "# with open(os.path.join(folder_path, \"pointx_fbs_rpt_dly_description.json\"),'w') as f:\n",
    "#     json.dump(schema_desc, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json, sqlparse, re, nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sql_metadata import Parser\n",
    "\n",
    "src_folder = \"src/pointx\"\n",
    "schema_description_file = \"pointx_keymatrix_dly_schema_description.json\"\n",
    "with open(os.path.join(src_folder, schema_description_file)) as f:\n",
    "    db = json.load(f)\n",
    "model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "lemmanizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema_vector = []\n",
    "\n",
    "schema_emb = {}\n",
    "table_name = db['table']\n",
    "table_description = db['description']\n",
    "schema_emb[table_name] = model.encode(table_description).tolist()\n",
    "columns = list(db['columns'].keys())\n",
    "for col in columns:\n",
    "    column_description = db['columns'][col]\n",
    "    schema_emb[col] = model.encode(column_description).tolist()\n",
    "schema_vector.append(schema_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def column_from_question(question,used_table_col = {}, default_score=0.6):\n",
    "    # question_tokens = [token.lower() for token in tokenizer.tokenize(question)]\n",
    "    question_tokens = [lemmanizer.lemmatize(token.lower()) for token in question.split()]\n",
    "    # print(question_tokens)\n",
    "    for table in schema_vector:\n",
    "        max_score = default_score\n",
    "        for token in question_tokens:\n",
    "            cols = [ key.lower() for key in table.keys()]\n",
    "            table_name = cols.pop(0)\n",
    "            if token == table_name: \n",
    "                max_score = 1.0\n",
    "                # plus the score of columns in exact match with table name\n",
    "                if used_table_col.get(token) is not None: \n",
    "                    for key in used_table_col[token]: \n",
    "                        used_table_col[token].update({key: used_table_col[token][key] + 0.1})\n",
    "            # exact match table and column\n",
    "            if token in cols: \n",
    "                used_table_col.setdefault(table_name, {}).update({token : max_score})\n",
    "\n",
    "    return used_table_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_tables_by_description(question, column_threshold = 0.4, table_threshold = 0.2, filter_tables = True):\n",
    "    question_emb = model.encode(question)\n",
    "    used_schema = {}\n",
    "    for i in range(len(schema_vector)):\n",
    "        table_name = list(schema_vector[i].keys())[0]\n",
    "\n",
    "        table_description_vector = schema_vector[i][table_name]\n",
    "        if filter_tables and util.cos_sim(table_description_vector, question_emb) < table_threshold: continue\n",
    "        \n",
    "        used_col = {}\n",
    "        for col, vec in schema_vector[i].items():\n",
    "            if col == table_name: continue\n",
    "            score = round(float(util.cos_sim(vec, question_emb)),2)\n",
    "            if score > column_threshold:\n",
    "                # column_description = [dbs[i]['columns'][col] for i in range(len(dbs)) if dbs[i]['table'] == table_name][0]\n",
    "                # print(f\"{table_name} - {col} : {score}\\nDescription : {column_description}\\n\")\n",
    "                used_col.update({col: score})\n",
    "        if len(used_col) > 0: used_schema[table_name] = used_col\n",
    "    return used_schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"user open app\"\n",
    "selected_table_column = filter_tables_by_description(question, column_threshold = 0.3, filter_tables = False)\n",
    "selected_table_column = column_from_question(question, used_table_col=selected_table_column)\n",
    "selected_table_column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
