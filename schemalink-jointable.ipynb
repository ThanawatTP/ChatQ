{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from googletrans import Translator\n",
    "from tqdm import tqdm\n",
    "from dotenv import load_dotenv\n",
    "from pathlib import Path\n",
    "import json, os\n",
    "\n",
    "translator = Translator()\n",
    "folder_path = \"src/pointx\"\n",
    "change_type = { \"string\" : \"text\",\n",
    "                \"int\" : \"number\",\n",
    "                \"bigint\": \"number\",\n",
    "                \"decimal(27,2)\" : \"number\",\n",
    "                \"double\" : \"number\",\n",
    "                \"timestamp\" : \"text\",\n",
    "                \"date\" : \"text\"\n",
    "}\n",
    "schema_desc_path = os.path.join(folder_path,\"ETL Mapping & Data Dict - PointX (1).xlsx\")\n",
    "# dotenv_path = Path('.env')\n",
    "# load_dotenv(dotenv_path=dotenv_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing Tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pointx_keymatrix_dly\tTable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(schema_desc_path, sheet_name='14')\n",
    "df.columns = df.iloc[17,:]\n",
    "df = df.iloc[18:,:].reset_index(drop=True)\n",
    "df.columns.name = None\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_types = {}\n",
    "col_descs = {}\n",
    "table_name = df['Table'].unique().tolist()[0]\n",
    "table_desc = \"\"\"The Key Matrix Dashboard Design table provides a detailed overview of dashboard-related database columns, \n",
    "including data types, status indicators, descriptions, conditions, business logic, and sample data, \n",
    "enabling a comprehensive understanding of the data structure for effective dashboard design.\"\"\"\n",
    "\n",
    "for i, row in tqdm(df.iterrows()):\n",
    "    col_name = row['Column']\n",
    "    data_type = change_type[row['Data Type'].lower()]\n",
    "    desc = translator.translate(row['Description'], dest='en').text\n",
    "\n",
    "    col_types[col_name] = data_type\n",
    "    col_descs[col_name] = desc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema_desc = {\n",
    "    \"table\": table_name,\n",
    "    \"description\": table_desc,\n",
    "    \"columns\": col_descs\n",
    "}\n",
    "\n",
    "# with open(os.path.join(folder_path, \"pointx_keymatrix_dly_schema_description.json\"),'w') as f:\n",
    "#     json.dump(schema_desc, f, indent=4)\n",
    "\n",
    "# with open(os.path.join(folder_path, \"pointx_keymatrix_dly_columns_type.json\"),'w') as f:\n",
    "#     json.dump(col_types, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(os.path.join(folder_path, \"pointx_keymatrix_dly_schema_description.json\"),'r') as f:\n",
    "#     col_descs = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pointx_cust_mly Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"src/pointx/schema/pointx_cust_mly_type.json\") as f:\n",
    "    col_type = json.load(f)\n",
    "col_names = set(col_type.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(\"src/pointx/Business Glossary 1.xlsx\")\n",
    "df = df[['col_name', 'descriptions']]\n",
    "df = df[df.applymap(lambda x: isinstance(x, str) and x.strip() != '')].dropna()\n",
    "df['descriptions'] = df['descriptions'].apply(lambda desc : translator.translate(desc, dest='en').text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_name = \"pointx_cust_mly\"\n",
    "table_desc = \"\"\"The table provides a comprehensive monthly overview of customer engagement within the app, \n",
    "capturing data related to accumulated points, usage patterns, and relevant metrics, \n",
    "facilitating in-depth analysis of user behavior and app performance.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_descs = df.set_index('col_name')['descriptions'].to_dict()\n",
    "for col in col_descs:\n",
    "    if col not in col_descs:\n",
    "        del col_descs[col]\n",
    "\n",
    "schema_desc = {\n",
    "    \"table\": table_name,\n",
    "    \"description\": table_desc,\n",
    "    \"columns\": col_descs\n",
    "}\n",
    "with open(os.path.join(folder_path, \"schema/pointx_cust_mly_schema_description.json\"),'w') as f:\n",
    "    json.dump(schema_desc, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pointx_fbs_rpt_dly Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_name = \"pointx_fbs_rpt_dly\"\n",
    "table_desc = \"\"\"Table records user interactions with the PointX app daily, capturing events such as app opens and deletions, \n",
    "providing key insights into user behavior, app version usage, and device characteristics \"\"\"\n",
    "\n",
    "df = pd.read_csv(\"src/pointx/pointx_fbs_rpt_dly_description.csv\")\n",
    "col_descs = df.set_index('Column')['Description'].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema_desc = {\n",
    "    \"table\": table_name,\n",
    "    \"description\": table_desc,\n",
    "    \"columns\": col_descs\n",
    "}\n",
    "\n",
    "# with open(os.path.join(folder_path, \"pointx_fbs_rpt_dly_description.json\"),'w') as f:\n",
    "#     json.dump(schema_desc, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "sentence_emb_model = SentenceTransformer('models/all-MiniLM-L6-v2')\n",
    "\n",
    "table_desc_vectors = {}     # { table1: vector , ...}\n",
    "schema_desc_vectors = {}    # { table1: { column1: vector, ...}}\n",
    "schema_datatypes = {}       # { table1: { column1: datatype, ...}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_schema(schema_description_path:str, schema_datatype_path:str):\n",
    "    with open(schema_description_path) as jsonfile:\n",
    "        new_schema_description = json.load(jsonfile)\n",
    "    with open(schema_datatype_path) as jsonfile:\n",
    "        new_schema_datatype = json.load(jsonfile)\n",
    "    \n",
    "    table_name = new_schema_description['table']\n",
    "    table_vector = sentence_emb_model.encode(new_schema_description['description'])\n",
    "    table_desc_vectors[table_name] = table_vector\n",
    "\n",
    "    schema_datatypes[table_name] = new_schema_datatype\n",
    "    column_vectors = {}\n",
    "    for col, desc in new_schema_description[\"columns\"].items():\n",
    "        column_vectors[col] = sentence_emb_model.encode(desc)\n",
    "    schema_desc_vectors[table_name] = column_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_table(table_name):\n",
    "    del table_desc_vectors[table_name]\n",
    "    del schema_desc_vectors[table_name]\n",
    "    del schema_datatypes[table_name]\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_schema(question:str, column_threshold:float = 0.4, table_threshold:float = 0.2, \n",
    "                  max_select_columns:int = 5, filter_tables:bool = True):\n",
    "    question_emb = sentence_emb_model.encode(question)\n",
    "    used_schemas = {}\n",
    "    found_table = []\n",
    "\n",
    "    # string matching with table, coumn and question tokens\n",
    "    for token in question.split():\n",
    "        found_columns = []\n",
    "        if token in schema_desc_vectors.keys():\n",
    "            print(\"Table offset  ---->\", token)\n",
    "            found_table.append(token)\n",
    "        for table, column in schema_desc_vectors.items():\n",
    "            if token in column.keys(): \n",
    "                found_columns.append(token)\n",
    "                print(\"Column matching  --->\",token)\n",
    "    \n",
    "    if filter_tables:       #filter table before\n",
    "        used_tables = []\n",
    "        for table_name, table_vector in table_desc_vectors.items():\n",
    "            if util.cos_sim(table_vector, question_emb) >= table_threshold: \n",
    "                used_tables.append(table_name)\n",
    "    else: used_tables = list(table_desc_vectors.keys())     # filtering schema all columns\n",
    "\n",
    "    for table in used_tables:\n",
    "        if table in found_table: table_offset = 0.1         #offset score for selected column in this table\n",
    "        else: table_offset = 0\n",
    "        used_schemas[table] = {}\n",
    "        for column, column_vector in schema_desc_vectors[table].items():\n",
    "            sim_score = util.cos_sim(column_vector, question_emb)\n",
    "            if (sim_score >= (column_threshold - table_offset)\n",
    "                or column in found_columns):\n",
    "                used_schemas[table][column] = round(float(sim_score),3)\n",
    "        if len(used_schemas[table]) > max_select_columns:\n",
    "            # Select the top k largest values from the dictionary\n",
    "            used_schemas[table] = dict(sorted(used_schemas[table].items(), key=lambda item: item[1], reverse=True)[:max_select_columns])\n",
    "    \n",
    "    return used_schemas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_prompt(question, used_schema):\n",
    "    full_sql = \"\"\n",
    "    for table, columns in used_schema.items():\n",
    "        sql = f\"CREATE TABLE {table} (\"\n",
    "        for column in columns:\n",
    "            try:\n",
    "                sql += f' {column} {schema_datatypes[table][column]},'\n",
    "            except KeyError: pass\n",
    "        sql = sql[:-1] + \" )\\n\\n\"\n",
    "        full_sql += sql\n",
    "    promp = full_sql + \"-- Using valid SQLite, answer the following questions for the tables provided above.\"\n",
    "    promp = promp + '\\n' + '-- ' + question\n",
    "    promp = promp + '\\n' + \"SELECT\"\n",
    "\n",
    "    return promp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "join_schema(\"src/pointx/schema/pointx_fbs_rpt_dly_schema_description.json\",\n",
    "            \"src/pointx/schema/pointx_fbs_rpt_dly_columns_type.json\")\n",
    "\n",
    "join_schema(\"src/pointx/schema/pointx_cust_mly_schema_description.json\",\n",
    "            \"src/pointx/schema/pointx_cust_mly_columns_type.json\")\n",
    "\n",
    "join_schema(\"src/pointx/schema/pointx_keymatrix_dly_schema_description.json\",\n",
    "            \"src/pointx/schema/pointx_keymatrix_dly_columns_type.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_table(\"pointx_keymatrix_dly\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's ask question!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Which date has the most transactions\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table : pointx_fbs_rpt_dly\n",
      "Selected columns : {'_date': 0.6}\n",
      "Table : pointx_cust_mly\n",
      "Selected columns : {'date_from_last_financial': 0.678, 'date_last_financial': 0.678, 'days_from_last_financial': 0.577, 'date_from_last_purchase': 0.557, 'date_last_purchase': 0.557}\n"
     ]
    }
   ],
   "source": [
    "result = filter_schema(question, column_threshold=0.5, table_threshold=0.3,filter_tables=False)\n",
    "for table, columns in result.items():\n",
    "    print(f\"Table : {table}\")\n",
    "    print(f\"Selected columns : {columns}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = create_prompt(question, result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CREATE TABLE pointx_fbs_rpt_dly ( _date text )\n",
      "\n",
      "CREATE TABLE pointx_cust_mly ( date_from_last_financial text, days_from_last_financial number, date_from_last_purchase text )\n",
      "\n",
      "-- Using valid SQLite, answer the following questions for the tables provided above.\n",
      "-- Which date has the most transactions\n",
      "SELECT\n"
     ]
    }
   ],
   "source": [
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QUESTION : Which date has the most transactions\n",
      "SQL : SELECT date_from_last_purchase FROM pointx_cust_mly GROUP BY date_from_last_purchase ORDER BY COUNT(*) DESC LIMIT 1;\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"models/nsql-350M\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"models/nsql-350M\")\n",
    "\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "\n",
    "generated_ids = model.generate(input_ids, max_length=500)\n",
    "sql = tokenizer.decode(generated_ids[0], skip_special_tokens=True).split('\\n')[-1]\n",
    "print(\"QUESTION :\",question)\n",
    "print(\"SQL :\",sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
