{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from googletrans import Translator\n",
    "from tqdm import tqdm\n",
    "import json, os\n",
    "\n",
    "translator = Translator()\n",
    "folder_path = \"src/pointx\"\n",
    "change_type = { \"string\" : \"text\",\n",
    "                \"int\" : \"number\",\n",
    "                \"bigint\": \"number\",\n",
    "                \"decimal(27,2)\" : \"number\",\n",
    "                \"double\" : \"number\",\n",
    "                \"timestamp\" : \"text\",\n",
    "                \"date\" : \"text\"\n",
    "}\n",
    "schema_desc_path = os.path.join(folder_path,\"ETL Mapping & Data Dict - PointX (1).xlsx\")\n",
    "# dotenv_path = Path('.env')\n",
    "# load_dotenv(dotenv_path=dotenv_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pointx_keymatrix_dly\tTable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(schema_desc_path, sheet_name='14')\n",
    "df.columns = df.iloc[17,:]\n",
    "df = df.iloc[18:,:].reset_index(drop=True)\n",
    "df.columns.name = None\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_types = {}\n",
    "col_descs = {}\n",
    "table_name = df['Table'].unique().tolist()[0]\n",
    "table_desc = \"\"\"The Key Matrix Dashboard Design table provides a detailed overview of dashboard-related database columns, \n",
    "including data types, status indicators, descriptions, conditions, business logic, and sample data, \n",
    "enabling a comprehensive understanding of the data structure for effective dashboard design.\"\"\"\n",
    "\n",
    "for i, row in tqdm(df.iterrows()):\n",
    "    col_name = row['Column']\n",
    "    data_type = change_type[row['Data Type'].lower()]\n",
    "    desc = translator.translate(row['Description'], dest='en').text\n",
    "\n",
    "    col_types[col_name] = data_type\n",
    "    col_descs[col_name] = desc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema_desc = {\n",
    "    \"table\": table_name,\n",
    "    \"description\": table_desc,\n",
    "    \"columns\": col_descs\n",
    "}\n",
    "\n",
    "# with open(os.path.join(folder_path, \"pointx_keymatrix_dly_schema_description.json\"),'w') as f:\n",
    "#     json.dump(schema_desc, f, indent=4)\n",
    "\n",
    "# with open(os.path.join(folder_path, \"pointx_keymatrix_dly_columns_type.json\"),'w') as f:\n",
    "#     json.dump(col_types, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(os.path.join(folder_path, \"pointx_keymatrix_dly_schema_description.json\"),'r') as f:\n",
    "#     col_descs = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pointx_cust_mly Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"src/pointx/schema/pointx_cust_mly_type.json\") as f:\n",
    "    col_type = json.load(f)\n",
    "col_names = set(col_type.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(\"src/pointx/Business Glossary 1.xlsx\")\n",
    "df = df[['col_name', 'descriptions']]\n",
    "df = df[df.applymap(lambda x: isinstance(x, str) and x.strip() != '')].dropna()\n",
    "df['descriptions'] = df['descriptions'].apply(lambda desc : translator.translate(desc, dest='en').text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_name = \"pointx_cust_mly\"\n",
    "table_desc = \"\"\"The table provides a comprehensive monthly overview of customer engagement within the app, \n",
    "capturing data related to accumulated points, usage patterns, and relevant metrics, \n",
    "facilitating in-depth analysis of user behavior and app performance.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_descs = df.set_index('col_name')['descriptions'].to_dict()\n",
    "for col in col_descs:\n",
    "    if col not in col_descs:\n",
    "        del col_descs[col]\n",
    "\n",
    "schema_desc = {\n",
    "    \"table\": table_name,\n",
    "    \"description\": table_desc,\n",
    "    \"columns\": col_descs\n",
    "}\n",
    "with open(os.path.join(folder_path, \"schema/pointx_cust_mly_schema_description.json\"),'w') as f:\n",
    "    json.dump(schema_desc, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pointx_fbs_rpt_dly Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_name = \"pointx_fbs_rpt_dly\"\n",
    "table_desc = \"\"\"Table records user interactions with the PointX app daily, capturing events such as app opens and deletions, \n",
    "providing key insights into user behavior, app version usage, and device characteristics \"\"\"\n",
    "\n",
    "df = pd.read_csv(\"src/pointx/pointx_fbs_rpt_dly_description.csv\")\n",
    "col_descs = df.set_index('Column')['Description'].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema_desc = {\n",
    "    \"table\": table_name,\n",
    "    \"description\": table_desc,\n",
    "    \"columns\": col_descs\n",
    "}\n",
    "\n",
    "# with open(os.path.join(folder_path, \"pointx_fbs_rpt_dly_description.json\"),'w') as f:\n",
    "#     json.dump(schema_desc, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, warnings, time\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "sentence_emb_model = SentenceTransformer('models/all-MiniLM-L6-v2')\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"models/nsql-350M\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"models/nsql-350M\")\n",
    "\n",
    "table_desc_vectors = {}     # { table1: vector , ...}\n",
    "schema_desc_vectors = {}    # { table1: { column1: vector, ...}}\n",
    "schema_datatypes = {}       # { table1: { column1: datatype, ...}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_schema(schema_description_path:str, schema_datatype_path:str):\n",
    "    with open(schema_description_path) as jsonfile:\n",
    "        new_schema_description = json.load(jsonfile)\n",
    "    with open(schema_datatype_path) as jsonfile:\n",
    "        new_schema_datatype = json.load(jsonfile)\n",
    "    \n",
    "    table_name = new_schema_description['table']\n",
    "    table_vector = sentence_emb_model.encode(new_schema_description['description'])\n",
    "    table_desc_vectors[table_name] = table_vector\n",
    "\n",
    "    schema_datatypes[table_name] = new_schema_datatype\n",
    "    column_vectors = {}\n",
    "    for col, desc in new_schema_description[\"columns\"].items():\n",
    "        column_vectors[col] = sentence_emb_model.encode(desc)\n",
    "    schema_desc_vectors[table_name] = column_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_table(table_name):\n",
    "    del table_desc_vectors[table_name]\n",
    "    del schema_desc_vectors[table_name]\n",
    "    del schema_datatypes[table_name]\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_schema(question:str, column_threshold:float = 0.4, table_threshold:float = 0.2, \n",
    "                  max_select_columns:int = 5, filter_tables:bool = True):\n",
    "    question_emb = sentence_emb_model.encode(question)\n",
    "    used_schemas = {}\n",
    "    found_table = []\n",
    "\n",
    "    # string matching with table, coumn and question tokens\n",
    "    for token in question.split():\n",
    "        found_columns = []\n",
    "        if token in schema_desc_vectors.keys():\n",
    "            print(\"Table string match  ---->\", token)\n",
    "            found_table.append(token)\n",
    "        for table, column in schema_desc_vectors.items():\n",
    "            if token in column.keys(): \n",
    "                found_columns.append(token)\n",
    "                print(\"Column matching  --->\",token)\n",
    "    \n",
    "    if filter_tables:       #filter table before\n",
    "        used_tables = []\n",
    "        for table_name, table_vector in table_desc_vectors.items():\n",
    "            if util.cos_sim(table_vector, question_emb) >= table_threshold: \n",
    "                used_tables.append(table_name)\n",
    "    else: used_tables = list(table_desc_vectors.keys())     # filtering schema with all columns\n",
    "\n",
    "    for table in used_tables:\n",
    "        if table in found_table: table_offset = 0.1         # offset score for selected column in this table\n",
    "        else: table_offset = 0\n",
    "        used_schemas[table] = {}\n",
    "        for column, column_vector in schema_desc_vectors[table].items():\n",
    "            sim_score = util.cos_sim(column_vector, question_emb)\n",
    "            if (sim_score >= (column_threshold - table_offset)\n",
    "                or column in found_columns):\n",
    "                used_schemas[table][column] = round(float(sim_score),3)\n",
    "        if max_select_columns and len(used_schemas[table]) > max_select_columns:\n",
    "            # Select the top k largest values from the dictionary\n",
    "            used_schemas[table] = dict(sorted(used_schemas[table].items(), key=lambda item: item[1], reverse=True)[:max_select_columns])\n",
    "    \n",
    "    return used_schemas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_prompt(question, used_schema):\n",
    "    full_sql = \"\"\n",
    "    for table, columns in used_schema.items():\n",
    "        if not len(columns): continue       # pass this table when no column\n",
    "        primary_keys = schema_datatypes[table][\"JOIN_KEY\"][\"PK\"]\n",
    "        foreign_keys = list(schema_datatypes[table][\"JOIN_KEY\"][\"FK\"].keys())\n",
    "        join_table_key = primary_keys + foreign_keys\n",
    "        \n",
    "        sql = f\"CREATE TABLE {table} (\"\n",
    "        for column in columns:\n",
    "            if column in join_table_key and len(join_table_key): join_table_key.remove(column)\n",
    "            try:\n",
    "                sql += f' {column} {schema_datatypes[table][column]},'\n",
    "            except KeyError: \n",
    "                print(f\"KeyError :{column}\")\n",
    "                \n",
    "        if len(join_table_key): # key for join of table are not selected\n",
    "            for column in join_table_key:\n",
    "                sql += f' {column} {schema_datatypes[table][column]},'\n",
    "\n",
    "        # All table contain PK (maybe)\n",
    "        if len(primary_keys):\n",
    "            sql += 'PRIMARY KEY ('\n",
    "            for pk in primary_keys: sql += f'\"{pk}\" ,'\n",
    "            sql = sql[:-1] + \")\"\n",
    "        if len(foreign_keys):\n",
    "            for fk, ref_table in schema_datatypes[table][\"JOIN_KEY\"][\"FK\"].items():\n",
    "                sql += f', FOREIGN KEY (\"{fk}\") REFERENCES \"{ref_table}\" (\"{fk}\"),'\n",
    "\n",
    "        sql = sql[:-1] + \" )\\n\\n\"\n",
    "        full_sql += sql\n",
    "    promp = full_sql + \"-- Using valid SQLite, answer the following questions for the tables provided above.\"\n",
    "    promp = promp + '\\n' + '-- ' + question\n",
    "    promp = promp + '\\n' + \"SELECT\"\n",
    "\n",
    "    return promp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join_schema(\"src/pointx/schemas/pointx_fbs_rpt_dly_schema_description.json\",\n",
    "#             \"src/pointx/schemas/pointx_fbs_rpt_dly_columns_type.json\")\n",
    "\n",
    "# join_schema(\"src/pointx/schemas/pointx_cust_mly_schema_description.json\",\n",
    "#             \"src/pointx/schemas/pointx_cust_mly_columns_type.json\")\n",
    "\n",
    "# join_schema(\"src/pointx/schemas/pointx_keymatrix_dly_schema_description.json\",\n",
    "#             \"src/pointx/schemas/pointx_keymatrix_dly_columns_type.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove_table(\"pointx_keymatrix_dly\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spider dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_desc_vectors = {}     # { table1: vector , ...}\n",
    "schema_desc_vectors = {}    # { table1: { column1: vector, ...}}\n",
    "schema_datatypes = {}       # { table1: { column1: datatype, ...}}\n",
    "\n",
    "join_schema(\"src/spider/cofee_shop/happy_hour_desc.json\",\n",
    "            \"src/spider/cofee_shop/happy_hour_datatype.json\")\n",
    "\n",
    "join_schema(\"src/spider/cofee_shop/happy_hour_member_desc.json\",\n",
    "            \"src/spider/cofee_shop/happy_hour_member_datatype.json\")\n",
    "\n",
    "join_schema(\"src/spider/cofee_shop/member_desc.json\",\n",
    "            \"src/spider/cofee_shop/member_datatype.json\")\n",
    "\n",
    "join_schema(\"src/spider/cofee_shop/shop_desc.json\",\n",
    "            \"src/spider/cofee_shop/shop_datatype.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's ask question!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"shop address with happy hour in April\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# question = \"How many unique user use this app\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = filter_schema(question, column_threshold=0.3, table_threshold=0.2, filter_tables=False, max_select_columns=False)\n",
    "prompt = create_prompt(question, result)\n",
    "# for table, columns in result.items():\n",
    "#     print(f\"Table : {table}\")\n",
    "#     print(f\"Selected columns : {columns}\")\n",
    "print(prompt)\n",
    "start_time = time.time()\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "    generated_ids = model.generate(input_ids, max_length=1000)\n",
    "    sql = tokenizer.decode(generated_ids[0], skip_special_tokens=True).split('\\n')[-1]\n",
    "    \n",
    "    # print(\"QUESTION :\",question)\n",
    "    print()\n",
    "    print(\"SQL :\",sql)\n",
    "    print(f\"TAKE {time.time()-start_time} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "\n",
    "\n",
    "conn = sqlite3.connect('src/spider/cofee_shop/coffee_shop.sqlite')\n",
    "cursor = conn.cursor()\n",
    "cursor.execute(sql)\n",
    "results = cursor.fetchall()\n",
    "conn.close()\n",
    "\n",
    "print(sql)\n",
    "print()\n",
    "for row in results:\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
