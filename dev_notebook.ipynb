{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modify Schemalink"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## JSON Structure Description\n",
    "\n",
    "The JSON embedded_data object has the following structure:\n",
    "```json\n",
    "{\n",
    "  \"name\": \"domain name\",\n",
    "  \"tables\": {\n",
    "    \"table_1\" : {\n",
    "      \"description\" : {\n",
    "        \"text\" : \"this is a description of table_1\",\n",
    "        \"vector\" : [Vector]\n",
    "      },\n",
    "      \"datatypes\" : {\n",
    "        \"JOIN_KEY\" : {\n",
    "          \"PK\" : [\"column_1\", \"column_3\"],\n",
    "          \"FK\" : {\n",
    "            \"column_3\" : \"table_2\"\n",
    "            }, \n",
    "          },\n",
    "        \"COLUMNS\" : {\n",
    "          \"column_1\" : \"number\",\n",
    "          \"column_2\" : \"text\",\n",
    "          \"column_3\" : \"text\"\n",
    "        }\n",
    "      },\n",
    "      \"class_labels\" : {\n",
    "        \"class_label_1\" : {\n",
    "          \"text\" : \"This is a class description for identify column type\",\n",
    "          \"vector\" : [Vector],\n",
    "        },\n",
    "        \"class_label_2\" : {\n",
    "          \"text\" : \"This is a class description for identify column type\",\n",
    "          \"vector\" : [Vector],\n",
    "        }\n",
    "      },\n",
    "      \"columns\" : {\n",
    "        \"column_1\" : {\n",
    "          \"text\" : \"This is a description of column_1\",\n",
    "          \"vector\" : [Vector],\n",
    "          \"column_classes\" : [\"class_label_1\"]\n",
    "        },\n",
    "        \"column_2\" : {\n",
    "          \"text\" : \"This is a description of column_2\",\n",
    "          \"vector\" : [Vector],\n",
    "          \"column_classes\" : [\"class_label_2\"]\n",
    "        },\n",
    "        \"column_3\" : {\n",
    "          \"text\" : \"This is a description of column_3\",\n",
    "          \"vector\" : [Vector],\n",
    "          \"column_classes\" : [\"class_label_1\", \"class_label_2\"]\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "  },\n",
    "  \"table_2\" : {...}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, math, torch, os\n",
    "from torch import Tensor\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from openai import OpenAI           # openai version 1.11.1\n",
    "\n",
    "def cos_sim(a: Tensor, b: Tensor):\n",
    "    \"\"\"\n",
    "    Computes the cosine similarity cos_sim(a[i], b[j]) for all i and j.\n",
    "    :return: Matrix with res[i][j]  = cos_sim(a[i], b[j])\n",
    "    \"\"\"\n",
    "    if not isinstance(a, torch.Tensor):\n",
    "        a = torch.tensor(a)\n",
    "\n",
    "    if not isinstance(b, torch.Tensor):\n",
    "        b = torch.tensor(b)\n",
    "\n",
    "    if len(a.shape) == 1:\n",
    "        a = a.unsqueeze(0)\n",
    "\n",
    "    if len(b.shape) == 1:\n",
    "        b = b.unsqueeze(0)\n",
    "\n",
    "    a_norm = torch.nn.functional.normalize(a, p=2, dim=1)\n",
    "    b_norm = torch.nn.functional.normalize(b, p=2, dim=1)\n",
    "    return torch.mm(a_norm, b_norm.transpose(0, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import json\n",
    "# from api import encode\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import warnings\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"models/nsql-350M\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"models/nsql-350M\")\n",
    "sen_emb = SentenceTransformer(\"models/all-MiniLM-L6-v2\")\n",
    "\n",
    "def encode(text):\n",
    "    return sen_emb.encode(text).tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_relate_topic(text:str, table_classes:dict,topic_threshold_score:float=0.4) -> list:\n",
    "    text_vec = encode(text)\n",
    "    topic_scores = [float(cos_sim(info['vector'], text_vec)) for info in table_classes.values()]\n",
    "    related_topic_indices = np.where(np.array(topic_scores) >= topic_threshold_score)[0]\n",
    "    \n",
    "    # If no topics meet the threshold, return the topic with maximum score\n",
    "    if not len(related_topic_indices) : \n",
    "        related_topic_indices = [np.argmax(topic_scores)]\n",
    "\n",
    "    related_topics = [list(table_classes.keys())[i] for i in related_topic_indices]\n",
    "    return related_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_embed_domain(domain_name):\n",
    "  schema_descriptions_files_path = f\"{domain_name}/descriptions\"\n",
    "  schema_datatypes_files_path = f\"{domain_name}/datatypes\"\n",
    "  \n",
    "  domain = {}\n",
    "  domain['name'] = domain_name\n",
    "  domain['tables'] = {}\n",
    "\n",
    "\n",
    "  #dev\n",
    "  schema_classes_file_path = [file for file in os.listdir(domain_name) if file.endswith(\"_classes.json\")][0]\n",
    "  with open(os.path.join(domain_name, schema_classes_file_path), \"r\") as file:\n",
    "    schema_classes = json.load(file)\n",
    "\n",
    "  schema_classes_vector = dict()\n",
    "  for table_name, table_classes in schema_classes.items():\n",
    "    schema_classes_vector[table_name] = dict()\n",
    "    for class_label, class_description in table_classes.items():\n",
    "       schema_classes_vector[table_name][class_label] = { \"text\" : class_description,\n",
    "                                                          \"vector\" : encode(class_description) }\n",
    "\n",
    "  description_files = sorted(os.listdir(schema_descriptions_files_path))\n",
    "  datatype_files = sorted(os.listdir(schema_datatypes_files_path))\n",
    "  for description, datatype in zip(description_files, \n",
    "                                   datatype_files):\n",
    "\n",
    "    with open(os.path.join(schema_descriptions_files_path, description), 'r') as file:\n",
    "      description_body = json.load(file)\n",
    "    \n",
    "    with open(os.path.join(schema_datatypes_files_path, datatype), 'r') as file:\n",
    "      datatype_body = json.load(file)\n",
    "\n",
    "    # dev\n",
    "      \n",
    "    table_name = description_body['table']\n",
    "    column_classes = schema_classes_vector[table_name]\n",
    "    table_description = {}\n",
    "\n",
    "    table_description['text'] = description_body['description']\n",
    "    table_description['vector']= encode(description_body['description'])\n",
    "\n",
    "    columns = {}\n",
    "    for col, desc in description_body['columns'].items():\n",
    "      column = {}\n",
    "      column['text'] = desc\n",
    "      column['vector'] = encode(desc)\n",
    "      column['column_classes'] = most_relate_topic(desc, column_classes)\n",
    "      columns[col] = column\n",
    "\n",
    "    table = {}\n",
    "    table['description'] = table_description\n",
    "    table['datatypes'] = datatype_body\n",
    "    table['class_labels'] = column_classes\n",
    "    table['columns'] = columns\n",
    "\n",
    "    domain['tables'][table_name] = table\n",
    "\n",
    "  return domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "domain = 'coffee_shop'\n",
    "\n",
    "with open(f'src/src_dev/{domain}/embedded_data.json', 'w') as file:\n",
    "    json.dump(new_embed_domain(f'src/src_dev/{domain}'), file, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess to dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['name', 'tables'])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "domain = 'coffee_shop'\n",
    "\n",
    "with open(f\"src/src_dev/{domain}/embedded_data.json\") as file:\n",
    "    embedded_file = json.load(file)\n",
    "\n",
    "embedded_file.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name\n",
      "tables\n",
      "    happy_hour\n",
      "        description\n",
      "            text\n",
      "            vector\n",
      "        datatypes\n",
      "            JOIN_KEY\n",
      "                PK\n",
      "                FK\n",
      "                    Shop_ID\n",
      "                        shop\n",
      "            COLUMNS\n",
      "                HH_ID\n",
      "                Shop_ID\n",
      "                Month\n",
      "                Num_of_staff_in_charge\n",
      "        class_labels\n",
      "            EventDetails\n",
      "                text\n",
      "                vector\n",
      "            StaffMetrics\n",
      "                text\n",
      "                vector\n",
      "        columns\n",
      "            HH_ID\n",
      "                text\n",
      "                vector\n",
      "                column_classes\n",
      "            Shop_ID\n",
      "                text\n",
      "                vector\n",
      "                column_classes\n",
      "            Month\n",
      "                text\n",
      "                vector\n",
      "                column_classes\n",
      "            Num_of_staff_in_charge\n",
      "                text\n",
      "                vector\n",
      "                column_classes\n",
      "    happy_hour_member\n",
      "        description\n",
      "            text\n",
      "            vector\n",
      "        datatypes\n",
      "            JOIN_KEY\n",
      "                PK\n",
      "                FK\n",
      "                    Member_ID\n",
      "                        member\n",
      "            COLUMNS\n",
      "                HH_ID\n",
      "                Member_ID\n",
      "                Total_amount\n",
      "        class_labels\n",
      "            ParticipationDetails\n",
      "                text\n",
      "                vector\n",
      "            SpendingMetrics\n",
      "                text\n",
      "                vector\n",
      "        columns\n",
      "            HH_ID\n",
      "                text\n",
      "                vector\n",
      "                column_classes\n",
      "            Member_ID\n",
      "                text\n",
      "                vector\n",
      "                column_classes\n",
      "            Total_amount\n",
      "                text\n",
      "                vector\n",
      "                column_classes\n",
      "    member\n",
      "        description\n",
      "            text\n",
      "            vector\n",
      "        datatypes\n",
      "            JOIN_KEY\n",
      "                PK\n",
      "                FK\n",
      "            COLUMNS\n",
      "                Member_ID\n",
      "                Name\n",
      "                Membership_card\n",
      "                Age\n",
      "                Time_of_purchase\n",
      "                Level_of_membership\n",
      "                Address\n",
      "        class_labels\n",
      "            MemberDetails\n",
      "                text\n",
      "                vector\n",
      "        columns\n",
      "            Member_ID\n",
      "                text\n",
      "                vector\n",
      "                column_classes\n",
      "            Name\n",
      "                text\n",
      "                vector\n",
      "                column_classes\n",
      "            Membership_card\n",
      "                text\n",
      "                vector\n",
      "                column_classes\n",
      "            Age\n",
      "                text\n",
      "                vector\n",
      "                column_classes\n",
      "            Time_of_purchase\n",
      "                text\n",
      "                vector\n",
      "                column_classes\n",
      "            Level_of_membership\n",
      "                text\n",
      "                vector\n",
      "                column_classes\n",
      "            Address\n",
      "                text\n",
      "                vector\n",
      "                column_classes\n",
      "    shop\n",
      "        description\n",
      "            text\n",
      "            vector\n",
      "        datatypes\n",
      "            JOIN_KEY\n",
      "                PK\n",
      "                FK\n",
      "            COLUMNS\n",
      "                Shop_ID\n",
      "                Address\n",
      "                Num_of_staff\n",
      "                Score\n",
      "                Open_Year\n",
      "        class_labels\n",
      "            ShopDetails\n",
      "                text\n",
      "                vector\n",
      "        columns\n",
      "            Shop_ID\n",
      "                text\n",
      "                vector\n",
      "                column_classes\n",
      "            Address\n",
      "                text\n",
      "                vector\n",
      "                column_classes\n",
      "            Num_of_staff\n",
      "                text\n",
      "                vector\n",
      "                column_classes\n",
      "            Score\n",
      "                text\n",
      "                vector\n",
      "                column_classes\n",
      "            Open_Year\n",
      "                text\n",
      "                vector\n",
      "                column_classes\n"
     ]
    }
   ],
   "source": [
    "def print_json_structure(data, indent=0):\n",
    "    if isinstance(data, dict):\n",
    "        for key, value in data.items():\n",
    "            print('  ' * indent + str(key))\n",
    "            print_json_structure(value, indent + 2)\n",
    "    elif isinstance(data, list):\n",
    "        for item in data:\n",
    "            print_json_structure(item, indent)\n",
    "\n",
    "\n",
    "print_json_structure(embedded_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def selected_columns(question, specific_tables:list, column_info_df, schema_classes, max_n=20) -> dict:\n",
    "\n",
    "    _df = column_info_df[['Table', 'Column', 'Vector', 'Class_name']]\n",
    "    _df = _df[_df['Table'].isin(specific_tables)]\n",
    "    question_vector = sen_emb.encode(question)\n",
    "    _df['Score'] = _df['Vector'].apply(lambda x: float(cos_sim(x, question_vector)))\n",
    "\n",
    "    # {'event_detail': 6, 'member_information': 2, 'shop_information': 4}\n",
    "    \n",
    "    topic_selected = dict()\n",
    "    for table in specific_tables:\n",
    "        table_select = (max_n // len(specific_tables))\n",
    "        topic_selected.update(most_relate_topic(question, table, schema_classes, top_n=table_select, base_n=1))\n",
    "        \n",
    "    used_schema = {table : dict() for table in specific_tables}\n",
    "    used_cols = []\n",
    "\n",
    "    for topic, num in topic_selected.items():\n",
    "        selected_col_index = _df[_df['Class_name'] == topic]['Score'].sort_values(ascending=False).head(num).index\n",
    "        used_cols.extend(_df.loc[selected_col_index, 'Column'].to_list())\n",
    "\n",
    "    used_cols = list(set(used_cols))\n",
    "\n",
    "    for i, row in _df[_df['Column'].isin(used_cols)].iterrows():\n",
    "        used_schema[row['Table']][row['Column']] = round(row['Score'],3)\n",
    "\n",
    "    return used_schema\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_relate_topic(text:str, table:str, schema_classes,\n",
    "                      top_n:int=10, base_n:int=1):\n",
    "    text_vec = encode(text)\n",
    "    topic_scores = [float(cos_sim(info['vector'], text_vec)) for info in schema_classes[table].values()]\n",
    "\n",
    "    probs = (topic_scores / np.sum(topic_scores)) * top_n\n",
    "    topic_selected = { key: max(base_n, math.ceil(score)) for key, score in zip(schema_classes[table].keys(), probs)}\n",
    "    # {'event_detail': 6, 'member_information': 2, 'shop_information': 4}\n",
    "    return topic_selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Table</th>\n",
       "      <th>Column</th>\n",
       "      <th>Description</th>\n",
       "      <th>Vector</th>\n",
       "      <th>Class_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>happy_hour</td>\n",
       "      <td>HH_ID</td>\n",
       "      <td>Unique identifier for the happy hour event</td>\n",
       "      <td>[-0.030357593670487404, 0.11128713935613632, 0...</td>\n",
       "      <td>EventDetails</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>happy_hour</td>\n",
       "      <td>HH_ID</td>\n",
       "      <td>Unique identifier for the happy hour event</td>\n",
       "      <td>[-0.030357593670487404, 0.11128713935613632, 0...</td>\n",
       "      <td>StaffMetrics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>happy_hour</td>\n",
       "      <td>Shop_ID</td>\n",
       "      <td>Identifier of the shop hosting the happy hour</td>\n",
       "      <td>[-0.020781254395842552, 0.10354368388652802, -...</td>\n",
       "      <td>EventDetails</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>happy_hour</td>\n",
       "      <td>Shop_ID</td>\n",
       "      <td>Identifier of the shop hosting the happy hour</td>\n",
       "      <td>[-0.020781254395842552, 0.10354368388652802, -...</td>\n",
       "      <td>StaffMetrics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>happy_hour</td>\n",
       "      <td>Month</td>\n",
       "      <td>Month in which the happy hour takes place</td>\n",
       "      <td>[0.053571637719869614, 0.0968250185251236, -0....</td>\n",
       "      <td>EventDetails</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Table   Column                                    Description  \\\n",
       "0  happy_hour    HH_ID     Unique identifier for the happy hour event   \n",
       "0  happy_hour    HH_ID     Unique identifier for the happy hour event   \n",
       "1  happy_hour  Shop_ID  Identifier of the shop hosting the happy hour   \n",
       "1  happy_hour  Shop_ID  Identifier of the shop hosting the happy hour   \n",
       "2  happy_hour    Month      Month in which the happy hour takes place   \n",
       "\n",
       "                                              Vector    Class_name  \n",
       "0  [-0.030357593670487404, 0.11128713935613632, 0...  EventDetails  \n",
       "0  [-0.030357593670487404, 0.11128713935613632, 0...  StaffMetrics  \n",
       "1  [-0.020781254395842552, 0.10354368388652802, -...  EventDetails  \n",
       "1  [-0.020781254395842552, 0.10354368388652802, -...  StaffMetrics  \n",
       "2  [0.053571637719869614, 0.0968250185251236, -0....  EventDetails  "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_data = { 'Table' : [],\n",
    "            'Column' : [],\n",
    "            'Description' : [],\n",
    "            'Vector' : [],\n",
    "            'Class_name' : []}\n",
    "\n",
    "schema_classes = dict()\n",
    "\n",
    "for table, table_info in embedded_file['tables'].items():\n",
    "    schema_classes[table] = table_info['class_labels']\n",
    "    for col in table_info['columns']:\n",
    "        df_data['Table'].append(table)\n",
    "        df_data['Column'].append(col)\n",
    "        df_data['Description'].append(table_info['columns'][col]['text'])\n",
    "        df_data['Vector'].append(table_info['columns'][col]['vector'])\n",
    "        df_data['Class_name'].append(table_info['columns'][col]['column_classes'])\n",
    "\n",
    "df = pd.DataFrame(df_data).explode('Class_name')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "happy_hour\n",
      "    EventDetails\n",
      "        text\n",
      "        vector\n",
      "    StaffMetrics\n",
      "        text\n",
      "        vector\n",
      "happy_hour_member\n",
      "    ParticipationDetails\n",
      "        text\n",
      "        vector\n",
      "    SpendingMetrics\n",
      "        text\n",
      "        vector\n",
      "member\n",
      "    MemberDetails\n",
      "        text\n",
      "        vector\n",
      "shop\n",
      "    ShopDetails\n",
      "        text\n",
      "        vector\n"
     ]
    }
   ],
   "source": [
    "print_json_structure(schema_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'happy_hour': {'HH_ID': 0.494,\n",
       "  'Shop_ID': 0.734,\n",
       "  'Month': 0.547,\n",
       "  'Num_of_staff_in_charge': 0.704},\n",
       " 'happy_hour_member': {'HH_ID': 0.492,\n",
       "  'Member_ID': 0.544,\n",
       "  'Total_amount': 0.61},\n",
       " 'member': {'Member_ID': 0.137,\n",
       "  'Membership_card': 0.188,\n",
       "  'Age': 0.168,\n",
       "  'Time_of_purchase': 0.331,\n",
       "  'Level_of_membership': 0.18,\n",
       "  'Address': 0.093},\n",
       " 'shop': {'Shop_ID': 0.361,\n",
       "  'Address': 0.346,\n",
       "  'Num_of_staff': 0.532,\n",
       "  'Score': 0.42,\n",
       "  'Open_Year': 0.35}}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# coffee shop test\n",
    "\n",
    "question = \"How many shop has happy hour more than 2 hours and has maximum customer member\"\n",
    "tables = ['happy_hour', 'happy_hour_member', 'member', 'shop']\n",
    "selected_columns(question, tables, df, schema_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test after modify schema link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import json\n",
    "# from api import encode\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from openai import OpenAI\n",
    "import google.generativeai as genai\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"models/nsql-350M\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"models/nsql-350M\")\n",
    "sen_emb = SentenceTransformer(\"models/all-MiniLM-L6-v2\")\n",
    "\n",
    "def encode(text):\n",
    "    return sen_emb.encode(text).tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, math, torch\n",
    "from torch import Tensor\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def cos_sim(a: Tensor, b: Tensor):\n",
    "    \"\"\"\n",
    "    Computes the cosine similarity cos_sim(a[i], b[j]) for all i and j.\n",
    "    :return: Matrix with res[i][j]  = cos_sim(a[i], b[j])\n",
    "    \"\"\"\n",
    "    if not isinstance(a, torch.Tensor):\n",
    "        a = torch.tensor(a)\n",
    "\n",
    "    if not isinstance(b, torch.Tensor):\n",
    "        b = torch.tensor(b)\n",
    "\n",
    "    if len(a.shape) == 1:\n",
    "        a = a.unsqueeze(0)\n",
    "\n",
    "    if len(b.shape) == 1:\n",
    "        b = b.unsqueeze(0)\n",
    "\n",
    "    a_norm = torch.nn.functional.normalize(a, p=2, dim=1)\n",
    "    b_norm = torch.nn.functional.normalize(b, p=2, dim=1)\n",
    "    return torch.mm(a_norm, b_norm.transpose(0, 1))\n",
    "\n",
    "class SchemaLinking():\n",
    "\n",
    "    def __init__(self, domain):\n",
    "        \n",
    "        self.domain = domain\n",
    "        self.split_pattern = r'[\\s\\n;().]'\n",
    "        self.verbose = False\n",
    "        \n",
    "        df_data = { 'Table' : [],\n",
    "                    'Column' : [],\n",
    "                    'Description' : [],\n",
    "                    'Vector' : [],\n",
    "                    'Class_name' : []}\n",
    "\n",
    "        self.schema_classes = dict()\n",
    "        self.schema_datatypes = {}      # { table1: { column1: datatype, ...}}\n",
    "        self.table_descriptions = {}    # { table1: description, ...}\n",
    "        self.sql_condition = {'=', '>', '<', '>=', '<=', '<>', '!='}\n",
    "\n",
    "        # preparing object variable\n",
    "        for table, table_info in domain['tables'].items():\n",
    "            self.schema_classes[table] = table_info['class_labels']\n",
    "            self.schema_datatypes[table] = table_info['datatypes']\n",
    "            self.table_descriptions[table] = table_info['description']\n",
    "            for col in table_info['columns']:\n",
    "                df_data['Table'].append(table)\n",
    "                df_data['Column'].append(col)\n",
    "                df_data['Description'].append(table_info['columns'][col]['text'])\n",
    "                df_data['Vector'].append(table_info['columns'][col]['vector'])\n",
    "                df_data['Class_name'].append(table_info['columns'][col]['column_classes'])\n",
    "\n",
    "        self.column_info_df = pd.DataFrame(df_data).explode('Class_name')\n",
    "        self.schema_columns_lower = set(self.column_info_df['Column'].str.lower().values)\n",
    "        self.schema_tables_lower = set(self.column_info_df['Table'].str.lower().values)\n",
    "\n",
    "        \n",
    "    \n",
    "    \n",
    "    def most_relate_topic(self, text:str, table:str, top_n:int=10, base_n:int=1) -> dict:\n",
    "        \"\"\"\n",
    "        Determine the most related topics to the given text based on schema classes.\n",
    "\n",
    "        Parameters:\n",
    "        text (str): The sentence for which related topics need to be determined.\n",
    "        table (str): The table representing the schema classes against which the sentence will be compared.\n",
    "        top_n (int): The maximum number of selects for all topics to be selected based on relevance score. Defaults to 10.\n",
    "        base_n (int): The minimum number of each topic must be selected. Defaults to 1.\n",
    "\n",
    "        Returns:\n",
    "        dict: A dictionary where keys are topic names and values are the count of how many times each topic should be selected.\n",
    "\n",
    "        Example:\n",
    "        SchemaLinking.most_relate_topic(\"example text\", \"example_table\")\n",
    "        {'topic1': 3, 'topic2': 2, 'topic3': 1}\n",
    "        \"\"\"\n",
    "\n",
    "        text_vec = encode(text)\n",
    "        # apply cosin similarity score for each column followed by question\n",
    "        topic_scores = [float(cos_sim(info['vector'], text_vec)) for info in self.schema_classes[table].values()]\n",
    "        # select number of topics based on score probability\n",
    "        probs = (topic_scores / np.sum(topic_scores)) * top_n\n",
    "        topic_selected = { key: max(base_n, math.ceil(score)) for key, score in zip(self.schema_classes[table].keys(), probs)}\n",
    "        \n",
    "        return topic_selected\n",
    "    \n",
    "\n",
    "    def filter_schema(self, question:str, specific_tables:list, max_n:int=10) -> dict:\n",
    "        \"\"\"\n",
    "        Filter the schema to obtain only the columns of each specified table to be used for generating SQL based on a question.\n",
    "\n",
    "        Parameters:\n",
    "        question (str): The question for which the SQL schema needs to be filtered.\n",
    "        specific_tables (list): List of specific tables for which columns should be selected.\n",
    "        max_n (int): The maximum number of columns to select per table.\n",
    "\n",
    "        Returns:\n",
    "        dict: A dictionary containing selected columns for each table along with their relevance scores.\n",
    "\n",
    "        Example:\n",
    "        SchemaLinking.filter_schema(\"example question\", [\"table1\", \"table2\"])\n",
    "        {'table1': {'column1': 0.845, 'column2': 0.723}, 'table2': {'column3': 0.912, 'column4': 0.654}}\n",
    "        \"\"\"\n",
    "\n",
    "        # filtered ued column\n",
    "        _df = self.column_info_df[['Table', 'Column', 'Vector', 'Class_name']]\n",
    "        _df = _df[_df['Table'].isin(specific_tables)]\n",
    "\n",
    "        question_vector = encode(question)\n",
    "        # apply similarity score of each column followed by question\n",
    "        _df['Score'] = _df['Vector'].apply(lambda x: float(cos_sim(x, question_vector)))\n",
    "\n",
    "        # check string matching conditions\n",
    "        columns_match = []\n",
    "        for word in question.split():\n",
    "            if word.lower() in self.schema_columns_lower:\n",
    "                columns_match.append(word)\n",
    "            if word.lower() in self.schema_tables_lower and word not in specific_tables:\n",
    "                specific_tables.append(word)\n",
    "\n",
    "        # if columns match\n",
    "        if columns_match:\n",
    "            print(\"String matching\", columns_match)\n",
    "            _df.loc[_df['Column'].isin(columns_match), 'Score'] = 1.0\n",
    "        \n",
    "        # get the number for selecting each topic\n",
    "        topic_selected = dict()\n",
    "        for table in specific_tables:\n",
    "            table_select = (max_n // len(specific_tables))\n",
    "            topic_selected.update(self.most_relate_topic(question, table, top_n=table_select, base_n=1))\n",
    "        \n",
    "        # prepare used schema each table\n",
    "        used_schema = {table : dict() for table in specific_tables}\n",
    "        used_cols = []\n",
    "\n",
    "        # select the top column n number followed by the highest score.\n",
    "        for topic, num in topic_selected.items():\n",
    "            selected_col_index = _df[_df['Class_name'] == topic]['Score'].sort_values(ascending=False).head(num).index\n",
    "            used_cols.extend(_df.loc[selected_col_index, 'Column'].to_list())\n",
    "\n",
    "        used_cols = list(set(used_cols))\n",
    "\n",
    "        for i, row in _df[_df['Column'].isin(used_cols)].iterrows():\n",
    "            used_schema[row['Table']][row['Column']] = round(row['Score'],3)\n",
    "\n",
    "        # Primary keys and foreign keys are always selected when using more than one table\n",
    "        if len(specific_tables) > 1:\n",
    "            for table in specific_tables:\n",
    "                table_pk = self.schema_datatypes[table][\"JOIN_KEY\"][\"PK\"]\n",
    "                table_fk = self.schema_datatypes[table][\"JOIN_KEY\"][\"FK\"]\n",
    "                for fk, ref_table_column in table_fk.items():\n",
    "                    if list(ref_table_column.keys())[0] not in specific_tables: del table_fk[fk]\n",
    "                column_keys = table_pk + list(table_fk.keys())\n",
    "                for col in column_keys:\n",
    "                    if col not in used_schema[table].keys():\n",
    "                        used_schema[table][col] = 0.5\n",
    "\n",
    "        return used_schema\n",
    "\n",
    "    def table_col_of_sql(self, sql_query:str) -> dict:\n",
    "        \"\"\"\n",
    "        Extract tables and their corresponding columns from the given SQL query.\n",
    "\n",
    "        Parameters:\n",
    "        sql_query (str): The SQL query from which tables and columns need to be extracted.\n",
    "\n",
    "        Returns:\n",
    "        dict: A dictionary containing tables as keys and lists of columns as values.\n",
    "\n",
    "        Example:\n",
    "        SchemaLinking.table_col_of_sql(\"SELECT column1, column2 FROM table1 WHERE column3 = 'value'\")\n",
    "        {'table1': ['column1', 'column2', 'column3']}\n",
    "        \"\"\"\n",
    "        \n",
    "        selected_schema = {}\n",
    "        query_split = re.split(self.split_pattern, sql_query)\n",
    "        for table in self.schema_datatypes.keys():\n",
    "            if table in query_split:\n",
    "                selected_col = []\n",
    "                for col in self.schema_datatypes[table]['COLUMNS'].keys():\n",
    "                    if col in query_split: selected_col.append(col)\n",
    "                selected_schema[table] = selected_col\n",
    "\n",
    "        return selected_schema\n",
    "\n",
    "    def masking_query(self, sql_query:str, condition_value_mask:bool=True) -> str:\n",
    "        \"\"\"\n",
    "        Mask specified columns and optionally condition values in the given SQL query.\n",
    "\n",
    "        Parameters:\n",
    "        sql_query (str): The SQL query to be masked.\n",
    "        condition_value_mask (bool): Whether to mask condition values. Defaults to True.\n",
    "\n",
    "        Returns:\n",
    "        str: The masked SQL query.\n",
    "\n",
    "        Example:\n",
    "        SchemaLinking.masking_query(\"SELECT column1, column2 FROM table1 WHERE column3 = 'value'\")\n",
    "        SELECT [MASK], [MASK] FROM [MASK] WHERE [MASK] = [MASK]\n",
    "        \"\"\"\n",
    "\n",
    "        if '*' in sql_query: sql_query = sql_query.replace('*', \"[MASK]\")\n",
    "        query_split = re.split(r'(?<=[() .,;])|(?=[() .,;])', sql_query)\n",
    "        mask_next = False\n",
    "\n",
    "        for i in range(len(query_split)):\n",
    "            token = query_split[i].lower()\n",
    "            # prepare mask condition value\n",
    "            if token.lower() == 'where': mask_next = True\n",
    "            if condition_value_mask and mask_next and (token in self.sql_condition and i + 1 < len(query_split)):\n",
    "                step_mask_next = 1\n",
    "                # find the condition value\n",
    "                while query_split[i + step_mask_next] == ' ': step_mask_next += 1\n",
    "                query_split[i + step_mask_next] = \"[MASK]\"\n",
    "            \n",
    "            if token in self.schema_columns_lower or token in self.schema_tables_lower:\n",
    "                query_split[i] = \"[MASK]\"\n",
    "\n",
    "        return \"\".join(query_split)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "def generate_sql(prompt):\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "        generated_ids = model.generate(input_ids, max_length=1000)\n",
    "        sql = tokenizer.decode(generated_ids[0], skip_special_tokens=True).split('\\n')[-1]\n",
    "    return sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "OPENAI_API_KEY = \"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\"\n",
    "GOOGLE_API_KEY = \"AIzaSyCL1lMVUqwf0nQKtLPk30tv7VUXTiKE-fE\"\n",
    "\n",
    "llm_model_name = \"gpt-3.5-turbo\"\n",
    "\n",
    "llm_stop = ['\\n\\n']\n",
    "temperature = 0\n",
    "llm_prompt = \"\"\"You are a SQL query assistant.\n",
    "I have some SQL where the [MASK] columns, condition values and tables is syntaxed and I want you to respond to output that populates the [MASK] column of the SQL input followed by the question and schema description (name - description - data type).\n",
    "If you don't know which column to fill in Do not include columns that you have created yourself. And only columns defined from the schema must be used. \n",
    "Do not use columns from other tables or schema. must also be used from the same table defined in the input.\n",
    "If you must enter conditional values Please decide the format or value based on the sample values of that column.\n",
    "If that column has many too long category value please decide base on column description.\n",
    "\n",
    "For example:\n",
    "table :     cat - this table contain cat information \n",
    "columns :    id - number for identify cat | number\n",
    "            name - name of cat | text\n",
    "            age - age of cat | number\n",
    "            birth_date - pet birthday in format 'YYYY-MM-DD' | datetime\n",
    "            gender - gender of cat (male, female) | text\n",
    "\n",
    "question : Show me number of cat for each gender which born before March 23, 2011.\n",
    "input : SELECT [MASK], COUNT([MASK]) FROM [MASK] WHERE [MASK] < [MASK] GROUP BY [MASK] ;\n",
    "output : SELECT gender, COUNT(*) FROM cat WHERE birth_date < '2011-03-23' GROUP BY gender;\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_prompt(schema_link:object, question:str, used_schema:dict) -> str:\n",
    "    \"\"\"\n",
    "    Generate a prompt for applying into SQL generation model based on the question and schema.\n",
    "\n",
    "    Parameters:\n",
    "    schema_link (object): The instance of the class containing schema information.\n",
    "    question (str): The question for which the prompt is generated.\n",
    "    used_schema (dict): A dictionary containing tables as keys and lists of columns as values after filtering the schema.\n",
    "\n",
    "    Returns:\n",
    "    str: A prompt for applying into SQL generation model.\n",
    "\n",
    "    Example:\n",
    "    prompt = create_prompt(schema_instance, \"What are the total sales?\", \n",
    "                          { 'sales': {'date' : 0.3, 'amount' : 0.61}, \n",
    "                            'products': {'name' : 0.23, 'price' : 0.57}})\n",
    "    print(prompt)\n",
    "\n",
    "    CREATE TABLE sales ( date DATE, amount INT,PRIMARY KEY (\"date\") )\n",
    "    -- Using valid SQLite, answer the following questions for the tables provided above.\n",
    "    -- What are the total sales?\n",
    "    SELECT\n",
    "    \"\"\"\n",
    "    full_sql = \"\"\n",
    "    for table, columns in used_schema.items():\n",
    "        if not len(columns): continue       # pass this table when no column\n",
    "        primary_keys = schema_link.schema_datatypes[table][\"JOIN_KEY\"][\"PK\"]\n",
    "        foreign_keys = list(schema_link.schema_datatypes[table][\"JOIN_KEY\"][\"FK\"].keys())\n",
    "        join_table_key = primary_keys + foreign_keys\n",
    "        \n",
    "        sql = f\"CREATE TABLE {table} (\"\n",
    "        for column in columns:\n",
    "            if column in join_table_key and len(join_table_key): join_table_key.remove(column)\n",
    "            try:\n",
    "                sql += f' {column} {schema_link.schema_datatypes[table][\"COLUMNS\"][column]},'\n",
    "            except KeyError: \n",
    "                print(f\"KeyError :{column}\")\n",
    "                \n",
    "        if len(join_table_key): # key for join of table are remaining\n",
    "            for column in join_table_key:\n",
    "                sql += f' {column} {schema_link.schema_datatypes[table][\"COLUMNS\"][column]},'\n",
    "\n",
    "        # All table contain PK (maybe)\n",
    "        if len(primary_keys):\n",
    "            sql += 'PRIMARY KEY ('\n",
    "            for pk_type in primary_keys: sql += f'\"{pk_type}\" ,'\n",
    "            sql = sql[:-1] + \"),\"\n",
    "\n",
    "        if len(foreign_keys):\n",
    "            for fk, ref_table_column in schema_link.schema_datatypes[table][\"JOIN_KEY\"][\"FK\"].items():\n",
    "                sql += f' FOREIGN KEY (\"{fk}\") REFERENCES \"{list(ref_table_column.keys())[0]}\" (\"{list(ref_table_column.values())[0]}\"),'\n",
    "\n",
    "        sql = sql[:-1] + \" )\\n\\n\"\n",
    "        full_sql += sql\n",
    "    prompt = full_sql + \"-- Using valid SQLite, answer the following questions for the tables provided above.\"\n",
    "    prompt = prompt + '\\n' + '-- ' + question\n",
    "    prompt = prompt + '\\n' + \"SELECT\"\n",
    "\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LLM_fill_column(schema_link, question:str , used_schema:dict, masked_query:str, llm_model:str) -> str:\n",
    "        \"\"\"\n",
    "        Fill the [MASK] query to complete the SQL query using the Language Model.\n",
    "\n",
    "        Parameters:\n",
    "        question (str): The question related to the SQL query.\n",
    "        used_schema (dict): Dictionary containing the schema information.\n",
    "        masked_query (str): The SQL query with masked columns.\n",
    "\n",
    "        Returns:\n",
    "        str: The complete SQL query.\n",
    "\n",
    "        Example:\n",
    "        SchemaLinking.LLM_fill_column(schema_instance, \"Show all employees\", {\"employees\": {\"employee_id\" : 0.45, \"employee_name\" : 0.37}}, \"SELECT [MASK] FROM employees;\")\n",
    "        SELECT employee_id, employee_name FROM employees;\n",
    "        \"\"\"\n",
    "        \n",
    "        full_prompt = \"\"\n",
    "        for table_name, column_score in used_schema.items():\n",
    "            _df = schema_link.column_info_df[schema_link.column_info_df['Table'] == table_name][['Column', 'Description']].drop_duplicates()\n",
    "            full_prompt += f\"\\ntable : {table_name} - {schema_link.table_descriptions[table_name]['text']}\\ncolumns:\"\n",
    "            for column_name in column_score:\n",
    "                full_prompt += f\"\\t{column_name} - {_df[_df['Column'] == column_name]['Description'].values[0]}\"\n",
    "                full_prompt += f\" | {schema_link.schema_datatypes[table_name]['COLUMNS'][column_name]}\\n\"\n",
    "\n",
    "        full_prompt += f\"question : {question}\\n\"\n",
    "        full_prompt += f\"input : {masked_query}\\noutput :\"\n",
    "        full_prompt = llm_prompt + full_prompt\n",
    "\n",
    "        if llm_model in ['openai', 'gpt-3.5-turbo', 'gpt-4-turbo']: \n",
    "            try:\n",
    "                client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "                response = client.chat.completions.create(\n",
    "                    model=llm_model,\n",
    "                    messages=[\n",
    "                            {\"role\": \"system\",\n",
    "                                \"content\": \"I will give you some x-y examples followed by a x, you need to give me the y, and no other content.\"},\n",
    "                            {\"role\": \"user\", \n",
    "                                \"content\": full_prompt},\n",
    "                            ],\n",
    "                    stop=llm_stop,\n",
    "                    temperature=temperature\n",
    "                )\n",
    "                return response.choices[0].message.content\n",
    "            \n",
    "            except Exception as e:\n",
    "                return f\"OpenAI Error :{e}\"\n",
    "        elif llm_model in ['gemini-pro']:\n",
    "            try:\n",
    "                genai.configure(api_key=GOOGLE_API_KEY)\n",
    "                gemini_model = genai.GenerativeModel(llm_model)\n",
    "                response = gemini_model.generate_content(full_prompt)\n",
    "                return response.text\n",
    "            \n",
    "            except Exception as e:\n",
    "                 return f\"Google AI Error : {e}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_reason(schema_link:object, sql_result:str) -> str:\n",
    "    \"\"\"\n",
    "    Get the reason message related to the selected columns and tables from the schema based on the SQL query.\n",
    "\n",
    "    Parameters:\n",
    "    schema_link (object): The instance of the class containing schema information.\n",
    "    sql_result (str): The SQL query result for which the reason message is generated.\n",
    "\n",
    "    Returns:\n",
    "    str: The reason message explaining the selection of columns and tables from the schema.\n",
    "\n",
    "    Example:\n",
    "    get_reason(schema_instance, \"SELECT column1, column2 FROM table1 WHERE column3 = 'value'\")\n",
    "\n",
    "    Table - table1 : Description of table1\n",
    "        Column - column1 : Description of column1\n",
    "        Column - column2 : Description of column2\n",
    "        Column - column3 : Description of column3\n",
    "    \"\"\"\n",
    "\n",
    "    table_col_sql = schema_link.table_col_of_sql(sql_result)\n",
    "    reason = \"\"\n",
    "\n",
    "    for table, cols in table_col_sql.items():\n",
    "        _df = schema_link.column_info_df[schema_link.column_info_df['Table'] == table][['Column', 'Description']].drop_duplicates()\n",
    "        table_reason = f\"Table - {table}\\t: {schema_link.table_descriptions[table]['text']}\\n\"\n",
    "        if len(cols):       # have columns of table\n",
    "            col_reason = \"\\n\".join([f\"\\tColumn - {c}\\t: {_df.loc[_df['Column'] == c, 'Description'].values[0]}\" for c in cols])\n",
    "        else: col_reason = \"\"\n",
    "        reason += str(table_reason + col_reason + \"\\n\\n\")\n",
    "\n",
    "    return reason"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "String matching ['address']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'happy_hour': {'Shop_ID': 0.368,\n",
       "  'Num_of_staff_in_charge': 0.291,\n",
       "  'HH_ID': 0.5,\n",
       "  'Month': 0.5},\n",
       " 'happy_hour_member': {'Member_ID': 0.239,\n",
       "  'Total_amount': 0.334,\n",
       "  'HH_ID': 0.5},\n",
       " 'member': {'Member_ID': 0.269,\n",
       "  'Membership_card': 0.322,\n",
       "  'Time_of_purchase': 0.328,\n",
       "  'Address': 0.316},\n",
       " 'shop': {'Shop_ID': 0.514, 'Address': 0.562, 'Num_of_staff': 0.592}}"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"src/src_dev/coffee_shop/embedded_data.json\", \"r\") as f:\n",
    "    domain = json.load(f)\n",
    "    domain_tables = list(domain['tables'].keys())\n",
    "\n",
    "schema_link = SchemaLinking(domain)\n",
    "question = \"Show me the shop ID and address of shops with sum of member's total amout more than 300 \"\n",
    "used_schema = schema_link.filter_schema(question, domain_tables, max_n=10)\n",
    "used_schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "String matching ['id', 'id']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'pointx_fbs_rpt_dly': {'user_id': 0.301,\n",
       "  'user_ltv_revenue': 0.392,\n",
       "  'user_ltv_currency': 0.323,\n",
       "  'traffic_source_medium': 0.303,\n",
       "  'ecommerce': 0.307,\n",
       "  'ecoupon_rank': 0.293,\n",
       "  'id': 1.0,\n",
       "  'merchant_id': 0.346,\n",
       "  'order_id': 0.309,\n",
       "  'place_id': 0.147,\n",
       "  'product_id': 0.249,\n",
       "  'total_amount': 0.337,\n",
       "  'transaction_id': 0.441,\n",
       "  'transaction_status': 0.331}}"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"src/src_dev/pointx/embedded_data.json\", \"r\") as f:\n",
    "    domain = json.load(f)\n",
    "    domain_tables = list(domain['tables'].keys())\n",
    "    \n",
    "schema_link = SchemaLinking(domain)\n",
    "question = \"Display the user id and revenue of user who has the highest total transactions id\"\n",
    "used_schema = schema_link.filter_schema(question, ['pointx_fbs_rpt_dly',], max_n=10)\n",
    "used_schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========= PROMPT =========\n",
      "CREATE TABLE pointx_fbs_rpt_dly ( user_id number, user_ltv_revenue number, user_ltv_currency number, traffic_source_medium text, ecommerce number, ecoupon_rank number, id number, merchant_id number, order_id number, place_id number, product_id number, total_amount number, transaction_id number, transaction_status number, event_id number, _date text,PRIMARY KEY (\"event_id\" ), FOREIGN KEY (\"_date\") REFERENCES \"pointx_keymatrix_dly\" (\"_date\") )\n",
      "\n",
      "-- Using valid SQLite, answer the following questions for the tables provided above.\n",
      "-- Display the user id and revenue of user who has the highest total transactions id\n",
      "SELECT\n",
      "\n",
      "========= SQL =========\n",
      "SELECT user_id, MAX(total_amount) FROM pointx_fbs_rpt_dly GROUP BY user_id ORDER BY SUM(total_amount) DESC LIMIT 1;\n",
      "\n",
      "========= REASON =========\n",
      "Table - pointx_fbs_rpt_dly\t: Table records user interactions with the PointX app daily, capturing events such as app opens and deletions, \n",
      "providing key insights into user behavior, app version usage, and device characteristics \n",
      "\tColumn - user_id\t: The user ID set via the setUserId API.\n",
      "\tColumn - total_amount\t: Total amount\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = create_prompt(schema_link, question, used_schema)\n",
    "print(\"========= PROMPT =========\")\n",
    "print(prompt)\n",
    "print()\n",
    "sql_result = generate_sql(prompt)\n",
    "print(\"========= SQL =========\")\n",
    "print(sql_result)\n",
    "print()\n",
    "reason = get_reason(schema_link, sql_result)\n",
    "print(\"========= REASON =========\")\n",
    "print(reason)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'SELECT [MASK], MAX([MASK]) FROM [MASK] GROUP BY [MASK] ORDER BY SUM([MASK]) DESC LIMIT 1;'"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masked_query = schema_link.masking_query(sql_result)\n",
    "masked_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========= QUESTION =========\n",
      "Display the user id and revenue of user who has the highest total transactions id\n",
      "\n",
      "========= SQL =========\n",
      "SELECT user_id, user_ltv_revenue FROM pointx_fbs_rpt_dly GROUP BY user_id ORDER BY COUNT(transaction_id) DESC LIMIT 1;\n",
      "\n",
      "========= REASON =========\n",
      "Table - pointx_fbs_rpt_dly\t: Table records user interactions with the PointX app daily, capturing events such as app opens and deletions, \n",
      "providing key insights into user behavior, app version usage, and device characteristics \n",
      "\tColumn - user_id\t: The user ID set via the setUserId API.\n",
      "\tColumn - user_ltv_revenue\t: The Lifetime Value (revenue) of the user. This field is not populated in intraday tables.\n",
      "\tColumn - transaction_id\t: Transaction id\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"========= QUESTION =========\")\n",
    "print(question)\n",
    "print()\n",
    "final_result = LLM_fill_column(schema_link, question, used_schema, masked_query, 'gpt-3.5-turbo')\n",
    "print(\"========= SQL =========\")\n",
    "print(final_result)\n",
    "print()\n",
    "reason = get_reason(schema_link, final_result)\n",
    "print(\"========= REASON =========\")\n",
    "print(reason)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def table_selected(question:str,n_select:int , table_descriptions_vector:dict):\n",
    "    question_vector = encode(question)\n",
    "    table_scores = {table : round(float(cos_sim(table_vector, question_vector)),3) for table, table_vector in table_descriptions_vector.items()}\n",
    "    # sum_score = sum(table_scores.values())\n",
    "    # tab_select = {table: math.floor(n_select * score / sum_score) for table, score in table_scores.items()}\n",
    "    return table_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Show me the user id and total revenue of that id\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'pointx_cust_mly': 0.244,\n",
       " 'pointx_fbs_rpt_dly': 0.138,\n",
       " 'pointx_keymatrix_dly': 0.114}"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table_description_vec = dict()\n",
    "\n",
    "for table_name, table_info in domain['tables'].items():\n",
    "    table_description_vec[table_name] = table_info['description']['vector']\n",
    "\n",
    "print(question)\n",
    "table_selected(question, 10, table_description_vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert SQL schema to Data source schema format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3, os, json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['src/spider/database/browser_web/browser_web.sqlite',\n",
       " 'src/spider/database/musical/musical.sqlite',\n",
       " 'src/spider/database/farm/farm.sqlite',\n",
       " 'src/spider/database/voter_1/voter_1.sqlite',\n",
       " 'src/spider/database/game_injury/game_injury.sqlite']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "folder_path = \"src/spider/database\"\n",
    "select_db = ['musical',\n",
    "             'farm', \n",
    "             'hospital_1', \n",
    "             'tvshow', \n",
    "             'cinema', \n",
    "             'restaurants', \n",
    "             'company_employee', \n",
    "             'company_offic', \n",
    "             'singer', \n",
    "             'coffee_shop']\n",
    "\n",
    "db = []\n",
    "\n",
    "if os.path.exists(folder_path) and os.path.isdir(folder_path):\n",
    "    files = os.listdir(folder_path)\n",
    "    for file in files:\n",
    "        # if file in select_db:\n",
    "        db_path = os.path.join(folder_path, file)\n",
    "        sqlite_db = [os.path.join(db_path, sql) for sql in os.listdir(db_path) if \".sqlite\" in sql]\n",
    "        db.append(*sqlite_db)\n",
    "\n",
    "db[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_schema(sqlite_db):\n",
    "    connection = sqlite3.connect(sqlite_db)\n",
    "    cursor = connection.cursor()\n",
    "\n",
    "    cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n",
    "    tables = cursor.fetchall()\n",
    "    table_names = []\n",
    "    for table in tables:\n",
    "        table_name = table[0]\n",
    "        table_names.append(table_name)\n",
    "        print(f\"Table: {table_name}\")\n",
    "        cursor.execute(f\"PRAGMA table_info({table_name});\")\n",
    "        columns = cursor.fetchall()\n",
    "\n",
    "        for column in columns:\n",
    "            _, cname, ctype, _, _, pk_sq = column\n",
    "            print(f\"\\tColumn: {cname} {ctype} {pk_sq}\")\n",
    "            # Get foreign keys for the table\n",
    "        cursor.execute(f\"PRAGMA foreign_key_list({table_name});\")\n",
    "        foreign_keys = cursor.fetchall()\n",
    "        print()\n",
    "\n",
    "        if foreign_keys:\n",
    "            print(\"Foreign Keys:\")\n",
    "            for fk in foreign_keys:\n",
    "                _, _, to_table, fk_column, to_column, _, _, _ = fk\n",
    "                print(f\"\\t{fk_column} REFERENCES {to_table}({to_column})\")\n",
    "            print()\n",
    "\n",
    "    \n",
    "    cursor.close()\n",
    "    connection.close()\n",
    "    return table_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "src/spider/database/musical/musical.sqlite\n",
      "Table: musical\n",
      "\tColumn: Musical_ID INT 1\n",
      "\tColumn: Name TEXT 0\n",
      "\tColumn: Year INT 0\n",
      "\tColumn: Award TEXT 0\n",
      "\tColumn: Category TEXT 0\n",
      "\tColumn: Nominee TEXT 0\n",
      "\tColumn: Result TEXT 0\n",
      "\n",
      "Table: actor\n",
      "\tColumn: Actor_ID INT 1\n",
      "\tColumn: Name TEXT 0\n",
      "\tColumn: Musical_ID INT 0\n",
      "\tColumn: Character TEXT 0\n",
      "\tColumn: Duration TEXT 0\n",
      "\tColumn: age INT 0\n",
      "\n",
      "Foreign Keys:\n",
      "\tMusical_ID REFERENCES actor(Actor_ID)\n",
      "\n",
      "---------------------------------\n",
      "src/spider/database/farm/farm.sqlite\n",
      "Table: city\n",
      "\tColumn: City_ID INT 1\n",
      "\tColumn: Official_Name TEXT 0\n",
      "\tColumn: Status TEXT 0\n",
      "\tColumn: Area_km_2 REAL 0\n",
      "\tColumn: Population REAL 0\n",
      "\tColumn: Census_Ranking TEXT 0\n",
      "\n",
      "Table: farm\n",
      "\tColumn: Farm_ID INT 1\n",
      "\tColumn: Year INT 0\n",
      "\tColumn: Total_Horses REAL 0\n",
      "\tColumn: Working_Horses REAL 0\n",
      "\tColumn: Total_Cattle REAL 0\n",
      "\tColumn: Oxen REAL 0\n",
      "\tColumn: Bulls REAL 0\n",
      "\tColumn: Cows REAL 0\n",
      "\tColumn: Pigs REAL 0\n",
      "\tColumn: Sheep_and_Goats REAL 0\n",
      "\n",
      "Table: farm_competition\n",
      "\tColumn: Competition_ID INT 1\n",
      "\tColumn: Year INT 0\n",
      "\tColumn: Theme TEXT 0\n",
      "\tColumn: Host_city_ID INT 0\n",
      "\tColumn: Hosts TEXT 0\n",
      "\n",
      "Foreign Keys:\n",
      "\tHost_city_ID REFERENCES city(City_ID)\n",
      "\n",
      "Table: competition_record\n",
      "\tColumn: Competition_ID INT 1\n",
      "\tColumn: Farm_ID INT 2\n",
      "\tColumn: Rank INT 0\n",
      "\n",
      "Foreign Keys:\n",
      "\tFarm_ID REFERENCES farm(Farm_ID)\n",
      "\tCompetition_ID REFERENCES farm_competition(Competition_ID)\n",
      "\n",
      "---------------------------------\n"
     ]
    }
   ],
   "source": [
    "# db_map_tables = dict({})\n",
    "for database_path in db[:5]:\n",
    "    for db_name in select_db:\n",
    "        if database_path.split('/')[-2] == db_name:\n",
    "            # if table in exists_table : continue\n",
    "            print(database_path)\n",
    "            get_schema(database_path)\n",
    "            # exists_table.append(table)\n",
    "            print('---------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_schema(schema_db_path, output_path):\n",
    "\n",
    "    connection = sqlite3.connect(schema_db_path)\n",
    "    cursor = connection.cursor()\n",
    "\n",
    "    cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n",
    "    tables = cursor.fetchall()\n",
    "    for table in tables:\n",
    "        \n",
    "        schema = {\n",
    "                    \"JOIN_KEY\" : {\n",
    "                        \"PK\" : list(),\n",
    "                        \"FK\" : dict()\n",
    "                    },\n",
    "                    \"COLUMNS\" : dict()\n",
    "        }\n",
    "        table_name = table[0]\n",
    "        print(f\"Table: {table_name}\")\n",
    "        schema_file_name = f\"{table_name}_datatype.json\"\n",
    "        output_file_path = os.path.join(output_path, schema_file_name)\n",
    "        cursor.execute(f\"PRAGMA table_info({table_name});\")\n",
    "        columns = cursor.fetchall()\n",
    "\n",
    "        for column in columns:\n",
    "            _, cname, ctype, _, _, pk_sq = column\n",
    "            schema['COLUMNS'][cname] = ctype\n",
    "\n",
    "            # if this column is primary key\n",
    "            if pk_sq: schema['JOIN_KEY']['PK'].append(cname)\n",
    "\n",
    "            print(f\"\\tColumn: {cname} {ctype} {pk_sq}\")\n",
    "\n",
    "            # Get foreign keys for the table\n",
    "        cursor.execute(f\"PRAGMA foreign_key_list({table_name});\")\n",
    "        foreign_keys = cursor.fetchall()\n",
    "        print()\n",
    "\n",
    "        if foreign_keys:\n",
    "            print(\"Foreign Keys:\")\n",
    "            for fk in foreign_keys:\n",
    "                _, _, to_table, fk_column, to_column, _, _, _ = fk\n",
    "                schema['JOIN_KEY']['FK'][fk_column] = {to_table : to_column}\n",
    "                print(f\"\\t{fk_column} REFERENCES {to_table}({to_column})\")\n",
    "            print()\n",
    "\n",
    "        with open(output_file_path, \"w\") as file:\n",
    "            json.dump(schema, file, indent=2)\n",
    "            print(f\"Dump {output_file_path} sucess\")\n",
    "    cursor.close()\n",
    "    connection.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table: shop\n",
      "\tColumn: Shop_ID INT 1\n",
      "\tColumn: Address TEXT 0\n",
      "\tColumn: Num_of_staff TEXT 0\n",
      "\tColumn: Score REAL 0\n",
      "\tColumn: Open_Year TEXT 0\n",
      "\n",
      "Dump test/shop_datatype.json sucess\n",
      "Table: member\n",
      "\tColumn: Member_ID INT 1\n",
      "\tColumn: Name TEXT 0\n",
      "\tColumn: Membership_card TEXT 0\n",
      "\tColumn: Age INT 0\n",
      "\tColumn: Time_of_purchase INT 0\n",
      "\tColumn: Level_of_membership INT 0\n",
      "\tColumn: Address TEXT 0\n",
      "\n",
      "Dump test/member_datatype.json sucess\n",
      "Table: happy_hour\n",
      "\tColumn: HH_ID INT 1\n",
      "\tColumn: Shop_ID INT 2\n",
      "\tColumn: Month TEXT 3\n",
      "\tColumn: Num_of_shaff_in_charge INT 0\n",
      "\n",
      "Foreign Keys:\n",
      "\tShop_ID REFERENCES shop(Shop_ID)\n",
      "\n",
      "Dump test/happy_hour_datatype.json sucess\n",
      "Table: happy_hour_member\n",
      "\tColumn: HH_ID INT 1\n",
      "\tColumn: Member_ID INT 2\n",
      "\tColumn: Total_amount REAL 0\n",
      "\n",
      "Foreign Keys:\n",
      "\tMember_ID REFERENCES member(Member_ID)\n",
      "\n",
      "Dump test/happy_hour_member_datatype.json sucess\n"
     ]
    }
   ],
   "source": [
    "transform_schema(\"src/src_dev/coffee_shop/coffee_shop.db\", 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
