{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from selenium.webdriver.common.by import By\n",
    "# from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.common.exceptions import ElementNotInteractableException, NoSuchElementException\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from time import sleep\n",
    "from tqdm import tqdm\n",
    "import re, gensim\n",
    "from googletrans import Translator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_to_words(sentences):\n",
    "  for sentence in sentences:\n",
    "    sentence = re.sub(r'https?://\\S+|www\\.\\S+', '', sentence)\n",
    "    sentence = ''.join([char for char in sentence if char.isascii()])\n",
    "    yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))\n",
    "  \n",
    "def list_of_sentence(text):\n",
    "    sentences = sent_tokenize(text.lower())\n",
    "    los = list(sent_to_words(sentences))\n",
    "    return los"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_list_of_lists_to_text(file_path, data, delimiter='\\t'):\n",
    "    with open(file_path, 'w') as file:\n",
    "        for sublist in data:\n",
    "            line = delimiter.join(sublist)\n",
    "            file.write(line + '\\n')\n",
    "\n",
    "def import_text_to_list_of_lists(file_path, delimiter='\\t'):\n",
    "    result = []\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            line = line.strip()  # Remove leading/trailing whitespace and newlines\n",
    "            sublist = line.split(delimiter)\n",
    "            result.append(sublist)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def page_contents(link, div_class = 'elementor-widget-container',tag='p'):\n",
    "    driver = webdriver.Safari()\n",
    "    driver.get(link)\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    div_elements = soup.find_all('div', class_=div_class)\n",
    "\n",
    "    paragraphs = []\n",
    "    for div in div_elements:\n",
    "        div_paragraphs = div.find_all(tag)\n",
    "        for paragraph in div_paragraphs:\n",
    "            paragraphs.append(paragraph.text.replace('\\xa0', ''))\n",
    "\n",
    "    driver.quit()\n",
    "    return paragraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_text(text, target_language):\n",
    "    translator = Translator()\n",
    "    detected_lang = \"th\"\n",
    "    translated_text = translator.translate(text, src=detected_lang, dest=target_language)\n",
    "    return translated_text.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scraping links on news page with click load more button\n",
    "\n",
    "driver = webdriver.Safari()\n",
    "driver.get('https://scbtechx.io/news/')\n",
    "wait_duration = 5\n",
    "\n",
    "while True:\n",
    "    try:\n",
    "        # Find the button element by its class\n",
    "        load_more_button = driver.find_element(By.CLASS_NAME, \"wpr-load-more-btn\")\n",
    "        driver.execute_script(\"arguments[0].scrollIntoView();\", load_more_button)\n",
    "        load_more_button.click()\n",
    "        print(\"Clicked 'Load More' button\")\n",
    "        sleep(wait_duration)\n",
    "\n",
    "    except ElementNotInteractableException:\n",
    "        break\n",
    "\n",
    "soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "section = soup.find(\"section\", class_=\"wpr-grid elementor-clearfix grid-images-loaded\")\n",
    "a_tags = section.find_all(\"a\")\n",
    "\n",
    "keep_links = [link.get('href') for link in a_tags]\n",
    "news_lisks = [link for link in set(keep_links) if link.startswith('https://scbtechx.io/') and 'https://scbtechx.io/category/' not in link]\n",
    "\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "links = ['https://scbtechx.io/privacy-notice/customers/',\n",
    "         'https://scbtechx.io/privacy-notice/non-customers/',\n",
    "         'https://scbtechx.io/data-platform/',\n",
    "         'https://scbtechx.io/ekyc/',\n",
    "         'https://scbtechx.io/ekyc-innovestx/',\n",
    "         'https://scbtechx.io/services-products/',\n",
    "         'https://scbtechx.io/terms-of-use/']\n",
    "\n",
    "links += news_lisks\n",
    "\n",
    "all_contents = []\n",
    "\n",
    "for link in tqdm(links):\n",
    "    tag = 'p'\n",
    "    if link == 'https://scbtechx.io/services-products/':\n",
    "        tag = 'h2'\n",
    "    all_contents.append(page_contents(link,tag=tag))\n",
    "    sleep(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_corpus = [list_of_sentence(\" \".join(c)) for c in all_contents]\n",
    "los_all_corpus = [list_of_sentence for corpus in all_corpus for list_of_sentence in corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'src/Corpus_TechX_new.txt'\n",
    "write_list_of_lists_to_text(file_path, los_all_corpus, delimiter='\\t')\n",
    "# techx_los = import_text_to_list_of_lists(file_path, delimiter='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Check word freq\n",
    "# from collections import defaultdict\n",
    "\n",
    "# word_freq = defaultdict(int)\n",
    "# for sublist in techx_los:\n",
    "#     for word in sublist:\n",
    "#         word_freq[word] += 1\n",
    "# word_freq = dict(word_freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PointX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scping links on promotion page\n",
    "\n",
    "driver = webdriver.Safari()\n",
    "driver.get('https://www.pointx.scb/promotions/')\n",
    "\n",
    "# Parse the HTML content using BeautifulSoup\n",
    "soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "\n",
    "\n",
    "div = soup.find(\"div\", class_=\"special-promotion-carousel-container carousel-share\")\n",
    "a_tags = div.find_all(\"a\")\n",
    "\n",
    "links = [a.get(\"href\") for a in a_tags]\n",
    "links = list(set(links))    # drop duplicate\n",
    "\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pointx_page_contents(link):\n",
    "    driver = webdriver.Safari()\n",
    "    driver.get(link)\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "    p_tags = soup.find_all(\"p\")\n",
    "    p_texts = [re.sub(r\"[\\n\\t]\",\" \",tag.text )for tag in p_tags if tag.text.strip()]\n",
    "    \n",
    "    li_tags = soup.find_all(\"li\")\n",
    "    li_texts = [re.sub(r\"[\\n\\t]\",\" \",tag.text ) for tag in li_tags if tag.text.strip()]\n",
    "\n",
    "    driver.quit()\n",
    "    return p_texts + li_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pages_cons = []\n",
    "\n",
    "for link in tqdm(links):\n",
    "    pages_cons.append(pointx_page_contents(link))\n",
    "    sleep(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_corpus = []\n",
    "for content in tqdm(pages_cons):\n",
    "    los_en = [translate_text(sentence,'en') for sentence in content]\n",
    "    all_corpus.append(list_of_sentence(\" \".join(los_en)))\n",
    "\n",
    "los_all_corpus = [list_of_sentence for corpus in all_corpus for list_of_sentence in corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'src/PointX_corpus.txt'\n",
    "write_list_of_lists_to_text(file_path, los_all_corpus, delimiter='\\t')\n",
    "# pointx_los = import_text_to_list_of_lists(file_path, delimiter='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SCB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Title = \"Business Maker\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Safari()\n",
    "driver.get('https://www.scb.co.th/en/personal-banking/stories.html')\n",
    "\n",
    "wait_duration = 5\n",
    "rounds = True\n",
    "tip4u_Title_button = driver.find_element(By.XPATH, f'//a[@class=\"ga_menu_section\" and @title=\"{Title}\"]')\n",
    "tip4u_Title_button.click()\n",
    "\n",
    "while (rounds):\n",
    "    try:\n",
    "        # Find the button element by its class\n",
    "        load_more_button = driver.find_element(By.XPATH, '//a[@class=\"btn-default mar-top ga_see_all_button\" and @title=\"Load more\"]')\n",
    "        # driver.execute_script(\"arguments[0].scrollIntoView();\", load_more_button)\n",
    "        load_more_button.click()\n",
    "        print(\"Clicked 'Load More' button\")\n",
    "        sleep(wait_duration)\n",
    "        # rounds -= 1\n",
    "    except :\n",
    "        break\n",
    "\n",
    "div_element = driver.find_element(By.CSS_SELECTOR, \"div.row.relative.clearfix\")\n",
    "\n",
    "link_elements = div_element.find_elements(By.TAG_NAME, \"a\")\n",
    "\n",
    "links = [link.get_attribute(\"href\") for link in link_elements]\n",
    "links = list(set(links))    # drop duplicate\n",
    "\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pages_cons = []\n",
    "for link in tqdm(links):\n",
    "    pages_cons.append(page_contents(link,div_class=\"content\"))\n",
    "    sleep(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_corpus = []\n",
    "for c in pages_cons:\n",
    "    all_corpus.append(list_of_sentence(\" \".join(c)))\n",
    "    \n",
    "los_all_corpus = [list_of_sentence for corpus in all_corpus for list_of_sentence in corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'src/SCB_corpus_businessmaker.txt'\n",
    "write_list_of_lists_to_text(file_path, los_all_corpus, delimiter='\\t')\n",
    "# scb_tip4u_los = import_text_to_list_of_lists(file_path, delimiter='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "InnoverstX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_news(page_link):\n",
    "    driver = webdriver.Safari()\n",
    "    driver.get(page_link)\n",
    "    div_elements = driver.find_elements(By.CLASS_NAME, 'blog_card_body_lower')\n",
    "    for div in div_elements:\n",
    "        a_tag = div.find_element(By.TAG_NAME, 'a')\n",
    "        yield a_tag.text, a_tag.get_attribute('href')\n",
    "        # print(a_tag.text, a_tag.get_attribute('href'))\n",
    "    sleep(5)\n",
    "    driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clicked 'Load More' button\n",
      "Clicked 'Load More' button\n",
      "Clicked 'Load More' button\n",
      "Clicked 'Load More' button\n",
      "Last page: 48\n"
     ]
    }
   ],
   "source": [
    "driver = webdriver.Safari()\n",
    "driver.get('https://www.innovestx.co.th/wealth-playlist')\n",
    "wait_duration = 5\n",
    "rounds = True\n",
    "\n",
    "while (rounds):\n",
    "    try:\n",
    "        # Find the button element by its class\n",
    "        load_more_button = driver.find_element(By.XPATH, '//a[@aria-label=\"Go to next page\"]')\n",
    "        load_more_button.click()\n",
    "        print(\"Clicked 'Load More' button\")\n",
    "        sleep(wait_duration)\n",
    "    except :\n",
    "        ul = driver.find_element(By.CLASS_NAME, 'pagination_page_number')\n",
    "        li_elements = ul.find_elements(By.TAG_NAME, 'li')\n",
    "        last_page = int(li_elements[-1].text.strip())\n",
    "        print(f\"Last page: {last_page}\")\n",
    "        break\n",
    "\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 48/48 [10:17<00:00, 12.86s/it]\n"
     ]
    }
   ],
   "source": [
    "news_topics, news_contents = [], []\n",
    "for num_page in tqdm(range(last_page)):\n",
    "    page_link = f\"https://www.innovestx.co.th/wealth-playlist/{num_page+1}\"\n",
    "    for topic, link in get_news(page_link):\n",
    "        news_topics.append(topic.strip())\n",
    "        news_contents.append(link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = {'News topics': news_topics, 'Links': news_contents}\n",
    "df = pd.DataFrame(data)\n",
    "df.to_csv('InnovestX_news.csv', index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
