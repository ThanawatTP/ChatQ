{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.common.exceptions import ElementNotInteractableException\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from time import sleep\n",
    "import re, gensim\n",
    "from googletrans import Translator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def normalize_text(text):\n",
    "#     # Keep only words, alphabets, and numbers\n",
    "#     # text = re.sub(r\"[^a-zA-Z0-9\\s]\", \"\", text)\n",
    "\n",
    "#     text = text.replace('\\u202f', ' ')\n",
    "    \n",
    "#     text = ''.join([char for char in text if char.isascii()])\n",
    "\n",
    "#     text = re.sub(r'https://\\S+', ' ', text)\n",
    "\n",
    "#     text = re.sub(r'[/.\\n\\t]', ' ', text)\n",
    "    \n",
    "#     text = re.sub(r'www.\\S+', ' ', text)\n",
    "#     # Normalize phone numbers\n",
    "#     text = re.sub(r'\\b\\d{2}-\\d{3}-\\d{4}\\b', ' ', text)\n",
    "#     # Normalize numeric sequences\n",
    "#     text = re.sub(r'\\b\\d+\\b', ' ', text)\n",
    "\n",
    "#     return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_to_words(sentences):\n",
    "  for sentence in sentences:\n",
    "    sentence = re.sub(r'https?://\\S+|www\\.\\S+', '', sentence)\n",
    "    sentence = ''.join([char for char in sentence if char.isascii()])\n",
    "    yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_of_sentence(text):\n",
    "    sentences = sent_tokenize(text.lower())\n",
    "    # preprocessing \n",
    "    los = list(sent_to_words(sentences))\n",
    "    # los = []\n",
    "    # for sen in sentences:\n",
    "    #     sen = normalize_text(sen)\n",
    "    #     tokensen = word_tokenize(sen)\n",
    "    #     tokensen = [element for element in tokensen if element[0].isalpha()]\n",
    "    #     los.append(tokensen)\n",
    "    return los"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_list_of_lists_to_text(file_path, data, delimiter='\\t'):\n",
    "    with open(file_path, 'w') as file:\n",
    "        for sublist in data:\n",
    "            line = delimiter.join(sublist)\n",
    "            file.write(line + '\\n')\n",
    "\n",
    "def import_text_to_list_of_lists(file_path, delimiter='\\t'):\n",
    "    result = []\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            line = line.strip()  # Remove leading/trailing whitespace and newlines\n",
    "            sublist = line.split(delimiter)\n",
    "            result.append(sublist)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def page_contents(link, div_class = 'elementor-widget-container'):\n",
    "    driver = webdriver.Safari()\n",
    "    driver.get(link)\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    div_elements = soup.find_all('div', class_=div_class)\n",
    "\n",
    "    paragraphs = []\n",
    "    for div in div_elements:\n",
    "        div_paragraphs = div.find_all('p')\n",
    "        for paragraph in div_paragraphs:\n",
    "            paragraphs.append(paragraph.text.replace('\\xa0', ''))\n",
    "\n",
    "    driver.quit()\n",
    "    return paragraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_text(text, target_language):\n",
    "\n",
    "    translator = Translator()\n",
    "    detected_lang = \"th\"\n",
    "    translated_text = translator.translate(text, src=detected_lang, dest=target_language)\n",
    "    return translated_text.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Safari()\n",
    "driver.get('https://scbtechx.io/services-products/')\n",
    "soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "sp_h2_content = soup.find_all('h2', class_='elementor-heading-title elementor-size-default')\n",
    "sp_content = [text.text for text in sp_h2_content]\n",
    "\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Safari()\n",
    "driver.get('https://scbtechx.io/terms-of-use/')\n",
    "soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "span_elements = soup.find_all('span', style='font-weight: 400;')\n",
    "tou_content = [span.text.replace('\\xa0', '') for span in span_elements]\n",
    "\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Safari()\n",
    "driver.get('https://scbtechx.io/news/')\n",
    "\n",
    "# Define the wait duration (in seconds)\n",
    "wait_duration = 5\n",
    "\n",
    "while True:\n",
    "    try:\n",
    "        # Find the button element by its class\n",
    "        load_more_button = driver.find_element(By.CLASS_NAME, \"wpr-load-more-btn\")\n",
    "        # load_more_button = WebDriverWait(driver, wait_duration).until(\n",
    "        #     EC.visibility_of_element_located((By.CLASS_NAME, \"wpr-load-more-btn\"))\n",
    "        # )\n",
    "        driver.execute_script(\"arguments[0].scrollIntoView();\", load_more_button)\n",
    "        load_more_button.click()\n",
    "        print(\"Clicked 'Load More' button\")\n",
    "        sleep(wait_duration)\n",
    "\n",
    "    except ElementNotInteractableException:\n",
    "        break\n",
    "\n",
    "soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "# Find the section tag with the specified class\n",
    "section = soup.find(\"section\", class_=\"wpr-grid elementor-clearfix grid-images-loaded\")\n",
    "\n",
    "# Find all <a> tags within the section\n",
    "a_tags = section.find_all(\"a\")\n",
    "keep_links = [link.get('href') for link in a_tags]\n",
    "news_lisks = [link for link in set(keep_links) if link.startswith('https://scbtechx.io/') and 'https://scbtechx.io/category/' not in link]\n",
    "\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "links = ['https://scbtechx.io/privacy-notice/customers/',\n",
    "         'https://scbtechx.io/privacy-notice/non-customers/',\n",
    "         'https://scbtechx.io/data-platform/',\n",
    "         'https://scbtechx.io/ekyc/',\n",
    "         'https://scbtechx.io/ekyc-innovestx/']\n",
    "\n",
    "links += news_lisks\n",
    "print(len(links))\n",
    "\n",
    "pages_cons = []\n",
    "for link in links:\n",
    "\n",
    "    pages_cons.append(page_contents(link))\n",
    "    \n",
    "    sleep(10)\n",
    "\n",
    "all_contents = pages_cons + [tou_content] + [sp_content]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_corpus = []\n",
    "for c in all_contents:\n",
    "    all_corpus.append(list_of_sentence(\" \".join(c)))\n",
    "\n",
    "los_all_corpus = [list_of_sentence for corpus in all_corpus for list_of_sentence in corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'src/TechX_corpus_norm.txt'\n",
    "# file_path = 'src/TechX_corpus_normTH.txt'\n",
    "write_list_of_lists_to_text(file_path, los_all_corpus, delimiter='\\t')\n",
    "techx_los = import_text_to_list_of_lists(file_path, delimiter='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Check word freq\n",
    "# from collections import defaultdict\n",
    "\n",
    "# word_freq = defaultdict(int)\n",
    "# for sublist in techx_los:\n",
    "#     for word in sublist:\n",
    "#         word_freq[word] += 1\n",
    "# word_freq = dict(word_freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PointX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Safari()\n",
    "driver.get('https://www.pointx.scb/promotions/')\n",
    "\n",
    "# Parse the HTML content using BeautifulSoup\n",
    "soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "\n",
    "# Find the <div> element with the specified class\n",
    "div = soup.find(\"div\", class_=\"special-promotion-carousel-container carousel-share\")\n",
    "# Find all <a> tags within the <div> element\n",
    "a_tags = div.find_all(\"a\")\n",
    "links = [a.get(\"href\") for a in a_tags]\n",
    "\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "links = list(set(links))\n",
    "links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pointx_page_contents(link):\n",
    "    driver = webdriver.Safari()\n",
    "    driver.get(link)\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "    p_tags = soup.find_all(\"p\")\n",
    "    p_texts = [re.sub(r\"[\\n\\t]\",\" \",tag.text )for tag in p_tags if tag.text.strip()]\n",
    "    # Find all <li> tags and extract the text\n",
    "    li_tags = soup.find_all(\"li\")\n",
    "    li_texts = [re.sub(r\"[\\n\\t]\",\" \",tag.text ) for tag in li_tags if tag.text.strip()]\n",
    "\n",
    "    driver.quit()\n",
    "    return p_texts + li_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pages_cons = []\n",
    "\n",
    "for link in links:\n",
    "    pages_cons.append(pointx_page_contents(link))\n",
    "    sleep(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_corpus = []\n",
    "for content in pages_cons:\n",
    "    los_en = [translate_text(sentence,'en') for sentence in content]\n",
    "    all_corpus.append(list_of_sentence(\" \".join(los_en)))\n",
    "\n",
    "los_all_corpus = [list_of_sentence for corpus in all_corpus for list_of_sentence in corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'src/PointX_corpus_norm.txt'\n",
    "write_list_of_lists_to_text(file_path, los_all_corpus, delimiter='\\t')\n",
    "pointx_los = import_text_to_list_of_lists(file_path, delimiter='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SCB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clicked 'Load More' button\n",
      "Clicked 'Load More' button\n",
      "Clicked 'Load More' button\n",
      "Clicked 'Load More' button\n",
      "Clicked 'Load More' button\n",
      "Clicked 'Load More' button\n",
      "Clicked 'Load More' button\n",
      "Clicked 'Load More' button\n",
      "Clicked 'Load More' button\n",
      "Clicked 'Load More' button\n"
     ]
    }
   ],
   "source": [
    "driver = webdriver.Safari()\n",
    "driver.get('https://www.scb.co.th/en/personal-banking/stories.html')\n",
    "# Find the \"Load more\" button element\n",
    "\n",
    "wait_duration = 5\n",
    "i = 8\n",
    "tip4u_button = driver.find_element(By.XPATH, '//a[@class=\"ga_menu_section\" and @title=\"Tips for You\"]')\n",
    "tip4u_button.click()\n",
    "\n",
    "while (i>0):\n",
    "    try:\n",
    "        # Find the button element by its class\n",
    "        load_more_button = driver.find_element(By.XPATH, '//a[@class=\"btn-default mar-top ga_see_all_button\" and @title=\"Load more\"]')\n",
    "        # driver.execute_script(\"arguments[0].scrollIntoView();\", load_more_button)\n",
    "        load_more_button.click()\n",
    "        print(\"Clicked 'Load More' button\")\n",
    "        sleep(wait_duration)\n",
    "        i -= 1\n",
    "    except ElementNotInteractableException:\n",
    "        break\n",
    "\n",
    "div_element = driver.find_element(By.CSS_SELECTOR, \"div.row.relative.clearfix\")\n",
    "\n",
    "# Find all the link elements within the div\n",
    "link_elements = div_element.find_elements(By.TAG_NAME, \"a\")\n",
    "\n",
    "# Extract the href attribute value from each link element\n",
    "links = [link.get_attribute(\"href\") for link in link_elements]\n",
    "\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "74"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
