{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_text(text):\n",
    "    # Keep only words, alphabets, and numbers\n",
    "    text = re.sub(r\"[^a-zA-Z0-9\\s]\", \"\", text)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_of_sentence(text):\n",
    "    los = []\n",
    "    sentences = sent_tokenize(text.lower())\n",
    "    for sen in sentences:\n",
    "        # sen = normalize_text(sen)\n",
    "        tokensen = word_tokenize(sen)\n",
    "        los.append(tokensen)\n",
    "    return los"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_list_of_lists_to_text(file_path, data, delimiter='\\t'):\n",
    "    with open(file_path, 'w') as file:\n",
    "        for sublist in data:\n",
    "            line = delimiter.join(sublist)\n",
    "            file.write(line + '\\n')\n",
    "\n",
    "def import_text_to_list_of_lists(file_path, delimiter='\\t'):\n",
    "    result = []\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            line = line.strip()  # Remove leading/trailing whitespace and newlines\n",
    "            sublist = line.split(delimiter)\n",
    "            result.append(sublist)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def page_contents(link, div_class = 'elementor-widget-container'):\n",
    "    driver = webdriver.Safari()\n",
    "    driver.get(link)\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    div_elements = soup.find_all('div', class_=div_class)\n",
    "\n",
    "    paragraphs = []\n",
    "    for div in div_elements:\n",
    "        div_paragraphs = div.find_all('p')\n",
    "        for paragraph in div_paragraphs:\n",
    "            paragraphs.append(paragraph.text.replace('\\xa0', ''))\n",
    "\n",
    "    driver.quit()\n",
    "    return paragraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Safari()\n",
    "driver.get('https://scbtechx.io/services-products/')\n",
    "soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "sp_h2_content = soup.find_all('h2', class_='elementor-heading-title elementor-size-default')\n",
    "sp_links = soup.find_all('a')\n",
    "\n",
    "sp_content = [text.text for text in sp_h2_content]\n",
    "\n",
    "# keep_links = [link.get('href') for link in sp_links]\n",
    "# filtered_list = list(filter(lambda element: element.startswith('https://scbtechx.io/'), set(keep_links)))\n",
    "\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Safari()\n",
    "driver.get('https://scbtechx.io/terms-of-use/')\n",
    "soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "span_elements = soup.find_all('span', style='font-weight: 400;')\n",
    "\n",
    "tou_content = [span.text.replace('\\xa0', '') for span in span_elements]\n",
    "\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "links = ['https://scbtechx.io/privacy-notice/customers/',\n",
    "         'https://scbtechx.io/privacy-notice/non-customers/',\n",
    "         'https://scbtechx.io/data-platform/',\n",
    "         'https://scbtechx.io/ekyc/',\n",
    "         'https://scbtechx.io/ekyc-innovestx/']\n",
    "\n",
    "pages_cons = []\n",
    "for link in links:\n",
    "    pages_cons.append(page_contents(link))\n",
    "\n",
    "all_contents = pages_cons + [tou_content] + [sp_content]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_corpus = []\n",
    "for c in all_contents:\n",
    "    all_corpus.append(list_of_sentence(\" \".join(c)))\n",
    "\n",
    "los_all_corpus = [list_of_sentence for corpus in all_corpus for list_of_sentence in corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'src/TechX_corpus.txt'\n",
    "write_list_of_lists_to_text(file_path, los_all_corpus, delimiter='\\t')\n",
    "my_list_of_lists = import_text_to_list_of_lists(file_path, delimiter='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check word freq\n",
    "from collections import defaultdict\n",
    "\n",
    "word_freq = defaultdict(int)\n",
    "for sublist in my_list_of_lists:\n",
    "    for word in sublist:\n",
    "        word_freq[word] += 1\n",
    "word_freq = dict(word_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
