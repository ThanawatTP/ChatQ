{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, json, re, time, ast, warnings, sqlite3\n",
    "import pandas as pd\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from dotenv import load_dotenv\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "sys.path.append('../')\n",
    "from filtering_schema.Description_base_linking import SchemaLinking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "# Set environment variables\n",
    "base_dir = \"../filtering_schema\"\n",
    "os.environ['nsql_model_path'] = os.path.join(base_dir, 'models', 'nsql-350M')\n",
    "os.environ['sentence_emb_model_path'] = os.path.join(base_dir, 'models', 'all-MiniLM-L6-v2')\n",
    "os.environ['schema_description_folder_path'] = os.path.join(base_dir, 'src', 'schemas', 'column-descriptions')\n",
    "os.environ['schema_data_types_folder_path'] = os.path.join(base_dir, 'src', 'schemas', 'column-datatypes')\n",
    "os.environ['column_threshold'] = '0.2'\n",
    "os.environ['table_threshold'] = '0.2'\n",
    "os.environ['max_select_column'] = '10'\n",
    "os.environ['filter_table'] = 'False'\n",
    "os.environ['verbose'] = 'False'\n",
    "\n",
    "\n",
    "schema_link = SchemaLinking()\n",
    "schema_link.selected_domain(schema_description_folder_path=os.environ.get('schema_description_folder_path'),\n",
    "                            schema_data_types_folder_path=os.environ.get('schema_data_types_folder_path'))\n",
    "\n",
    "verbose = bool(os.environ.get('verbose').lower() == 'true')\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(os.environ.get('nsql_model_path'))\n",
    "nsql_model = AutoModelForCausalLM.from_pretrained(os.environ.get('nsql_model_path'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = 'gpt-3.5-turbo-1106'\n",
    "pointx_nlqsql_df = pd.read_csv(\"../src/pointx/PointX_nlqsql_pair.csv\")\n",
    "pointx_nlqsql_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../filtering_schema/src/schemas/column-descriptions/pointx_keymatrix_dly_description.json') as f:\n",
    "    pointx_keymatrix_dly_description = json.load(f)\n",
    "\n",
    "with open('../filtering_schema/src/schemas/column-descriptions/pointx_fbs_rpt_dly_description.json') as f:\n",
    "    pointx_fbs_rpt_dly_description = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM Schema-linking by giving the schema description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SQL_columns(sql_query, table_name):\n",
    "    if table_name == 'pointx_keymatrix_dly':\n",
    "        shema_description = pointx_keymatrix_dly_description\n",
    "    elif table_name == 'pointx_fbs_rpt_dly':\n",
    "        shema_description = pointx_fbs_rpt_dly_description\n",
    "    used_cols = [col for col in shema_description['columns'].keys() if col in re.split(r'[()\\.\\,\\s]+', sql_query) ] \n",
    "    return used_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_divide(numerator, denominator):\n",
    "    try:\n",
    "        result = round(numerator / denominator,2)\n",
    "    except :\n",
    "        result = \"ZeroDivisionError\"\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1_columns(actual_columns, result_columns):\n",
    "    col_TP = len(set(actual_columns) & set(result_columns))\n",
    "    col_FP = len(set(result_columns) - set(actual_columns))\n",
    "    col_FN = len(set(actual_columns) - set(result_columns))\n",
    "\n",
    "    if col_TP is not None and col_FN is not None and col_FP is not None:\n",
    "        if len(actual_columns) == 0 : col_recall = 1\n",
    "        else: col_recall = safe_divide(col_TP, col_TP + col_FN)\n",
    "        col_precision = safe_divide(col_TP, col_TP + col_FP)\n",
    "        \n",
    "    if col_precision is not None and col_recall is not None and col_recall != \"ZeroDivisionError\" and col_precision != \"ZeroDivisionError\":\n",
    "        col_f1 = 2 * safe_divide(col_precision * col_recall, col_precision + col_recall)\n",
    "        if type(col_f1) == str: col_f1 = \"ZeroDivisionError\"\n",
    "    else: col_f1 = \"ERROR\"\n",
    "    \n",
    "    return col_recall, col_precision, col_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pointx_keymatrix_dly_context = \"\"\"\n",
    "{\n",
    "    \"table\": \"pointx_keymatrix_dly\",\n",
    "    \"description\": \"The Key Matrix Dashboard Design table provides a detailed overview of dashboard-related database columns, \\nincluding data types, status indicators, descriptions, conditions, business logic, and sample data, \\nenabling a comprehensive understanding of the data structure for effective dashboard design.\",\n",
    "    \"columns\": {\n",
    "        \"month_id\": \"Transaction month\",\n",
    "        \"ntx_pointx_financial\": \"All financial transactions, both Payment, Top Up, Transfer\",\n",
    "        \"ntx_pointx_financial_out\": \"Number of Payment Transaction, Transfer\",\n",
    "        \"ncust_user\": \"All the number of Customer in the systemAnd can use Point XPP\",\n",
    "        \"ncust_pointx\": \"Point X Customers, both Customer Type, Easy and Non Easy, not including Guest.\",\n",
    "        \"ncust_visit\": \"All the number of Customer in the systemAnd come to use the point x app on that day\",\n",
    "        \"ncust_pointx_visit\": \"The number of point x Customers, both Customer Type, is Easy and Non Easy, not including Guest that came to use the point x app on that day.\",\n",
    "        \"ncust_pointx_financial\": \"The number of point x Customers, both Customer Type, is Easy and Non Easy, not including Guest at the financial transaction.\",\n",
    "        \"ncust_guest\": \"The number of point x Customers, both Customer Type is Guest.\",\n",
    "        \"ncust_guest_visit\": \"Point X Customers, both Customer Type, is a guest that came to use the point x app on that day.\",\n",
    "        \"ncust_register_success\": \"New customers that Register comes each day.\",\n",
    "        \"amt_point_topup\": \"The number of points caused by all topups\",\n",
    "        \"amt_point_topup_auto\": \"Auto Converse Point Topup number\",\n",
    "        \"amt_point_topup_onboard\": \"Point Topup number onboard\",\n",
    "        \"amt_point_topup_onetime\": \"Number of Point Topup Manual\",\n",
    "        \"amt_point_transfer_out\": \"The number of points that transferred to other users\",\n",
    "        \"amt_point_pay\": \"The number of points caused by all payment\",\n",
    "        \"amt_point_pay_sku\": \"The number of points caused by payment via X-Store.\",\n",
    "        \"amt_point_pay_qr\": \"The number of points caused by payment via Scan & Pay.\",\n",
    "        \"amt_point_pay_pyw\": \"The number of points caused by payment via Paywise.\",\n",
    "        \"amt_point_pay_pyw_rbh\": \"The number of points caused by payment via Robinhood Platform.\",\n",
    "        \"amt_point_pay_qr_29\": \"The number of points caused by payment via Scan & Pay type QR 29.\",\n",
    "        \"amt_point_pay_qr_30\": \"The number of points caused by payment via Scan & Pay type QR 30.\",\n",
    "        \"amt_point_pay_qr_cs\": \"The number of points caused by payment via Scan & Pay type QRCS.\",\n",
    "        \"amt_point_payment_p_only\": \"The number of points caused by payment by Payment Method is the use of Point Only.\",\n",
    "        \"amt_point_payment_p_casa\": \"The number of points caused by payment by Payment Method is the use of Point, plus payment by transferring money.\",\n",
    "        \"amt_point_payment_p_cc\": \"The number of points caused by payment by Payment Method is to use Point, plus payment through credit cards.\",\n",
    "        \"rate_point_per_baht_pay\": \"The average of the Relased Rate at all Payment customers.\",\n",
    "        \"rate_point_per_baht_pay_sku\": \"The average rate of the customer Payment via X-Store (Weigthed Average with Point AMOUNT)\",\n",
    "        \"rate_point_per_baht_pay_qr\": \"The rate of the Rate that Payment customers via Scan & Pay (Weigthed Average with Point Amount paid).\",\n",
    "        \"rate_point_per_baht_pay_pyw\": \"The average rate of the Payment customers via Paywise (Weighted Average with Point Amount paid).\",\n",
    "        \"rate_point_per_baht_pay_pyw_rbh\": \"The average rate of the Payment customers via Robinhood Platform (Weigthed Average with Point AMOUNT)\",\n",
    "        \"rate_point_per_baht_pay_qr_29\": \"The average rate of the customer Payment with Transaction Type type QR 29 (Weigthed Average with Point AMOUNT)\",\n",
    "        \"rate_point_per_baht_pay_qr_30\": \"The average rate of the customer Payment with Transaction Type type QR 30 (Weigthed Average with Point AMOUNT)\",\n",
    "        \"rate_point_per_baht_pay_qr_cs\": \"The average rate of the customer Payment with Transaction Type type QRCS (Weigted Average with Point Amount paid)\",\n",
    "        \"rate_point_per_baht_pay_weight\": \"The average of the Relased Rate at all Payment customers.\",\n",
    "        \"rate_point_per_baht_pay_sku_weight\": \"The average rate of the customer Payment via X-Store (Weigthed Average with Point AMOUNT)\",\n",
    "        \"rate_point_per_baht_pay_qr_weight\": \"The rate of the Rate that Payment customers via Scan & Pay (Weigthed Average with Point Amount paid).\",\n",
    "        \"rate_point_per_baht_pay_pyw_weight\": \"The average rate of the Payment customers via Paywise (Weighted Average with Point Amount paid).\",\n",
    "        \"rate_point_per_baht_pay_pyw_rbh_weight\": \"The average rate of the Payment customers via Robinhood Platform (Weigthed Average with Point AMOUNT)\",\n",
    "        \"rate_point_per_baht_pay_qr_29_weight\": \"The average rate of the customer Payment with Transaction Type type QR 29 (Weigthed Average with Point AMOUNT)\",\n",
    "        \"rate_point_per_baht_pay_qr_30_weight\": \"The average rate of the customer Payment with Transaction Type type QR 30 (Weigthed Average with Point AMOUNT)\",\n",
    "        \"rate_point_per_baht_pay_qr_cs_weight\": \"The average rate of the customer Payment with Transaction Type type QRCS (Weigted Average with Point Amount paid)\",\n",
    "        \"rate_baht_per_point_pay\": \"The average of the Cost Per Point at all Payment customers (Weigted Average with Point Amount paid)\",\n",
    "        \"rate_baht_per_point_pay_sku\": \"The average of the Cost Per Point that the Payment customers via X-Store (Weigthed Average with Point AMOUNT).\",\n",
    "        \"rate_baht_per_point_pay_qr\": \"The average Cost Per Point that Payment customers via Scan & Pay (Weigthed Average with Point Amount paid)\",\n",
    "        \"rate_baht_per_point_pay_pyw\": \"The average of the Cost Per Point that the Paywise (Weigthed Average with Point AMOUNT) is calculated from 1 divided by release rate.\",\n",
    "        \"rate_baht_per_point_pay_pyw_rbh\": \"The average of the Cost Per Point that Payment customers via Robinhood Platform (Weigthed Average with Point AMOUNT)\",\n",
    "        \"rate_baht_per_point_pay_qr_29\": \"The average of Cost Per Point at Payment customers with Transaction Type type QR 29 (Weigthed Average with Point AMOUNT)\",\n",
    "        \"rate_baht_per_point_pay_qr_30\": \"The average of Cost Per Point at Payment customers with Transaction Type type QR 30 (Weigthed Average with Point AMOUNT)\",\n",
    "        \"rate_baht_per_point_pay_qr_cs\": \"The average of Cost Per Point at Payment customers with Transaction Type type QRCS (Weigthed Average with Point AMOUNT)\",\n",
    "        \"rate_baht_per_point_pay_weight\": \"The average of the Cost Per Point at all Payment customers (Weigted Average with Point Amount paid)\",\n",
    "        \"rate_baht_per_point_pay_sku_weight\": \"The average of the Cost Per Point that the Payment customers via X-Store (Weigthed Average with Point AMOUNT).\",\n",
    "        \"rate_baht_per_point_pay_qr_weight\": \"The average of the Cost Per Point at Payment customers via Scan & Pay (Weigthed Average with Point Amount paid).\",\n",
    "        \"rate_baht_per_point_pay_pyw_weight\": \"The average of the Cost Per Point that the Paywise (Weigthed Average with Point AMOUNT) is calculated from 1 divided by release rate.\",\n",
    "        \"rate_baht_per_point_pay_pyw_rbh_weight\": \"The average Cost Per Point that Payment customers via Robinhood Platform (Weigthed Average with Point Amount paid)\",\n",
    "        \"rate_baht_per_point_pay_qr_29_weight\": \"The average of the Cost Per Point at Payment customers with Transaction Type type QR 29 (Weigthed Average with Point AMOUNT)\",\n",
    "        \"rate_baht_per_point_pay_qr_30_weight\": \"The average of Cost Per Point at Payment customers with Transaction Type type QR 30 (Weigthed Average with Point AMOUNT)\",\n",
    "        \"rate_baht_per_point_pay_qr_cs_weight\": \"The average of Cost Per Point at Payment customers with Transaction Type type QRCS (Weigthed Average with Point AMOUNT)\",\n",
    "        \"n_topup_point\": \"The number of transactions caused by all topups.\",\n",
    "        \"n_topup_point_onboard\": \"Number of Transaction Topup Episode Onboard\",\n",
    "        \"n_topup_point_onetime\": \"Number of Transaction Topup Manual\",\n",
    "        \"n_topup_point_auto\": \"Auto converse transaction top\",\n",
    "        \"n_transfer_point_out\": \"The number of transactions that transferred to other users\",\n",
    "        \"n_purchase\": \"The number of transactions caused by payment does not include Reverse Scan & Pay (2002A), Reverse Paywise (2004A) and Cancel Order (7015A) of X-Store.\",\n",
    "        \"n_purchase_sku\": \"The number of transactions caused by payment via X-Store does not include the Cancel Order (7015A) of X-Store.\",\n",
    "        \"n_purchase_qr\": \"The number of transactions caused by payment via Scan & Pay does not include Reverse Scan & Pay (2002A).\",\n",
    "        \"n_purchase_pyw\": \"The number of transactions caused by payment via Paywise does not include Reverse Paywise (2004A).\",\n",
    "        \"n_purchase_pyw_rbh\": \"The number of transactions caused by payment via Robinhood Platform does not include Reverse Scan & Pay (2002A).\",\n",
    "        \"n_purchase_qr_29\": \"The number of transactions caused by payment via Scan & Pay with Transaction Type type QR 29, not including Reverse Scan & Pay (2002A).\",\n",
    "        \"n_purchase_qr_30\": \"The number of transactions caused by payment via Scan & Pay with Transaction Type QR 30, not including Reverse Scan & Pay (2002A).\",\n",
    "        \"n_purchase_qr_cs\": \"The number of transactions caused by payment via Scan & Pay with Transaction Type QRCS excluding Reverse Scan & Pay (2002A).\",\n",
    "        \"n_point_payment_p_only\": \"The number of transactions caused by payment by Payment Method is to use Point Only, not including Reverse Scan & Pay (2002A), Reverse Paywise (2004A) and Cancel Order (7015A) of X-Store.\",\n",
    "        \"n_point_payment_p_casa\": \"The number of transactions caused by payment by Payment Method is to use POINT, plus the payment of accounting does not include the Reverse Scan & Pay (2002A), Reverse Paywise (2004A) and Cancel (7015A) of X-Store.\",\n",
    "        \"n_point_payment_p_cc\": \"The number of transactions caused by payment by Payment Method is to use POINT, combined with credit card payments, not including Reverse Scan & Pay (2002A), Reverse Paywise (2004A) and Cancel Order (7015A) of X-STORE.\",\n",
    "        \"mtd1_ncust_user\": \"All the number of Customer in the systemAnd can use Point XPP\",\n",
    "        \"mtd1_ncust_pointx\": \"Point X Customers, both Customer Type, Easy and Non Easy, not including Guest.\",\n",
    "        \"mtd1_ncust_visit\": \"All the number of Customer in the systemAnd use Point XPP from 1 of the month to the latest transaction information\",\n",
    "        \"mtd1_ncust_pointx_visit\": \"The number of point x Customers, both Customer Type, is Easy and Non Easy, not including the Guest that has been active from the 1st day of the month to the latest transaction data.\",\n",
    "        \"mtd1_ncust_pointx_financial\": \"The number of point x Customers, both Customer Type, is Easy and Non Easy, not including the Guest at the financial transaction (broken program at Reverse) from 1 of the month to the latest transaction data.\",\n",
    "        \"mtd1_ncust_guest\": \"The number of point x Customers, both Customer Type is Guest.\",\n",
    "        \"mtd1_ncust_guest_visit\": \"The number of point x Customers, both Customer Type, is a guest that comes to use Point XP from the 1st day to the latest transaction data.\",\n",
    "        \"mtd1_amt_point_topup\": \"The number of points caused by topups from 1 of the month to the latest transaction day.\",\n",
    "        \"mtd1_amt_point_topup_auto\": \"Auto Converse Point Topup from 1 of the month until the latest transaction\",\n",
    "        \"mtd1_amt_point_topup_onboard\": \"Point Topup number onboard from 1 of the month until the latest transaction\",\n",
    "        \"mtd1_amt_point_topup_onetime\": \"Manual number points from 1 of the month until the latest transaction\",\n",
    "        \"mtd1_amt_point_transfer_out\": \"The number of points that transferred to other users from 1 of the month until the latest transaction\",\n",
    "        \"mtd1_amt_point_pay\": \"The number of points caused by Payment from the 1st day of the month until the latest transaction.\",\n",
    "        \"mtd1_amt_point_pay_sku\": \"The number of points caused by payment via X-Store from 1 of the month until the latest transaction day.\",\n",
    "        \"mtd1_amt_point_pay_qr\": \"The number of points caused by payment via Scan & Pay from 1 of the month until the latest transaction.\",\n",
    "        \"mtd1_amt_point_pay_pyw\": \"The number of points caused by payment via Paywise from the 1st day of the month until the latest transaction.\",\n",
    "        \"mtd1_amt_point_pay_pyw_rbh\": \"The number of points caused by payment via Robinhood Platform from the 1st day of the month until the latest transaction.\",\n",
    "        \"mtd1_amt_point_pay_qr_29\": \"The number of points caused by payment via Scan & Pay with Transaction Type type QR 29 from the 1st day of the month to the latest transaction.\",\n",
    "        \"mtd1_amt_point_pay_qr_30\": \"The number of points caused by payment via Scan & Pay with Transaction Type QR 30 from 1 day of the month until the latest transaction.\",\n",
    "        \"mtd1_amt_point_pay_qr_cs\": \"The number of points caused by payment via Scan & Pay with Transaction Type QRCS from 1 day of the month until the latest transaction.\",\n",
    "        \"mtd1_amt_point_payment_p_only\": \"The number of points caused by payment by Payment Method is the use of Point Only from 1 day of the month until the latest transaction.\",\n",
    "        \"mtd1_amt_point_payment_p_casa\": \"The number of points caused by payment by Payment Method is the use of Point, plus payment by transferring money.From the 1st day of the month until the latest transaction\",\n",
    "        \"mtd1_amt_point_payment_p_cc\": \"The number of points caused by payment by Payment Method is to use Point, plus payment through credit cards.From the 1st day of the month until the latest transaction\",\n",
    "        \"mtd1_rate_point_per_baht_pay\": \"The average rate of all the customers from 1 of the month until the latest transaction\",\n",
    "        \"mtd1_rate_point_per_baht_pay_sku\": \"The rate of the Rate that Payment customers via X-Store from 1 of the month until the latest transaction.\",\n",
    "        \"mtd1_rate_point_per_baht_pay_qr\": \"The rate of the Rate that Payment customers via Scan & Pay from 1 of the month until the latest transaction.\",\n",
    "        \"mtd1_rate_point_per_baht_pay_pyw\": \"The rate of the Rate that Payment customers via Paywise from 1 of the month until the latest transaction.\",\n",
    "        \"mtd1_rate_point_per_baht_pay_pyw_rbh\": \"The rate of the Rate that Payment customers via Robinhood Platform from 1 of the month until the latest transaction.\",\n",
    "        \"mtd1_rate_point_per_baht_pay_qr_29\": \"The average rate of the customer Payment with Transaction Type type QR 29 from 1 day of the month until the latest transaction.\",\n",
    "        \"mtd1_rate_point_per_baht_pay_qr_30\": \"The average rate of the customer Payment with Transaction Type type QR 30 from 1 day of the month until the latest transaction.\",\n",
    "        \"mtd1_rate_point_per_baht_pay_qr_cs\": \"The average rate of the customer Payment with Transaction Type type QRCS from the 1st day of the month to the latest transaction.\",\n",
    "        \"mtd1_rate_point_per_baht_pay_weight\": \"The average rate of all Payment customers (Weigthed Average with Point AMOUNT) from 1 day of the month until the latest transaction.\",\n",
    "        \"mtd1_rate_point_per_baht_pay_sku_weight\": \"The average rate of the customer Payment via X-Store (Weigthed Average with Point AMOUNT) from 1 day of the month until the latest transaction.\",\n",
    "        \"mtd1_rate_point_per_baht_pay_qr_weight\": \"The rate of the Rate that Payment customers via Scan & Pay (Weigted Average with Point Amount paid from 1 day of the month until the latest transaction.\",\n",
    "        \"mtd1_rate_point_per_baht_pay_pyw_weight\": \"The average rate of the Payment customers via Paywise (Weigthed Average with Point AMOUNT) from 1 day of the month until the latest transaction.\",\n",
    "        \"mtd1_rate_point_per_baht_pay_pyw_rbh_weight\": \"The average rate of the Payment customers via Robinhood Platform (Weigthed Average with Point AMOUNT) from 1 day of the month to the latest transaction.\",\n",
    "        \"mtd1_rate_point_per_baht_pay_qr_29_weight\": \"The average rate of the customer Payment with Transaction Type type QR 29 (Weigthed Average with Point Amount paid from 1 day of the month to the latest transaction.\",\n",
    "        \"mtd1_rate_point_per_baht_pay_qr_30_weight\": \"The average rate of the customer Payment with Transaction Type type QR 30 (Weigthed Average with Point Amount paid from 1 day of the month until the latest transaction.\",\n",
    "        \"mtd1_rate_point_per_baht_pay_qr_cs_weight\": \"The rate of the Rate that the customer payment with the type of transaction type QRCS (Weigted Average with Point Amount paid from 1 day of the month to the latest transaction date.\",\n",
    "        \"mtd1_rate_baht_per_point_pay\": \"The average of the Cost Per Point at all Payment customers from 1 of the month until the latest transaction day.\",\n",
    "        \"mtd1_rate_baht_per_point_pay_sku\": \"The Cost Per Point, the Payment customers via X-Store from 1 of the month until the latest transaction.\",\n",
    "        \"mtd1_rate_baht_per_point_pay_qr\": \"The average of Cost Per Point at Payment customers via Scan & Pay from 1 of the month until the latest transaction.\",\n",
    "        \"mtd1_rate_baht_per_point_pay_pyw\": \"The average of the Cost Per Point that Payment customers via Paywise from 1 day of the month until the latest transaction.\",\n",
    "        \"mtd1_rate_baht_per_point_pay_pyw_rbh\": \"The average of the Cost Per Point that Payment customers via Robinhood Platform from 1 day of the month until the latest transaction.\",\n",
    "        \"mtd1_rate_baht_per_point_pay_qr_29\": \"The average of Cost Per Point at Payment customers with Transaction Type type QR 29 from the 1st day of the month until the latest transaction.\",\n",
    "        \"mtd1_rate_baht_per_point_pay_qr_30\": \"The average of Cost Per Point at Payment customers with Transaction Type type QR 30 from the 1st day of the month until the latest transaction.\",\n",
    "        \"mtd1_rate_baht_per_point_pay_qr_cs\": \"The average of Cost Per Point at Payment customers with Transaction Type type QRCS from the 1st day of the month to the latest transaction.\",\n",
    "        \"mtd1_rate_baht_per_point_pay_weight\": \"The average of the Cost Per Point at all Payment customers (Weigted Average with Point Amount paid from 1 day of the month until the latest transaction.\",\n",
    "        \"mtd1_rate_baht_per_point_pay_sku_weight\": \"The average of the Cost Per Point that the Payment customers via X-Store (Weigthed Average with Point AMOUNT) from 1 day of the month to the latest transaction.\",\n",
    "        \"mtd1_rate_baht_per_point_pay_qr_weight\": \"The average of Cost Per Point at Payment customers via Scan & Pay (Weigted Average with Point Amount paid from 1 day of the month until the latest transaction.\",\n",
    "        \"mtd1_rate_baht_per_point_pay_pyw_weight\": \"The average of the Cost Per Point at Paywise (Weigthed Average with Point AMOUNT) from 1 day of the month until the latest transaction.\",\n",
    "        \"mtd1_rate_baht_per_point_pay_pyw_rbh_weight\": \"The average of the Cost Per Point that Payment customers via Robinhood Platform (Weigthed Average with Point AMOUNT) from 1 day of the month until the latest transaction.\",\n",
    "        \"mtd1_rate_baht_per_point_pay_qr_29_weight\": \"The average of Cost Per Point at Payment Customers with Transaction Type type QR 29 (Weigted Average with Point AMOUNT) from 1 day of the month until the latest transaction.\",\n",
    "        \"mtd1_rate_baht_per_point_pay_qr_30_weight\": \"The average of the Cost Per Point at Payment Customers with Transaction Type type QR 30 (Weigted Average with Point AMOUNT) from 1 day of the month until the latest transaction.\",\n",
    "        \"mtd1_rate_baht_per_point_pay_qr_cs_weight\": \"The average of Cost Per Point at Payment Customers with Transaction Type type QRCS (Weigthed Average with Point Amount paid from 1 day of the month until the latest transaction.\",\n",
    "        \"mtd1_n_topup_point\": \"The number of transactions caused by topups from 1 of the month to the latest transaction day.\",\n",
    "        \"mtd1_n_topup_point_onboard\": \"The number of Transaction Topup onboard from 1 of the month until the latest transaction\",\n",
    "        \"mtd1_n_topup_point_onetime\": \"Number of Transaction Topup Manual from 1 of the month until the latest transaction\",\n",
    "        \"mtd1_n_topup_point_auto\": \"Auto converse transaction topg from 1 of the month until the latest transaction\",\n",
    "        \"mtd1_n_transfer_point_out\": \"The number of transactions that transferred to other users from 1 of the month until the latest transaction\",\n",
    "        \"mtd1_n_purchase\": \"The number of transactions caused by payment from 1 day of the month to the latest transaction.\",\n",
    "        \"mtd1_n_purchase_sku\": \"The number of transactions caused by payment via X-Store from 1 of the month until the latest transaction.\",\n",
    "        \"mtd1_n_purchase_qr\": \"The number of transactions caused by payment via Scan & Pay from the 1st day of the month until the latest transaction.\",\n",
    "        \"mtd1_n_purchase_pyw\": \"The number of transactions caused by payment via Paywise from the 1st day of the month until the latest transaction.\",\n",
    "        \"mtd1_n_purchase_pyw_rbh\": \"The number of transactions caused by payment via Robinhood Platform from 1 day of the month until the latest transaction.\",\n",
    "        \"mtd1_n_purchase_qr_29\": \"The number of transactions caused by payment via Scan & Pay with Transaction Type type QR 29 from the 1st day of the month to the latest transaction.\",\n",
    "        \"mtd1_n_purchase_qr_30\": \"The number of transactions caused by payment via Scan & Pay with Transaction Type QR 30 from 1 day of the month until the latest transaction.\",\n",
    "        \"mtd1_n_purchase_qr_cs\": \"The number of transactions caused by payment via Scan & Pay with Transaction Type QRCS from 1 day of the month until the latest transaction.\",\n",
    "        \"mtd1_n_point_payment_p_only\": \"The number of transactions caused by payment by Payment Method is the use of Point Only from the 1st day of the month until the latest transaction.\",\n",
    "        \"mtd1_n_point_payment_p_casa\": \"The number of transactions caused by payment by Payment Method is the use of Point, plus payment by transferring money.From the 1st day of the month until the latest transaction\",\n",
    "        \"mtd1_n_point_payment_p_cc\": \"The number of transactions caused by payment by Payment Method is to use Point, plus payment through credit cards.From the 1st day of the month until the latest transaction\",\n",
    "        \"amt_point_topup_auto_cardx\": \"Point Topup number onboard of Cardx customers\",\n",
    "        \"amt_point_topup_auto_wealth\": \"Point Topup number onboard of wealth customers\",\n",
    "        \"amt_point_topup_extnl\": \"Point Topup number via External Partner\",\n",
    "        \"mtd1_amt_point_topup_auto_cardx\": \"The Auto Converse Point Topup number of Cardx customers from 1 of the month until the latest transaction.\",\n",
    "        \"mtd1_amt_point_topup_auto_wealth\": \"The Auto Converse Point Topup number of Wealth customers from 1 of the month until the latest transaction.\",\n",
    "        \"mtd1_amt_point_topup_extnl\": \"Point Topup number via partner from 1 of the month until the latest transaction day\",\n",
    "        \"mtd1_n_topup_point_auto_cardx\": \"The number of transactions caused by the Auto Converse top of the Cardx customer group from 1 of the month until the latest transaction.\",\n",
    "        \"mtd1_n_topup_point_auto_wealth\": \"The number of transactions caused by the Auto Converse top of the Wealth customer group from 1 of the month until the latest transaction.\",\n",
    "        \"mtd1_n_topup_point_extnl\": \"The number of transactions caused by the auto converse top via partner from the 1st day of the month to the latest transaction.\",\n",
    "        \"mtd1_ncust_partner\": \"The number of users obtained from partner companiesCollecting the 1st day of the month\",\n",
    "        \"mtd1_ncust_partner_new\": \"The number of users that register comes from partner companies.Collecting the 1st day of the month\",\n",
    "        \"n_topup_point_auto_cardx\": \"The number of Transaction Topup Auto Converse of Cardx customers\",\n",
    "        \"n_topup_point_auto_wealth\": \"The number of Transaction Topup Auto Converse of Wealth Customers\",\n",
    "        \"n_topup_point_extnl\": \"Number of Transaction Topup via External Partner\",\n",
    "        \"ncust_partner\": \"The number of users obtained from the partner company that day\",\n",
    "        \"ncust_partner_new\": \"The number of users that register came from the partner company that day.\",\n",
    "        \"n_transfer_point_out_extnl\": \"The number of transactions that transferred to other users via External Partner.\",\n",
    "        \"mtd1_n_transfer_point_out_extnl\": \"The number of transactions transferred to other users via Partner from the 1st day until the latest transaction.\",\n",
    "        \"amt_point_transfer_out_extnl\": \"The number of points that transferred to other User via External Partner\",\n",
    "        \"mtd1_amt_point_transfer_out_extnl\": \"The number of points that transferred to other users via Partner from 1 of the month until the latest transaction.\",\n",
    "        \"_dl_load_ts\": \"Data download date\",\n",
    "        \"_date\": \"Transaction date\"\n",
    "}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pointx_fbs_rpt_dly_context = \"\"\"\n",
    "{\n",
    "    \"table\": \"pointx_fbs_rpt_dly\",\n",
    "    \"description\": \"Table records user interactions with the PointX app daily, capturing events such as app opens and deletions, \\nproviding key insights into user behavior, app version usage, and device characteristics \",\n",
    "    \"columns\": {\n",
    "        \"event_date\": \"The date on which the event was logged (YYYYMMDD format in the registered timezone of your app).\",\n",
    "        \"event_month\": \"The year month on which the event was logged (YYYY-MM format).\",\n",
    "        \"event_bundle_sequence_id\": \"The sequential ID of the bundle in which these events were uploaded.\",\n",
    "        \"event_timestamp\": \"The time (in microseconds, UTC) at which the event was logged on the client.\",\n",
    "        \"event_name\": \"The name of the event activity that occurred from the user's use.\",\n",
    "        \"customer_id\": \"customer identification same values as pointx id in application\",\n",
    "        \"user_pseudo_id\": \"The pseudonymous id (e.g., app instance ID) for the user.\",\n",
    "        \"user_id\": \"The user ID set via the setUserId API.\",\n",
    "        \"event_previous_timestamp\": \"The time (in microseconds, UTC) at which the event was previously logged on the client.\",\n",
    "        \"event_value_in_usd\": \"The currency-converted value (in USD) of the event's \\\"value\\\" parameter.\",\n",
    "        \"event_server_timestamp_offset\": \"Timestamp offset between collection time and upload time in micros.\",\n",
    "        \"privacy_info_analytics_storage\": \"Whether Analytics storage is enabled for the user.\",\n",
    "        \"privacy_info_ads_storage\": \"Whether ad targeting is enabled for a user.\",\n",
    "        \"privacy_info_uses_transient_token\": \"Whether a web user has denied Analytics storage and the developer has enabled measurement without cookies based on transient tokens in server data.\",\n",
    "        \"user_properties_ga_session_number\": \"Session number identifies the number of sessions that a user has started up to the current session (e.g., a user's third or fifth session on your site).\",\n",
    "        \"user_properties_ga_session_number_set_timestamp_micros\": \"Timestamp of sessions that a user has started up to the current session (e.g., a user's third or fifth session on your site).\",\n",
    "        \"user_properties_ga_session_id\": \"Session ID identifies the session that an event came from. For example, two different session IDs are generated when a user has two separate sessions on your site.\",\n",
    "        \"user_properties_ga_session_id_set_timestamp_micros\": \"Timestamp that an event came from. For example, two different session IDs are generated when a user has two separate sessions on your site.\",\n",
    "        \"user_properties_first_open_time\": \"Timestamp of user's first open\",\n",
    "        \"user_properties_first_open_time_set_timestamp_micros\": \"Timestamp of user's first open in micros timestamp\",\n",
    "        \"user_first_touch_timestamp\": \"Timestamp of user's first touch\",\n",
    "        \"user_ltv_revenue\": \"The Lifetime Value (revenue) of the user. This field is not populated in intraday tables.\",\n",
    "        \"user_ltv_currency\": \"The Lifetime Value (currency) of the user. This field is not populated in intraday tables.\",\n",
    "        \"device_category\": \"The device category (mobile, tablet, desktop).\",\n",
    "        \"device_mobile_brand_name\": \"The device brand name.\",\n",
    "        \"device_mobile_model_name\": \"The device model name.\",\n",
    "        \"device_mobile_marketing_name\": \"The device marketing name.\",\n",
    "        \"device_mobile_os_hardware_model\": \"The device model information retrieved directly from the operating system.\",\n",
    "        \"device_operating_system\": \"The operating system of the device.\",\n",
    "        \"device_operating_system_version\": \"The OS version.\",\n",
    "        \"device_vendor_id\": \"IDFV (present only if IDFA is not collected).\",\n",
    "        \"device_advertising_id\": \"Advertising ID/IDFA.\",\n",
    "        \"device_language\": \"The OS language.\",\n",
    "        \"device_is_limited_ad_tracking\": \"The device's Limit Ad Tracking setting.\",\n",
    "        \"device_time_zone_offset_seconds\": \"The offset from GMT in seconds.\",\n",
    "        \"device_browser\": \"The browser in which the user viewed content.\",\n",
    "        \"device_browser_version\": \"The version of the browser in which the user viewed content.\",\n",
    "        \"device_web_info_browser\": \"The browser in which the user viewed content\",\n",
    "        \"device_web_info_browser_version\": \"The version of the browser in which the user viewed content.\",\n",
    "        \"device_web_info_hostname\": \"The hostname associated with the logged event.\",\n",
    "        \"geo_continent\": \"The continent from which events were reported, based on IP address.\",\n",
    "        \"geo_country\": \"The country from which events were reported, based on IP address.\",\n",
    "        \"geo_region\": \"The region from which events were reported, based on IP address.\",\n",
    "        \"geo_city\": \"The city from which events were reported, based on IP address.\",\n",
    "        \"geo_sub_continent\": \"The subcontinent from which events were reported, based on IP address.\",\n",
    "        \"geo_metro\": \"The metro from which events were reported, based on IP address.\",\n",
    "        \"app_info_id\": \"The package name or bundle ID of the app.\",\n",
    "        \"app_info_version\": \"The app's versionName (Android) or short bundle version.\",\n",
    "        \"app_info_install_store\": \"The store that installed the app.\",\n",
    "        \"app_info_firebase_app_id\": \"The Firebase App ID associated with the app\",\n",
    "        \"app_info_install_source\": \"The source that installed the app.\",\n",
    "        \"traffic_source_name\": \"Name of the marketing campaign that first acquired the user. This field is not populated in intraday tables.\",\n",
    "        \"traffic_source_medium\": \"Name of the medium (paid search, organic search, email, etc.) that first acquired the user. This field is not populated in intraday tables.\",\n",
    "        \"traffic_source_source\": \"Name of the network that first acquired the user. This field is not populated in intraday tables.\",\n",
    "        \"stream_id\": \"The numeric ID of the stream.\",\n",
    "        \"platform\": \"The platform on which the app was built.\",\n",
    "        \"event_dimensions_hostname\": \"Includes the subdomain and domain names of a URL; for example, the Host Name of www.example.com/contact.html is www.example.com.\",\n",
    "        \"ecommerce\": \"A record of information about ecommerce. (Currenntly collect as string)\",\n",
    "        \"items\": \"A repeated record of items included in this event. (Currenntly collect as string)\",\n",
    "        \"source_date\": \"Source date of firebase file name\",\n",
    "        \"address_id\": \"Delivery address identification\",\n",
    "        \"auto_earn_display\": \"In case of event delete account the values of auto_earn flag\",\n",
    "        \"banner_description\": \"Highlight banner description\",\n",
    "        \"banner_rank\": \"Highlight banner item index\",\n",
    "        \"banner_title\": \"Highlight banner title\",\n",
    "        \"campaign\": \"Firebase's campagin name\",\n",
    "        \"campaign_info_source\": \"Firebase's campagin source information\",\n",
    "        \"card_sub_product\": \"Possible Values : SCB Beyond , SCB Toyota platinum\",\n",
    "        \"change_language\": \"Possible Values : EN, TH\",\n",
    "        \"coupon_id\": \"Coupon Identification Number\",\n",
    "        \"customer_device_lat\": \"Customer's device latitude\",\n",
    "        \"customer_device_long\": \"Customer's device longtitude\",\n",
    "        \"customer_lat\": \"Customer's latitude\",\n",
    "        \"customer_long\": \"Customer's longtitude\",\n",
    "        \"customer_type\": \"Customer Type e.g. guest, N/A\",\n",
    "        \"deal_title\": \"Deal of the day title\",\n",
    "        \"deal_type\": \"Deal of the day type\",\n",
    "        \"debug_event\": \"Debug event\",\n",
    "        \"deleteacount_button\": \"Delete account button\",\n",
    "        \"delivery_address\": \"Delivery address \",\n",
    "        \"delivery_fee\": \"Delivery fee e.g. Fee \",\n",
    "        \"delivery_option\": \"Delivery option e.g. Standard Shipping \",\n",
    "        \"delivery_type\": \"Delivery type e.g. Standard Shipping \",\n",
    "        \"e_coupon_display\": \"e coupon display\",\n",
    "        \"each_point_card\": \"Point in each credit cards, This value in array format\",\n",
    "        \"ecatalog_list\": \"ecatalog list\",\n",
    "        \"ecatalog_rank\": \"ecatalog rank\",\n",
    "        \"ecoupon_rank\": \"ecoupon rank\",\n",
    "        \"ecoupon_title\": \"ecoupon title\",\n",
    "        \"engaged_session_event\": \"Engaged session event\",\n",
    "        \"engagement_time_msec\": \"Engagement time millisecond\",\n",
    "        \"entrances\": \"Number of entrance\",\n",
    "        \"error_message\": \"Error message\",\n",
    "        \"error_value\": \"Error value\",\n",
    "        \"event_id\": \"Event Identification\",\n",
    "        \"fatal\": \"Fatal\",\n",
    "        \"firebase_conversion\": \"Firebase conversion\",\n",
    "        \"firebase_error\": \"Firebase error\",\n",
    "        \"firebase_event_origin\": \"Firebase event origin\",\n",
    "        \"firebase_previous_class\": \"Firebase previous class\",\n",
    "        \"firebase_previous_id\": \"Firebase previous identification\",\n",
    "        \"firebase_previous_screen\": \"Firebase previous screen\",\n",
    "        \"firebase_screen\": \"Firebase screen\",\n",
    "        \"firebase_screen_class\": \"Firebase screen claass\",\n",
    "        \"firebase_screen_id\": \"Firebase screen identification\",\n",
    "        \"flashdeals_rank\": \"Flash deals rank\",\n",
    "        \"flashdeals_title\": \"Flash deals title\",\n",
    "        \"from_customer_name\": \"Full customer name who transferred point to another customer\",\n",
    "        \"from_customer_profile_name\": \"Customer profile name in application  who transferred point to another customer\",\n",
    "        \"ga_session_id\": \"Session Identification\",\n",
    "        \"ga_session_number\": \"Sesion Number\",\n",
    "        \"id\": \"Category Identification\",\n",
    "        \"ignore_referrer\": \"Ignore referer\",\n",
    "        \"item_code\": \"Item code\",\n",
    "        \"latitude\": \"Latitude\",\n",
    "        \"longitude\": \"Longitude\",\n",
    "        \"link_classes\": \"Link classes\",\n",
    "        \"link_domain\": \"Link domain\",\n",
    "        \"link_url\": \"Link url\",\n",
    "        \"list_card_sub_product\": \"List card sub product\",\n",
    "        \"list_each_point_card\": \"List each point card\",\n",
    "        \"medium\": \"Medium\",\n",
    "        \"merchant_id\": \"Merchant id\",\n",
    "        \"message_type\": \"Message type\",\n",
    "        \"offer_type\": \"Offer type\",\n",
    "        \"order_id\": \"Order id\",\n",
    "        \"order_status\": \"Order status\",\n",
    "        \"outbound\": \"Outbound\",\n",
    "        \"page\": \"Page\",\n",
    "        \"page_location\": \"Page location\",\n",
    "        \"page_referrer\": \"Page referrer\",\n",
    "        \"page_title\": \"Page title\",\n",
    "        \"payment_method\": \"Payment method\",\n",
    "        \"percent_scrolled\": \"Percent scrolled\",\n",
    "        \"place_id\": \"Place id\",\n",
    "        \"place_lat\": \"Place lat\",\n",
    "        \"place_long\": \"Place long\",\n",
    "        \"place_name\": \"Place name\",\n",
    "        \"point_balance_display\": \"Point balance display\",\n",
    "        \"points\": \"Points\",\n",
    "        \"points_per_unit\": \"Points per unit\",\n",
    "        \"points_remaining\": \"Points remaining\",\n",
    "        \"previous_app_version\": \"Previous app version\",\n",
    "        \"previous_first_open_count\": \"Previous first open count\",\n",
    "        \"previous_os_version\": \"Previous os version\",\n",
    "        \"primary_address\": \"Primary address\",\n",
    "        \"product_id\": \"Product id\",\n",
    "        \"quantity\": \"Quantity\",\n",
    "        \"reason_id\": \"Reason id\",\n",
    "        \"recommendedForYou_rank\": \"RecommendedForYou rank\",\n",
    "        \"search_list_id\": \"Search list id\",\n",
    "        \"search_list_id_scan_and_pay\": \"Search list id scan and pay\",\n",
    "        \"search_list_id_xstore\": \"Search list id xstore\",\n",
    "        \"session_engaged\": \"Session engaged\",\n",
    "        \"shop_list_id\": \"Shop list id\",\n",
    "        \"sku_catagory_name\": \"Sku catagory name\",\n",
    "        \"sku_group_type\": \"Sku group type\",\n",
    "        \"sku_id\": \"The SKU (Stock Keeping Unit) ID serves as a unique identifier assigned to a specific product variant, enabling efficient inventory management and tracking within a retail or e-commerce system.\",\n",
    "        \"source\": \"Source\",\n",
    "        \"source_page_name\": \"Source page name\",\n",
    "        \"stock_code\": \"Stock code\",\n",
    "        \"system_app\": \"System app\",\n",
    "        \"system_app_update\": \"System app update\",\n",
    "        \"tab_name\": \"Tab name\",\n",
    "        \"term\": \"Term\",\n",
    "        \"text_search\": \"Text search\",\n",
    "        \"timestamp\": \"Timestamp\",\n",
    "        \"to_customer_name\": \"To customer name\",\n",
    "        \"to_customer_profile_name\": \"To customer profile name\",\n",
    "        \"toggle\": \"Toggle\",\n",
    "        \"total_amount\": \"Total amount\",\n",
    "        \"total_point\": \"Total point\",\n",
    "        \"total_points\": \"Total points\",\n",
    "        \"transaction_id\": \"Transaction id\",\n",
    "        \"transaction_status\": \"Transaction status\",\n",
    "        \"transaction_type\": \"Transaction type\",\n",
    "        \"unable_to_proceed\": \"Unable to proceed\",\n",
    "        \"update_with_analytics\": \"Update with analytics\",\n",
    "        \"client_code\": \"Client code\",\n",
    "        \"client_member_id\": \"Client member id\",\n",
    "        \"_dl_load_ts\": \"Date of data loading\",\n",
    "        \"_date\": \"Transaction's occurrence date\"\n",
    "    }\n",
    "}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_schemalink_template = \"\"\"Your task is to select the columns related to the question to create the sql query in the next step.\n",
    "Please selected related columns following by the question from this schema.\n",
    "Answer the list of columns that you think are relevant and sufficient to create sql.\n",
    "Example result : ['column1', 'column3']\n",
    "{domain_knowledge}\n",
    "\n",
    "Question: {question}\n",
    "Result:\"\"\"\n",
    "\n",
    "schemalink_prompt = PromptTemplate(\n",
    "        input_variables=[\"domain_knowledge\",\"question\"],\n",
    "        template = llm_schemalink_template                      \n",
    ")\n",
    "\n",
    "schemalink_chain = LLMChain(\n",
    "        llm=ChatOpenAI(temperature=0, model=model, request_timeout=120),\n",
    "        prompt=schemalink_prompt,\n",
    "        output_key=\"result\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "measurement_data = {\n",
    "    \"Question\" : [],\n",
    "    \"Actual result\" : [],\n",
    "    \"Schemalink result\" : [],\n",
    "    \"LLM result\" : [],\n",
    "    \"SL recall\" : [],\n",
    "    \"SL precision\" : [],\n",
    "    \"SL f1\" : [],\n",
    "    \"LLM recall\" : [],\n",
    "    \"LLM precision\" : [],\n",
    "    \"LLM f1\" : []\n",
    "}\n",
    "for i, row in pointx_nlqsql_df.iterrows():\n",
    "    \n",
    "    if row['Table'] == 'pointx_keymatrix_dly':\n",
    "        domain_context = pointx_keymatrix_dly_context\n",
    "    elif row['Table'] == 'pointx_fbs_rpt_dly':\n",
    "        domain_context = pointx_fbs_rpt_dly_context\n",
    "\n",
    "    actual_columns = SQL_columns(row['Actual SQL'], row['Table'])\n",
    "    module_result = list(schema_link.filter_schema(row['Question'])[row['Table']].keys())\n",
    "    llm_result = ast.literal_eval(schemalink_chain({\"domain_knowledge\":domain_context,\"question\":row['Question']})['result'])\n",
    "\n",
    "    module_recall, module_precision, module_f1 = f1_columns(actual_columns, module_result)\n",
    "    llm_recall, llm_precision, llm_f1 = f1_columns(actual_columns, llm_result)\n",
    "\n",
    "    measurement_data['Question'].append(row['Question'])\n",
    "    measurement_data[\"Actual result\"].append(actual_columns)\n",
    "    measurement_data[\"Schemalink result\"].append(module_result)\n",
    "    measurement_data[\"LLM result\"].append(llm_result)\n",
    "    measurement_data[\"SL recall\"].append(module_recall)\n",
    "    measurement_data[\"SL precision\"].append(module_precision)\n",
    "    measurement_data[\"SL f1\"].append(module_f1)\n",
    "    measurement_data['LLM recall'].append(llm_recall)\n",
    "    measurement_data['LLM precision'].append(llm_precision)\n",
    "    measurement_data['LLM f1'].append(llm_f1)\n",
    "\n",
    "    print(actual_columns)\n",
    "    print(module_result)\n",
    "    # print(module_recall, module_precision, module_f1)\n",
    "    print(llm_result)\n",
    "    # print(llm_recall, llm_precision, llm_f1)\n",
    "    print()\n",
    "\n",
    "    if not i % 5 and i > 1:\n",
    "        print(f\"Complete {i} of {pointx_nlqsql_df.shape[0]}\")\n",
    "        time.sleep(60)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "measurement_df = pd.DataFrame(measurement_data)\n",
    "merged_df = pd.merge(pointx_nlqsql_df, measurement_df, on='Question', how='outer')\n",
    "\n",
    "merged_df.to_excel('EXP_LLM_schemalink.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# splite chunk of question by schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_schemalink_loq_template = \"\"\"Your task is to select the columns related to the question to create the sql query in the next step.\n",
    "Please selected related columns following by the question from this schema.\n",
    "Answer the list of columns that you think are relevant and sufficient to create sql.\n",
    "If you're not sure which column to choose when the columns are similar, You can select those columns.\n",
    "The input will be a list of questions, and the answers you give will be a list of columns that list the answers to each question in order.\n",
    "\n",
    "Example List of questions : ['question1', 'question2', 'question3', 'question4']\n",
    "Example Result : [['column1', 'column3'], ['column2', 'column4', 'column5'], ['column1'], ['column6', 'column3']]\n",
    "\n",
    "Schema: {domain_knowledge}\n",
    "\n",
    "List of questions: {list_of_questions}\n",
    "Result:\"\"\"\n",
    "\n",
    "schemalink_loq_prompt = PromptTemplate(\n",
    "        input_variables=[\"domain_knowledge\",\"list_of_questions\"],\n",
    "        template = llm_schemalink_loq_template                      \n",
    ")\n",
    "\n",
    "schemalink_chain = LLMChain(\n",
    "        llm=ChatOpenAI(temperature=0, model=model, request_timeout=120),\n",
    "        prompt=schemalink_loq_prompt,\n",
    "        output_key=\"result\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dict_append_val(d, table, list_of_questions, list_of_actual_columns):\n",
    "    if table not in d.keys():\n",
    "        d[table] = {'loq' : [], 'loc' : [] }\n",
    "\n",
    "    d[table]['loq'].extend(list_of_questions)\n",
    "    d[table]['loc'].extend(list_of_actual_columns)\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "measurement_data = {\n",
    "    \"Question\" : [],\n",
    "    \"Actual result\" : [],\n",
    "    \"Schemalink result\" : [],\n",
    "    \"LLM result\" : [],\n",
    "    \"SL recall\" : [],\n",
    "    \"SL precision\" : [],\n",
    "    \"SL f1\" : [],\n",
    "    \"LLM recall\" : [],\n",
    "    \"LLM precision\" : [],\n",
    "    \"LLM f1\" : []\n",
    "}\n",
    "\n",
    "llm_loq = list()\n",
    "list_of_actualcols = list()\n",
    "table_loq_dict = dict()\n",
    "\n",
    "for i, row in pointx_nlqsql_df.iterrows():\n",
    "\n",
    "    # first time\n",
    "    if not i : temp_domain = row['Table']\n",
    "\n",
    "    actual_columns = SQL_columns(row['Actual SQL'], row['Table'])\n",
    "    module_result = list(schema_link.filter_schema(row['Question'])[row['Table']].keys())\n",
    "    module_recall, module_precision, module_f1 = f1_columns(actual_columns, module_result)\n",
    "    \n",
    "    if temp_domain != row['Table']:\n",
    "        table_loq_dict = dict_append_val(table_loq_dict, temp_domain, llm_loq, list_of_actualcols)\n",
    "        temp_domain = row['Table']\n",
    "        list_of_actualcols = [actual_columns]\n",
    "        llm_loq = [row['Question']]\n",
    "    else:\n",
    "        llm_loq.append(row['Question'])\n",
    "        list_of_actualcols.append(actual_columns)\n",
    "\n",
    "    if not (pointx_nlqsql_df.shape[0] - 1 - i):   # last record\n",
    "        table_loq_dict = dict_append_val(table_loq_dict, temp_domain, llm_loq, list_of_actualcols)\n",
    "\n",
    "    measurement_data['Question'].append(row['Question'])\n",
    "    measurement_data[\"Actual result\"].append(actual_columns)\n",
    "    measurement_data[\"Schemalink result\"].append(module_result)\n",
    "    measurement_data[\"SL recall\"].append(module_recall)\n",
    "    measurement_data[\"SL precision\"].append(module_precision)\n",
    "    measurement_data[\"SL f1\"].append(module_f1)\n",
    "    # print(actual_columns)\n",
    "    # print(module_result)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, val in table_loq_dict.items():\n",
    "    print(len(val['loc']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_list(input_list, chunk_size):\n",
    "    for i in range(0, len(input_list), chunk_size):\n",
    "        yield input_list[i:i + chunk_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for table, list_of_value in table_loq_dict.items():\n",
    "    if table == 'pointx_keymatrix_dly':\n",
    "        domain_context = pointx_keymatrix_dly_context\n",
    "    elif table == 'pointx_fbs_rpt_dly':\n",
    "        domain_context = pointx_fbs_rpt_dly_context\n",
    "    loq = list_of_value.get('loq')\n",
    "    loc = list_of_value.get('loc')\n",
    "\n",
    "    chunk_size = 10\n",
    "    loq_chunks = list(split_list(loq, chunk_size))\n",
    "    \n",
    "    for loq_chunk in loq_chunks:\n",
    "        str_list_of_result = schemalink_chain({\"domain_knowledge\": domain_context, \"list_of_questions\": str(loq_chunk)})\n",
    "        llm_result = ast.literal_eval(str_list_of_result['result'])\n",
    "        \n",
    "        for i in range(len(llm_result)):\n",
    "            print(loq_chunk[i],'\\n', llm_result[i],'\\n', loc[i],'\\n')\n",
    "            llm_recall, llm_precision, llm_f1 = f1_columns(loc[i], llm_result[i])\n",
    "            measurement_data[\"LLM result\"].append(llm_result[i])\n",
    "            measurement_data['LLM recall'].append(llm_recall)\n",
    "            measurement_data['LLM precision'].append(llm_precision)\n",
    "            measurement_data['LLM f1'].append(llm_f1)\n",
    "            \n",
    "        time.sleep(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "measurement_df = pd.DataFrame(measurement_data)\n",
    "merged_df = pd.merge(pointx_nlqsql_df, measurement_df, on='Question', how='outer')\n",
    "\n",
    "merged_df.to_excel('EXP_LLM_schemalink.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Score re-check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chech_df = pd.read_excel(\"EXP_LLM_schemalink.xlsx\")\n",
    "chech_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, row in chech_df[['Actual result', 'Schemalink result', 'LLM result']].iterrows():\n",
    "    actual_cols = ast.literal_eval(row['Actual result'])\n",
    "    module_cols = ast.literal_eval(row['Schemalink result'])\n",
    "    llm_cols = ast.literal_eval(row['LLM result'])\n",
    "    \n",
    "    module_recall, module_precision, module_f1 = f1_columns(actual_cols, module_cols)\n",
    "    llm_recall, llm_precision, llm_f1 = f1_columns(actual_cols, llm_cols)\n",
    "\n",
    "    chech_df.at[i, 'SL recall'] = module_recall\n",
    "    chech_df.at[i, 'SL precision'] = module_precision\n",
    "    chech_df.at[i, 'SL f1'] = module_f1\n",
    "    chech_df.at[i, 'LLM recall'] = llm_recall\n",
    "    chech_df.at[i, 'LLM precision'] = llm_precision\n",
    "    chech_df.at[i, 'LLM f1'] = llm_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chech_df.to_excel('EXP_LLM_schemalink.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM full pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Table</th>\n",
       "      <th>Question</th>\n",
       "      <th>Actual SQL</th>\n",
       "      <th>Actual result</th>\n",
       "      <th>Schemalink result</th>\n",
       "      <th>LLM result</th>\n",
       "      <th>SL recall</th>\n",
       "      <th>SL precision</th>\n",
       "      <th>SL f1</th>\n",
       "      <th>LLM recall</th>\n",
       "      <th>LLM precision</th>\n",
       "      <th>LLM f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>pointx_keymatrix_dly</td>\n",
       "      <td>What is the total number of all financial tran...</td>\n",
       "      <td>SELECT month_id, SUM(ntx_pointx_financial) FRO...</td>\n",
       "      <td>['month_id', 'ntx_pointx_financial']</td>\n",
       "      <td>['mtd1_n_transfer_point_out', 'mtd1_n_purchase...</td>\n",
       "      <td>['month_id', 'ntx_pointx_financial']</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.58</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>pointx_keymatrix_dly</td>\n",
       "      <td>What is the total amount of points generated b...</td>\n",
       "      <td>SELECT SUM(amt_point_topup) FROM pointx_keymat...</td>\n",
       "      <td>['month_id', 'amt_point_topup']</td>\n",
       "      <td>['mtd1_amt_point_topup', 'mtd1_n_topup_point',...</td>\n",
       "      <td>['month_id', 'amt_point_topup']</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.28</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>pointx_keymatrix_dly</td>\n",
       "      <td>What is the total amount of points generated b...</td>\n",
       "      <td>SELECT month_id, SUM(amt_point_pay) FROM point...</td>\n",
       "      <td>['month_id', 'amt_point_pay']</td>\n",
       "      <td>['amt_point_pay', 'mtd1_amt_point_pay', 'mtd1_...</td>\n",
       "      <td>['month_id', 'amt_point_pay']</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.28</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>pointx_keymatrix_dly</td>\n",
       "      <td>What is the average rate of released points fo...</td>\n",
       "      <td>SELECT AVG(rate_point_per_baht_pay) FROM point...</td>\n",
       "      <td>['rate_point_per_baht_pay']</td>\n",
       "      <td>['rate_baht_per_point_pay', 'rate_baht_per_poi...</td>\n",
       "      <td>['rate_point_per_baht_pay']</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.34</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>pointx_keymatrix_dly</td>\n",
       "      <td>Can you determine the average number of custom...</td>\n",
       "      <td>SELECT month_id, AVG(ncust_visit) FROM pointx_...</td>\n",
       "      <td>['month_id', 'ncust_visit']</td>\n",
       "      <td>['ncust_visit', 'ncust_pointx_visit', 'mtd1_nc...</td>\n",
       "      <td>['month_id', 'ncust_visit']</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.28</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Table                                           Question  \\\n",
       "0  pointx_keymatrix_dly  What is the total number of all financial tran...   \n",
       "1  pointx_keymatrix_dly  What is the total amount of points generated b...   \n",
       "2  pointx_keymatrix_dly  What is the total amount of points generated b...   \n",
       "3  pointx_keymatrix_dly  What is the average rate of released points fo...   \n",
       "4  pointx_keymatrix_dly  Can you determine the average number of custom...   \n",
       "\n",
       "                                          Actual SQL  \\\n",
       "0  SELECT month_id, SUM(ntx_pointx_financial) FRO...   \n",
       "1  SELECT SUM(amt_point_topup) FROM pointx_keymat...   \n",
       "2  SELECT month_id, SUM(amt_point_pay) FROM point...   \n",
       "3  SELECT AVG(rate_point_per_baht_pay) FROM point...   \n",
       "4  SELECT month_id, AVG(ncust_visit) FROM pointx_...   \n",
       "\n",
       "                          Actual result  \\\n",
       "0  ['month_id', 'ntx_pointx_financial']   \n",
       "1       ['month_id', 'amt_point_topup']   \n",
       "2         ['month_id', 'amt_point_pay']   \n",
       "3           ['rate_point_per_baht_pay']   \n",
       "4           ['month_id', 'ncust_visit']   \n",
       "\n",
       "                                   Schemalink result  \\\n",
       "0  ['mtd1_n_transfer_point_out', 'mtd1_n_purchase...   \n",
       "1  ['mtd1_amt_point_topup', 'mtd1_n_topup_point',...   \n",
       "2  ['amt_point_pay', 'mtd1_amt_point_pay', 'mtd1_...   \n",
       "3  ['rate_baht_per_point_pay', 'rate_baht_per_poi...   \n",
       "4  ['ncust_visit', 'ncust_pointx_visit', 'mtd1_nc...   \n",
       "\n",
       "                             LLM result  SL recall SL precision SL f1  \\\n",
       "0  ['month_id', 'ntx_pointx_financial']        1.0          0.4  0.58   \n",
       "1       ['month_id', 'amt_point_topup']        0.5          0.2  0.28   \n",
       "2         ['month_id', 'amt_point_pay']        0.5          0.2  0.28   \n",
       "3           ['rate_point_per_baht_pay']        1.0          0.2  0.34   \n",
       "4           ['month_id', 'ncust_visit']        0.5          0.2  0.28   \n",
       "\n",
       "   LLM recall  LLM precision LLM f1  \n",
       "0         1.0            1.0      1  \n",
       "1         1.0            1.0      1  \n",
       "2         1.0            1.0      1  \n",
       "3         1.0            1.0      1  \n",
       "4         1.0            1.0      1  "
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pointx_nlqsql_df = pd.read_excel(\"EXP_LLM_schemalink.xlsx\")\n",
    "pointx_nlqsql_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_fill_columns_template = \"\"\"You are a SQL query assistant.\n",
    "I have some SQL where the [MASK] column is syntaxed and I want you to respond to output that populates the [MASK] column of the SQL input followed by the question and schema description (name - description).\n",
    "If you don't know which column to fill in Do not include columns that you have created yourself. And only columns defined from the schema must be used. \n",
    "Do not use columns from other tables or schema. must also be used from the same table defined in the input.\n",
    "\n",
    "#################\n",
    "\n",
    "Schema :    cat\n",
    "            Table_name : cat\n",
    "            Table_desciprion : this table contain cat information\n",
    "            Columns : \n",
    "                id : number for identify cat\n",
    "                name : name of cat\n",
    "                age : age of cat\n",
    "                gender : gender of cat\n",
    "\n",
    "Question: Count number of cate each gender.\n",
    "MASK sql: SELECT [MASK], COUNT([MASK]) FROM cat GROUP BY [MASK];\n",
    "Result: SELECT gender, COUNT(*) FROM cat GROUP BY gender;\n",
    "\n",
    "#################\n",
    "\n",
    "Schema: {domain_knowledge}\n",
    "\n",
    "Question: {question}\n",
    "MASK sql: {mask_column}\n",
    "Result:\"\"\"\n",
    "\n",
    "llm_fill_columns_prompt = PromptTemplate(\n",
    "        input_variables=[\"domain_knowledge\", \"mask_column\", \"question\"],\n",
    "        template = llm_fill_columns_template                      \n",
    ")\n",
    "\n",
    "llm_fill_columns_chain = LLMChain(\n",
    "        llm=ChatOpenAI(temperature=0, model=model, request_timeout=120),\n",
    "        prompt=llm_fill_columns_prompt,\n",
    "        output_key=\"result\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_gensql_template = \"\"\"You are a SQL query assistant.\n",
    "Please create an SQL from the question and schema I have provided to you.\n",
    "\n",
    "Schema: {domain_knowledge}\n",
    "\n",
    "Question: {question}\n",
    "Result:\"\"\"\n",
    "\n",
    "llm_gensql_prompt = PromptTemplate(\n",
    "        input_variables=[\"domain_knowledge\", \"question\"],\n",
    "        template = llm_gensql_template                      \n",
    ")\n",
    "\n",
    "llm_gensql_chain = LLMChain(\n",
    "        llm=ChatOpenAI(temperature=0, model=model, request_timeout=120),\n",
    "        prompt=llm_gensql_prompt,\n",
    "        output_key=\"result\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_prompt(question:str, used_schema):\n",
    "    full_sql = \"\"\n",
    "    for table, columns in used_schema.items():\n",
    "        if not len(columns): continue       # pass this table when no column\n",
    "        primary_keys = schema_link.schema_datatypes[table][\"JOIN_KEY\"][\"PK\"]\n",
    "        foreign_keys = list(schema_link.schema_datatypes[table][\"JOIN_KEY\"][\"FK\"].keys())\n",
    "        join_table_key = primary_keys + foreign_keys\n",
    "        \n",
    "        sql = f\"CREATE TABLE {table} (\"\n",
    "        for column in columns:\n",
    "            if column in join_table_key and len(join_table_key): join_table_key.remove(column)\n",
    "            try:\n",
    "                sql += f' {column} {schema_link.schema_datatypes[table][\"COLUMNS\"][column]},'\n",
    "            except KeyError: \n",
    "                print(f\"KeyError :{column}\")\n",
    "                \n",
    "        if len(join_table_key): # key for join of table are remaining\n",
    "            for column in join_table_key:\n",
    "                sql += f' {column} {schema_link.schema_datatypes[table][\"COLUMNS\"][column]},'\n",
    "\n",
    "        # A lot of tables contain primary key\n",
    "        if len(primary_keys):\n",
    "            sql = sql[:-1] + ' PRIMARY KEY ('\n",
    "            for pk_type in primary_keys: sql += f'\"{pk_type}\" ,'\n",
    "            sql = sql[:-1] + \"),\"\n",
    "        # If table contains foreign key\n",
    "        if len(foreign_keys):\n",
    "            for fk, ref_table in schema_link.schema_datatypes[table][\"JOIN_KEY\"][\"FK\"].items():\n",
    "                sql = sql[:-1] + f' FOREIGN KEY (\"{fk}\") REFERENCES \"{ref_table}\" (\"{fk}\"),'\n",
    "\n",
    "        sql = sql[:-1] + \" )\\n\\n\"\n",
    "        full_sql += sql\n",
    "    prompt = full_sql + \"-- Using valid SQLite, answer the following questions for the tables provided above.\"\n",
    "    prompt = prompt + '\\n' + '-- ' + question\n",
    "    prompt = prompt + '\\n' + \"SELECT\"\n",
    "\n",
    "    if verbose: print(prompt)\n",
    "\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_used_schema(tables:list, list_of_columns:list) -> dict:\n",
    "    used_schema = dict()\n",
    "    for tab in tables:\n",
    "        used_schema[tab] = list_of_columns\n",
    "    return used_schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_pointx_db(sql_query):\n",
    "    try:\n",
    "        conn = sqlite3.connect(f'../src/pointx/database/pointx.db')\n",
    "        cursor = conn.cursor()\n",
    "    except:\n",
    "        return \"CANNOT CONNECT DATABASE\"\n",
    "    try:\n",
    "        cursor.execute(sql_query)\n",
    "        results = cursor.fetchall()\n",
    "    except:\n",
    "        return \"CANNOT FETCHING DATA\"\n",
    "    conn.close()\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_sql(prompt:str) -> str:\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "        generated_ids = nsql_model.generate(input_ids, max_length=1000)\n",
    "        sql = tokenizer.decode(generated_ids[0], skip_special_tokens=True).split('\\n')[-1]\n",
    "    return sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_column(sql_query:str, columns:list) -> str:\n",
    "    if '*' in sql_query: sql_query = sql_query.replace('*', \"[MASK]\")\n",
    "    for col in columns:\n",
    "        if col in sql_query: sql_query = sql_query.replace(col, \"[MASK]\")\n",
    "    return sql_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_schema_description(used_schema):\n",
    "    for table, list_of_columns in used_schema.items():\n",
    "        if table == 'pointx_keymatrix_dly':\n",
    "            shema_description = pointx_keymatrix_dly_description\n",
    "        elif table == 'pointx_fbs_rpt_dly':\n",
    "            shema_description = pointx_fbs_rpt_dly_description\n",
    "\n",
    "        used_schema[table] = dict()\n",
    "        used_schema[table]['Table_name'] = table\n",
    "        used_schema[table]['Table_description'] = shema_description['description']\n",
    "        used_schema[table]['Columns'] = dict()\n",
    "        for col in list_of_columns:\n",
    "            used_schema[table]['Columns'][col] = shema_description['columns'][col]\n",
    "    return used_schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COMPLETE 1 from 106\n",
      "SELECT month_id, SUM(ntx_pointx_financial) FROM pointx_keymatrix_dly GROUP BY month_id;\n",
      "[('2022-07', 447), ('2022-08', 259)]\n",
      "\n",
      "COMPLETE 2 from 106\n",
      "SELECT SUM(amt_point_topup) FROM pointx_keymatrix_dly WHERE month_id = '2022-08';\n",
      "[(178992.0,)]\n",
      "\n",
      "COMPLETE 3 from 106\n",
      "SELECT month_id, SUM(amt_point_pay) FROM pointx_keymatrix_dly WHERE month_id LIKE '2022%' GROUP BY month_id;\n",
      "[('2022-07', 30075.0), ('2022-08', 30045.0)]\n",
      "\n",
      "COMPLETE 4 from 106\n",
      "SELECT AVG(rate_point_per_baht_pay) FROM pointx_keymatrix_dly;\n",
      "[(13.825421897546896,)]\n",
      "\n",
      "COMPLETE 5 from 106\n",
      "SELECT month_id, AVG(ncust_visit) FROM pointx_keymatrix_dly GROUP BY month_id;\n",
      "[('2022-07', 47.61290322580645), ('2022-08', 55.1578947368421)]\n",
      "\n",
      "COMPLETE 6 from 106\n",
      "SELECT SUM(n_topup_point) FROM pointx_keymatrix_dly WHERE month_id LIKE '2022%';\n",
      "[(437,)]\n",
      "\n",
      "COMPLETE 7 from 106\n",
      "SELECT SUM(n_purchase) FROM pointx_keymatrix_dly;\n",
      "[(221,)]\n",
      "\n",
      "COMPLETE 8 from 106\n",
      "SELECT month_id, SUM(ncust_register_success) FROM pointx_keymatrix_dly GROUP BY month_id;\n",
      "[('2022-07', 668), ('2022-08', 609)]\n",
      "\n",
      "COMPLETE 9 from 106\n",
      "SELECT AVG(rate_point_per_baht_pay_sku) FROM pointx_keymatrix_dly;\n",
      "[(1.305,)]\n",
      "\n",
      "COMPLETE 10 from 106\n",
      "SELECT SUM(n_topup_point_onetime) FROM pointx_keymatrix_dly WHERE month_id = '2022-08';\n",
      "[(39,)]\n",
      "\n",
      "COMPLETE 11 from 106\n",
      "SELECT SUM(amt_point_transfer_out_extnl) FROM pointx_keymatrix_dly;\n",
      "[(None,)]\n",
      "\n",
      "COMPLETE 12 from 106\n",
      "SELECT month_id, amt_point_topup_auto_cardx FROM pointx_keymatrix_dly;\n",
      "[('2022-07', None), ('2022-07', None), ('2022-07', None), ('2022-07', None), ('2022-07', None), ('2022-08', None), ('2022-07', None), ('2022-08', None), ('2022-08', None), ('2022-08', None), ('2022-07', None), ('2022-08', None), ('2022-07', None), ('2022-07', None), ('2022-07', None), ('2022-07', None), ('2022-08', None), ('2022-07', None), ('2022-07', None), ('2022-07', None), ('2022-08', None), ('2022-07', None), ('2022-08', None), ('2022-08', None), ('2022-08', None), ('2022-07', None), ('2022-07', None), ('2022-07', None), ('2022-07', None), ('2022-07', None), ('2022-07', None), ('2022-07', None), ('2022-08', None), ('2022-08', None), ('2022-08', None), ('2022-07', None), ('2022-08', None), ('2022-08', None), ('2022-07', None), ('2022-07', None), ('2022-07', None), ('2022-08', None), ('2022-07', None), ('2022-07', None), ('2022-07', None), ('2022-08', None), ('2022-07', None), ('2022-07', None), ('2022-08', None), ('2022-08', None)]\n",
      "\n",
      "COMPLETE 13 from 106\n",
      "SELECT month_id, SUM(amt_point_topup_auto_cardx) FROM pointx_keymatrix_dly GROUP BY month_id;\n",
      "[('2022-07', None), ('2022-08', None)]\n",
      "\n",
      "COMPLETE 14 from 106\n",
      "SELECT _date, SUM(ncust_pointx) FROM pointx_keymatrix_dly GROUP BY _date;\n",
      "[('2022-07-01', 195), ('2022-07-02', 195), ('2022-07-03', 195), ('2022-07-04', 196), ('2022-07-05', 196), ('2022-07-06', 196), ('2022-07-07', 198), ('2022-07-08', 223), ('2022-07-09', 249), ('2022-07-10', 254), ('2022-07-11', 266), ('2022-07-12', 275), ('2022-07-13', 278), ('2022-07-14', 285), ('2022-07-15', 471), ('2022-07-16', 595), ('2022-07-17', 621), ('2022-07-18', 646), ('2022-07-19', 658), ('2022-07-20', 672), ('2022-07-21', 680), ('2022-07-22', 686), ('2022-07-23', 691), ('2022-07-24', 693), ('2022-07-25', 701), ('2022-07-26', 710), ('2022-07-27', 712), ('2022-07-28', 712), ('2022-07-29', 714), ('2022-07-30', 716), ('2022-07-31', 725), ('2022-08-01', 728), ('2022-08-02', 731), ('2022-08-03', 735), ('2022-08-04', 741), ('2022-08-05', 753), ('2022-08-06', 755), ('2022-08-07', 756), ('2022-08-08', 758), ('2022-08-09', 763), ('2022-08-10', 766), ('2022-08-11', 768), ('2022-08-12', 768), ('2022-08-13', 768), ('2022-08-14', 769), ('2022-08-15', 770), ('2022-08-16', 778), ('2022-08-17', 783), ('2022-08-18', 792), ('2022-08-19', 1247)]\n",
      "\n",
      "COMPLETE 15 from 106\n",
      "SELECT month_id, AVG(rate_point_per_baht_pay_pyw) FROM pointx_keymatrix_dly GROUP BY month_id;\n",
      "[('2022-07', 2.4516129032258065), ('2022-08', 3.4473684210526314)]\n",
      "\n",
      "COMPLETE 16 from 106\n",
      "SELECT month_id, SUM(n_purchase_pyw_rbh) FROM pointx_keymatrix_dly GROUP BY month_id;\n",
      "[('2022-07', 7), ('2022-08', 10)]\n",
      "\n",
      "COMPLETE 17 from 106\n",
      "SELECT SUM(n_topup_point_onetime) FROM pointx_keymatrix_dly WHERE _date = '2022-08-11';\n",
      "[(3,)]\n",
      "\n",
      "COMPLETE 18 from 106\n",
      "SELECT ncust_guest_visit FROM pointx_keymatrix_dly WHERE _date = '2022-07-12';\n",
      "[(3,)]\n",
      "\n",
      "COMPLETE 19 from 106\n",
      "SELECT month_id, SUM(n_point_payment_p_only) FROM pointx_keymatrix_dly GROUP BY month_id;\n",
      "[('2022-07', 10), ('2022-08', 8)]\n",
      "\n",
      "COMPLETE 20 from 106\n",
      "SELECT month_id, SUM(n_purchase) FROM pointx_keymatrix_dly GROUP BY month_id;\n",
      "[('2022-07', 137), ('2022-08', 84)]\n",
      "\n",
      "COMPLETE 21 from 106\n",
      "SELECT AVG(rate_point_per_baht_pay_weight) FROM pointx_keymatrix_dly;\n",
      "[(12.635267164160373,)]\n",
      "\n",
      "COMPLETE 22 from 106\n",
      "SELECT SUM(n_purchase_qr_30) FROM pointx_keymatrix_dly;\n",
      "[(110,)]\n",
      "\n",
      "COMPLETE 23 from 106\n",
      "SELECT ncust_pointx_visit FROM pointx_keymatrix_dly WHERE _date = '2022-08-12';\n",
      "[(12,)]\n",
      "\n",
      "COMPLETE 24 from 106\n",
      "SELECT month_id, SUM(n_purchase_pyw) FROM pointx_keymatrix_dly GROUP BY month_id;\n",
      "[('2022-07', 8), ('2022-08', 10)]\n",
      "\n",
      "COMPLETE 25 from 106\n",
      "SELECT SUM(n_transfer_point_out_extnl) FROM pointx_keymatrix_dly;\n",
      "[(None,)]\n",
      "\n",
      "COMPLETE 26 from 106\n",
      "SELECT SUM(n_purchase_pyw_rbh) FROM pointx_keymatrix_dly;\n",
      "[(17,)]\n",
      "\n",
      "COMPLETE 27 from 106\n",
      "SELECT SUM(n_topup_point_auto_cardx) FROM pointx_keymatrix_dly;\n",
      "[(None,)]\n",
      "\n",
      "COMPLETE 28 from 106\n",
      "SELECT _date, SUM(n_topup_point_onboard) FROM pointx_keymatrix_dly GROUP BY _date;\n",
      "[('2022-07-01', 0), ('2022-07-02', 0), ('2022-07-03', 0), ('2022-07-04', 0), ('2022-07-05', 0), ('2022-07-06', 0), ('2022-07-07', 1), ('2022-07-08', 4), ('2022-07-09', 8), ('2022-07-10', 0), ('2022-07-11', 2), ('2022-07-12', 3), ('2022-07-13', 1), ('2022-07-14', 0), ('2022-07-15', 45), ('2022-07-16', 24), ('2022-07-17', 3), ('2022-07-18', 6), ('2022-07-19', 5), ('2022-07-20', 5), ('2022-07-21', 1), ('2022-07-22', 2), ('2022-07-23', 0), ('2022-07-24', 0), ('2022-07-25', 1), ('2022-07-26', 2), ('2022-07-27', 0), ('2022-07-28', 0), ('2022-07-29', 0), ('2022-07-30', 0), ('2022-07-31', 0), ('2022-08-01', 0), ('2022-08-02', 0), ('2022-08-03', 1), ('2022-08-04', 0), ('2022-08-05', 2), ('2022-08-06', 0), ('2022-08-07', 1), ('2022-08-08', 0), ('2022-08-09', 0), ('2022-08-10', 1), ('2022-08-11', 0), ('2022-08-12', 1), ('2022-08-13', 0), ('2022-08-14', 0), ('2022-08-15', 0), ('2022-08-16', 0), ('2022-08-17', 0), ('2022-08-18', 0), ('2022-08-19', 24)]\n",
      "\n",
      "COMPLETE 29 from 106\n",
      "SELECT ncust_pointx_financial FROM pointx_keymatrix_dly WHERE month_id = '2022-08';\n",
      "[(6,), (37,), (12,), (9,), (17,), (5,), (10,), (5,), (5,), (3,), (18,), (8,), (9,), (7,), (5,), (8,), (10,), (7,), (11,)]\n",
      "\n",
      "COMPLETE 30 from 106\n",
      "SELECT month_id, SUM(n_purchase_qr_cs) FROM pointx_keymatrix_dly GROUP BY month_id;\n",
      "[('2022-07', 20), ('2022-08', 7)]\n",
      "\n",
      "COMPLETE 31 from 106\n",
      "SELECT _date, SUM(mtd1_ncust_partner_new) FROM pointx_keymatrix_dly GROUP BY _date;\n",
      "[('2022-07-01', None), ('2022-07-02', None), ('2022-07-03', None), ('2022-07-04', None), ('2022-07-05', None), ('2022-07-06', None), ('2022-07-07', None), ('2022-07-08', None), ('2022-07-09', None), ('2022-07-10', None), ('2022-07-11', None), ('2022-07-12', None), ('2022-07-13', None), ('2022-07-14', None), ('2022-07-15', None), ('2022-07-16', None), ('2022-07-17', None), ('2022-07-18', None), ('2022-07-19', None), ('2022-07-20', None), ('2022-07-21', None), ('2022-07-22', None), ('2022-07-23', None), ('2022-07-24', None), ('2022-07-25', None), ('2022-07-26', None), ('2022-07-27', None), ('2022-07-28', None), ('2022-07-29', None), ('2022-07-30', None), ('2022-07-31', None), ('2022-08-01', None), ('2022-08-02', None), ('2022-08-03', None), ('2022-08-04', None), ('2022-08-05', None), ('2022-08-06', None), ('2022-08-07', None), ('2022-08-08', None), ('2022-08-09', None), ('2022-08-10', None), ('2022-08-11', None), ('2022-08-12', None), ('2022-08-13', None), ('2022-08-14', None), ('2022-08-15', None), ('2022-08-16', None), ('2022-08-17', None), ('2022-08-18', None), ('2022-08-19', None)]\n",
      "\n",
      "COMPLETE 32 from 106\n",
      "SELECT SUM(n_purchase_pyw) FROM pointx_keymatrix_dly;\n",
      "[(18,)]\n",
      "\n",
      "COMPLETE 33 from 106\n",
      "SELECT AVG(rate_point_per_baht_pay_sku_weight) FROM pointx_keymatrix_dly;\n",
      "[(1.305,)]\n",
      "\n",
      "COMPLETE 34 from 106\n",
      "SELECT SUM(ncust_pointx_financial) FROM pointx_keymatrix_dly WHERE _date = '2022-08-08';\n",
      "[(5,)]\n",
      "\n",
      "COMPLETE 35 from 106\n",
      "SELECT month_id, AVG(rate_baht_per_point_pay_pyw_weight) FROM pointx_keymatrix_dly GROUP BY month_id;\n",
      "[('2022-07', 0.029138044162348185), ('2022-08', 0.039628976748090315)]\n",
      "\n",
      "COMPLETE 36 from 106\n",
      "SELECT _date, amt_point_pay FROM pointx_keymatrix_dly ORDER BY amt_point_pay DESC LIMIT 5;\n",
      "[('2022-08-14', 13546.0), ('2022-07-27', 12420.0), ('2022-07-16', 7050.0), ('2022-08-18', 5280.0), ('2022-07-26', 3300.0)]\n",
      "\n",
      "COMPLETE 37 from 106\n",
      "SELECT _date, mtd1_ncust_partner_new FROM pointx_keymatrix_dly ORDER BY mtd1_ncust_partner_new DESC LIMIT 3;\n",
      "[('2022-07-06', None), ('2022-07-01', None), ('2022-07-08', None)]\n",
      "\n",
      "COMPLETE 38 from 106\n",
      "SELECT COUNT(*) FROM pointx_keymatrix_dly WHERE n_point_payment_p_cc > 0;\n",
      "[(18,)]\n",
      "\n",
      "COMPLETE 39 from 106\n",
      "SELECT event_date, COUNT(DISTINCT user_pseudo_id) FROM (     SELECT event_date, ga_session_id, user_pseudo_id     FROM pointx_fbs_rpt_dly     GROUP BY event_date, ga_session_id, user_pseudo_id     HAVING SUM(engagement_time_msec) > 10*1000 ) GROUP BY event_date ORDER BY event_date ASC\n",
      "[('2022-05-23', 1), ('2022-05-24', 6), ('2022-05-25', 4), ('2022-05-26', 2), ('2022-05-27', 7), ('2022-05-28', 1), ('2022-05-30', 2)]\n",
      "\n",
      "COMPLETE 40 from 106\n",
      "SELECT event_month, COUNT(DISTINCT user_pseudo_id) FROM (     SELECT event_month, ga_session_id, user_pseudo_id     FROM pointx_fbs_rpt_dly     GROUP BY event_month, ga_session_id, user_pseudo_id     HAVING SUM(engagement_time_msec) > 10*1000 ) GROUP BY event_month ORDER BY event_month ASC\n",
      "[('2022-05', 22)]\n",
      "\n",
      "COMPLETE 41 from 106\n",
      "SELECT AVG(cnt) FROM   (SELECT event_date, COUNT(DISTINCT user_pseudo_id) as cnt   FROM (       SELECT event_date, ga_session_id, user_pseudo_id       FROM pointx_fbs_rpt_dly       GROUP BY event_date, ga_session_id, user_pseudo_id       HAVING SUM(engagement_time_msec) > 10*1000   )   GROUP BY event_date   ORDER BY event_date ASC) WHERE event_date >= DATEADD(DAY, -7, GETDATE())\n",
      "CANNOT FETCHING DATA\n",
      "\n",
      "COMPLETE 42 from 106\n",
      "SELECT AVG(cnt) FROM   (SELECT event_date, COUNT(DISTINCT user_pseudo_id) as cnt   FROM (       SELECT event_date, ga_session_id, user_pseudo_id       FROM pointx_fbs_rpt_dly       GROUP BY event_date, ga_session_id, user_pseudo_id       HAVING SUM(engagement_time_msec) > 10*1000   )   GROUP BY event_date   ORDER BY event_date ASC) WHERE event_date >= DATEADD(DAY, -30, GETDATE())\n",
      "CANNOT FETCHING DATA\n",
      "\n",
      "COMPLETE 43 from 106\n",
      "SELECT user_pseudo_id, MAX(last_active) FROM   (SELECT event_date, ga_session_id, user_pseudo_id, MAX(event_timestamp) as last_active   FROM pointx_fbs_rpt_dly   GROUP BY event_date, ga_session_id, user_pseudo_id   HAVING SUM(engagement_time_msec) > 10*1000) GROUP BY user_pseudo_id\n",
      "[('0198f28f3b2d07688c66a6aabf1a6c18', '2022-05-25T20:57:03.975+0000'), ('0412E26637DD42D6B3DCE818E153495B', '2022-05-24T17:22:44.809+0000'), ('2618A002B7F649AD88D1E3E2AE0C9D34', '2022-05-30T15:05:31.311+0000'), ('474ae23630781fc3b8d08ee6bc8038a4', '2022-05-28T16:07:04.821+0000'), ('5AF61BAC90A94DF2A25B432D527FD88E', '2022-05-30T17:02:09.937+0000'), ('5C7F578875D448F9B17AB7512FEF401B', '2022-05-25T01:32:49.712+0000'), ('70b17c4bfcebf709bc1a45f98b5d1a39', '2022-05-27T14:52:14.617+0000'), ('76487BD129734D0BA622A19E3348EDFB', '2022-05-27T13:15:59.241+0000'), ('78995275425D44D5B4418B639FE41B3A', '2022-05-24T18:45:55.027+0000'), ('78e36f49bff175a8fe7bd910af2016d9', '2022-05-27T15:19:40.929+0000'), ('850BC09AF0CE488AB8DC4C70B39DC35F', '2022-05-24T12:06:51.837+0000'), ('8984e7e0d2373e31fdece9a43e47e99e', '2022-05-23T17:46:44.976+0000'), ('89970B97EFAA447F85C61A10C70ED0B0', '2022-05-26T16:26:17.768+0000'), ('9CD78C6F93AC42B7804C376277750F4A', '2022-05-24T16:20:38.551+0000'), ('9DE0535A75E7420DAFC90F1C5B67E2D7', '2022-05-24T01:45:43.556+0000'), ('A387790874A44EF9A9F5D6B5E1885656', '2022-05-25T15:27:58.272+0000'), ('D0662C11CE7E454ABCD9732FCBC6CD92', '2022-05-25T22:19:40.994+0000'), ('D23C83AC69D74FAC836FB942CECE6DFA', '2022-05-27T15:21:19.018+0000'), ('D7D907A202874EA4A4A3F48E2BF6186F', '2022-05-24T14:51:38.809+0000'), ('EC05EE85432F44BE81415BDB06E0714B', '2022-05-27T14:46:27.893+0000'), ('F51A60AC085F4416B246F636099DAB85', '2022-05-27T18:34:10.527+0000'), ('daf44a8e8f02d47895ef65027e06428c', '2022-05-27T10:26:03.568+0000')]\n",
      "\n",
      "COMPLETE 44 from 106\n",
      "SELECT COUNT(DISTINCT user_pseudo_id) FROM    (SELECT ga_session_id, user_pseudo_id FROM pointx_fbs_rpt_dly   GROUP BY ga_session_id, user_pseudo_id   HAVING SUM(engagement_time_msec) >= 10*1000)\n",
      "[(22,)]\n",
      "\n",
      "COMPLETE 45 from 106\n",
      "SELECT AVG(cnt) FROM   (SELECT event_date, COUNT(DISTINCT user_pseudo_id) as cnt   FROM (       SELECT event_date, ga_session_id, user_pseudo_id       FROM pointx_fbs_rpt_dly       GROUP BY event_date, ga_session_id, user_pseudo_id       HAVING SUM(engagement_time_msec) > 10*1000   )   GROUP BY event_date   ORDER BY event_date ASC) WHERE DAYOFWEEK(event_date) BETWEEN 2 AND 6\n",
      "CANNOT FETCHING DATA\n",
      "\n",
      "COMPLETE 46 from 106\n",
      "SELECT AVG(cnt) FROM   (SELECT event_date, COUNT(DISTINCT user_pseudo_id) as cnt   FROM (       SELECT event_date, ga_session_id, user_pseudo_id       FROM pointx_fbs_rpt_dly       GROUP BY event_date, ga_session_id, user_pseudo_id       HAVING SUM(engagement_time_msec) > 10*1000   )   GROUP BY event_date   ORDER BY event_date ASC) WHERE DAYOFWEEK(event_date) IN (1, 7)\n",
      "CANNOT FETCHING DATA\n",
      "\n",
      "COMPLETE 47 from 106\n",
      "SELECT user_pseudo_id, EXTRACT(WEEK FROM event_timestamp), EXTRACT(YEAR FROM event_timestamp), COUNT(event_name) FROM pointx_fbs_rpt_dly WHERE event_name = \"session_start\" GROUP BY user_pseudo_id, EXTRACT(WEEK FROM event_timestamp), EXTRACT(YEAR FROM event_timestamp)\n",
      "CANNOT FETCHING DATA\n",
      "\n",
      "COMPLETE 48 from 106\n",
      "SELECT week, year, AVG(cnt) FROM   (SELECT user_pseudo_id, EXTRACT(WEEK FROM event_timestamp) AS week, EXTRACT(YEAR FROM event_timestamp) AS year, COUNT(event_name) as cnt FROM pointx_fbs_rpt_dly   WHERE event_name = \"session_start\"   GROUP BY user_pseudo_id, EXTRACT(WEEK FROM event_timestamp), EXTRACT(YEAR FROM event_timestamp)) GROUP BY week, year\n",
      "CANNOT FETCHING DATA\n",
      "\n",
      "COMPLETE 49 from 106\n",
      "SELECT user_pseudo_id, EXTRACT(MONTH FROM event_timestamp), EXTRACT(YEAR FROM event_timestamp), COUNT(event_name) FROM pointx_fbs_rpt_dly WHERE event_name = \"session_start\" GROUP BY user_pseudo_id, EXTRACT(MONTH FROM event_timestamp), EXTRACT(YEAR FROM event_timestamp)\n",
      "CANNOT FETCHING DATA\n",
      "\n",
      "COMPLETE 50 from 106\n",
      "SELECT user_pseudo_id, EXTRACT(MONTH FROM event_timestamp), EXTRACT(YEAR FROM event_timestamp), COUNT(event_name) FROM pointx_fbs_rpt_dly WHERE event_name = \"session_start\" GROUP BY user_pseudo_id, EXTRACT(MONTH FROM event_timestamp), EXTRACT(YEAR FROM event_timestamp)\n",
      "CANNOT FETCHING DATA\n",
      "\n",
      "COMPLETE 51 from 106\n",
      "SELECT month, year, AVG(cnt) FROM   (SELECT user_pseudo_id, EXTRACT(MONTH FROM event_timestamp) AS month, EXTRACT(YEAR FROM event_timestamp) AS year, COUNT(event_name) AS cnt FROM pointx_fbs_rpt_dly   WHERE event_name = \"session_start\"   GROUP BY user_pseudo_id, EXTRACT(MONTH FROM event_timestamp), EXTRACT(YEAR FROM event_timestamp)) GROUP BY month, year\n",
      "CANNOT FETCHING DATA\n",
      "\n",
      "COMPLETE 52 from 106\n",
      "SELECT user_pseudo_id, COUNT(event_name) FROM pointx_fbs_rpt_dly WHERE event_name = \"session_start\" GROUP BY user_pseudo_id\n",
      "[('0198f28f3b2d07688c66a6aabf1a6c18', 2), ('0412E26637DD42D6B3DCE818E153495B', 1), ('2618A002B7F649AD88D1E3E2AE0C9D34', 1), ('474ae23630781fc3b8d08ee6bc8038a4', 1), ('5AF61BAC90A94DF2A25B432D527FD88E', 3), ('5C7F578875D448F9B17AB7512FEF401B', 2), ('70b17c4bfcebf709bc1a45f98b5d1a39', 2), ('76487BD129734D0BA622A19E3348EDFB', 1), ('78995275425D44D5B4418B639FE41B3A', 1), ('78e36f49bff175a8fe7bd910af2016d9', 1), ('850BC09AF0CE488AB8DC4C70B39DC35F', 1), ('8984e7e0d2373e31fdece9a43e47e99e', 2), ('89970B97EFAA447F85C61A10C70ED0B0', 3), ('9CD78C6F93AC42B7804C376277750F4A', 1), ('9DE0535A75E7420DAFC90F1C5B67E2D7', 2), ('A387790874A44EF9A9F5D6B5E1885656', 4), ('D0662C11CE7E454ABCD9732FCBC6CD92', 1), ('D23C83AC69D74FAC836FB942CECE6DFA', 2), ('D7D907A202874EA4A4A3F48E2BF6186F', 1), ('EC05EE85432F44BE81415BDB06E0714B', 2), ('F51A60AC085F4416B246F636099DAB85', 1), ('d75e40462e09492af8788e72df9aa0e4', 1), ('daf44a8e8f02d47895ef65027e06428c', 1), ('fb0d14acd0679ee580e8fa800d8476b7', 1)]\n",
      "\n",
      "COMPLETE 53 from 106\n",
      "SELECT user_pseudo_id, DATEDIFF(DAY, MAX(event_timestamp), GETDATE()) FROM pointx_fbs_rpt_dly WHERE event_name = \"session_start\" GROUP BY user_pseudo_id\n",
      "CANNOT FETCHING DATA\n",
      "\n",
      "COMPLETE 54 from 106\n",
      "SELECT AVG(day) FROM   (SELECT user_pseudo_id, DATEDIFF(DAY, MAX(event_timestamp), GETDATE()) as day FROM pointx_fbs_rpt_dly   WHERE event_name = \"session_start\"   GROUP BY user_pseudo_id)\n",
      "CANNOT FETCHING DATA\n",
      "\n",
      "COMPLETE 55 from 106\n",
      "SELECT COUNT(DISTINCT user_pseudo_id) FROM   (SELECT user_pseudo_id, COUNT(DISTINCT event_date) FROM pointx_fbs_rpt_dly   WHERE event_date >= DATEADD(DAY, -7, GETDATE())   GROUP BY user_pseudo_id)\n",
      "CANNOT FETCHING DATA\n",
      "\n",
      "COMPLETE 56 from 106\n",
      "SELECT COUNT(DISTINCT user_pseudo_id) FROM   (SELECT user_pseudo_id, COUNT(DISTINCT event_date) FROM pointx_fbs_rpt_dly   WHERE event_name = \"session_start\" AND event_date >= DATEADD(DAY, -7, GETDATE())   GROUP BY user_pseudo_id   HAVING COUNT(DISTINCT event_date) > 1)\n",
      "CANNOT FETCHING DATA\n",
      "\n",
      "COMPLETE 57 from 106\n",
      "SELECT COUNT(DISTINCT user_pseudo_id) FROM   (SELECT user_pseudo_id, COUNT(DISTINCT event_date) FROM pointx_fbs_rpt_dly   WHERE event_name = \"session_start\" AND event_date >= DATEADD(DAY, -30, GETDATE())   GROUP BY user_pseudo_id   HAVING COUNT(DISTINCT event_date) > 1)\n",
      "CANNOT FETCHING DATA\n",
      "\n",
      "COMPLETE 58 from 106\n",
      "SELECT COUNT(DISTINCT user_pseudo_id) FROM   (SELECT user_pseudo_id, COUNT(DISTINCT event_date) as count FROM pointx_fbs_rpt_dly   WHERE event_name = \"session_start\" AND event_date >= DATEADD(DAY, -30, GETDATE())   GROUP BY user_pseudo_id   HAVING COUNT(DISTINCT event_date) >= 0.25*30)\n",
      "CANNOT FETCHING DATA\n",
      "\n",
      "COMPLETE 59 from 106\n",
      "SELECT COUNT(DISTINCT user_pseudo_id) FROM   (SELECT user_pseudo_id, COUNT(DISTINCT event_date) as count FROM pointx_fbs_rpt_dly   WHERE event_name = \"session_start\" AND event_date >= DATEADD(DAY, -30, GETDATE())   GROUP BY user_pseudo_id   HAVING COUNT(DISTINCT event_date) >= 0.5*30)\n",
      "CANNOT FETCHING DATA\n",
      "\n",
      "COMPLETE 60 from 106\n",
      "SELECT EXTRACT(HOUR FROM event_timestamp) as hour, EXTRACT(DAY FROM event_timestamp) as day, COUNT(DISTINCT user_pseudo_id)  FROM pointx_fbs_rpt_dly WHERE event_name = \"session_start\" GROUP BY hour, day\n",
      "CANNOT FETCHING DATA\n",
      "\n",
      "COMPLETE 61 from 106\n",
      "SELECT hour, AVG(cnt) as avg_cnt FROM   (SELECT EXTRACT(HOUR FROM event_timestamp) as hour, EXTRACT(DAY FROM event_timestamp) as day, COUNT(DISTINCT user_pseudo_id) as cnt   FROM pointx_fbs_rpt_dly   WHERE event_name = \"session_start\"   GROUP BY hour, day) GROUP BY hour ORDER BY hour ASC\n",
      "CANNOT FETCHING DATA\n",
      "\n",
      "COMPLETE 62 from 106\n",
      "SELECT hour, AVG(cnt) as avg_cnt FROM   (SELECT EXTRACT(HOUR FROM event_timestamp) as hour, EXTRACT(DAY FROM event_timestamp) as day, COUNT(DISTINCT user_pseudo_id) as cnt   FROM pointx_fbs_rpt_dly   WHERE event_name = \"session_start\"   GROUP BY hour, day) GROUP BY hour ORDER BY avg_cnt DESC LIMIT 1\n",
      "CANNOT FETCHING DATA\n",
      "\n",
      "COMPLETE 63 from 106\n",
      "SELECT AVG(time) FROM     (SELECT user_pseudo_id, ga_session_id, SUM(engagement_time_msec)/(1000*60) as time FROM pointx_fbs_rpt_dly     WHERE engagement_time_msec != 0 AND ga_session_id IS NOT NULL     GROUP BY user_pseudo_id, ga_session_id)\n",
      "[(1.2432432432432432,)]\n",
      "\n",
      "COMPLETE 64 from 106\n",
      "SELECT AVG(session) FROM   (SELECT user_pseudo_id, COUNT(DISTINCT ga_session_id) as session FROM pointx_fbs_rpt_dly   GROUP BY user_pseudo_id, event_date)\n",
      "[(1.4285714285714286,)]\n",
      "\n",
      "COMPLETE 65 from 106\n",
      "SELECT AVG(session) FROM   (SELECT user_pseudo_id, COUNT(DISTINCT ga_session_id) as session FROM pointx_fbs_rpt_dly   GROUP BY user_pseudo_id, EXTRACT(WEEK FROM event_timestamp), EXTRACT(YEAR FROM event_timestamp))\n",
      "CANNOT FETCHING DATA\n",
      "\n",
      "COMPLETE 66 from 106\n",
      "SELECT AVG(time) FROM   (SELECT user_pseudo_id, event_date, SUM(engagement_time_msec/(1000*60)) as time FROM pointx_fbs_rpt_dly   GROUP BY user_pseudo_id, event_date)\n",
      "[(0.5714285714285714,)]\n",
      "\n",
      "COMPLETE 67 from 106\n",
      "SELECT AVG(time) FROM   (SELECT user_pseudo_id, event_date, SUM(engagement_time_msec/(1000*60)) as time FROM pointx_fbs_rpt_dly   GROUP BY user_pseudo_id, event_date) WHERE DAYOFWEEK(event_date) BETWEEN 2 AND 6\n",
      "CANNOT FETCHING DATA\n",
      "\n",
      "COMPLETE 68 from 106\n",
      "SELECT AVG(time) FROM   (SELECT user_pseudo_id, event_date, SUM(engagement_time_msec/(1000*60)) as time FROM pointx_fbs_rpt_dly   GROUP BY user_pseudo_id, event_date) WHERE DAYOFWEEK(event_date) IN (1, 7)\n",
      "CANNOT FETCHING DATA\n",
      "\n",
      "COMPLETE 69 from 106\n",
      "SELECT COUNT(DISTINCT user_pseudo_id) FROM pointx_fbs_rpt_dly WHERE event_name = \"app_remove\" AND event_timestamp >= DATEADD(DAY, -30, GETDATE())\n",
      "CANNOT FETCHING DATA\n",
      "\n",
      "COMPLETE 70 from 106\n",
      "SELECT user_pseudo_id, DATEDIFF(DAY, MIN(user_first_touch_timestamp), MAX(event_timestamp)) FROM pointx_fbs_rpt_dly GROUP BY user_pseudo_id\n",
      "CANNOT FETCHING DATA\n",
      "\n",
      "COMPLETE 71 from 106\n",
      "SELECT event_name,         COUNT(event_name) as cnt,         COUNT(event_name) / SUM(COUNT(event_name)) OVER() as proportion FROM pointx_fbs_rpt_dly GROUP BY event_name ORDER BY cnt DESC LIMIT 10\n",
      "[('pointx_dealoftheday_landing', 277, 0), ('screen_view', 194, 0), ('user_engagement', 177, 0), ('mypointx_landing', 47, 0), ('session_start', 38, 0), ('pointx_home_bottom_bar', 25, 0), ('payatmcht_landing', 23, 0), ('pointx_view_mypoint', 19, 0), ('pointx_payandmerchant_bottom_bar', 15, 0), ('pointx_mypointx_bottom_bar', 14, 0)]\n",
      "\n",
      "COMPLETE 72 from 106\n",
      "SELECT COUNT(event_name) FROM pointx_fbs_rpt_dly WHERE event_name = \"virtualmall_landing\" AND event_timestamp >= DATEADD(DAY, -7, GETDATE())\n",
      "CANNOT FETCHING DATA\n",
      "\n",
      "COMPLETE 73 from 106\n",
      "SELECT COUNT(DISTINCT user_pseudo_id) FROM pointx_fbs_rpt_dly WHERE event_name = \"virtualmall_landing\" AND event_timestamp >= DATEADD(DAY, -7, GETDATE())\n",
      "CANNOT FETCHING DATA\n",
      "\n",
      "COMPLETE 74 from 106\n",
      "SELECT COUNT(event_name) FROM pointx_fbs_rpt_dly WHERE event_name = \"virtualmall_landing\" AND event_timestamp >= DATEADD(DAY, -30, GETDATE())\n",
      "CANNOT FETCHING DATA\n",
      "\n",
      "COMPLETE 75 from 106\n",
      "SELECT COUNT(DISTINCT user_pseudo_id) FROM pointx_fbs_rpt_dly WHERE event_name = \"virtualmall_landing\" AND event_timestamp >= DATEADD(DAY, -30, GETDATE())\n",
      "CANNOT FETCHING DATA\n",
      "\n",
      "COMPLETE 76 from 106\n",
      "SELECT COUNT(cnt) FROM     (SELECT COUNT(DISTINCT user_pseudo_id) as cnt FROM pointx_fbs_rpt_dly     WHERE event_name = \"virtualmall_landing\" AND event_timestamp >= DATEADD(DAY, -7, GETDATE())     GROUP BY user_pseudo_id     HAVING COUNT(event_name) > 1)\n",
      "CANNOT FETCHING DATA\n",
      "\n",
      "COMPLETE 77 from 106\n",
      "SELECT COUNT(cnt) FROM     (SELECT COUNT(DISTINCT user_pseudo_id) as cnt FROM pointx_fbs_rpt_dly     WHERE event_name = \"\"virtualmall_landing\"\" AND event_timestamp >= DATEADD(DAY, -30, GETDATE())     GROUP BY user_pseudo_id     HAVING COUNT(event_name) > 1)\n",
      "CANNOT FETCHING DATA\n",
      "\n",
      "COMPLETE 78 from 106\n",
      "SELECT COUNT(event_name) FROM pointx_fbs_rpt_dly WHERE event_name = \"virtualmall_search\" AND event_timestamp >= DATEADD(DAY, -7, GETDATE())\n",
      "CANNOT FETCHING DATA\n",
      "\n",
      "COMPLETE 79 from 106\n",
      "SELECT COUNT(cnt) FROM   (SELECT COUNT(DISTINCT user_pseudo_id) as cnt FROM pointx_fbs_rpt_dly   WHERE event_name = \"virtualmall_landing\" AND event_timestamp >= DATEADD(DAY, -30, GETDATE())   GROUP BY user_pseudo_id   HAVING COUNT(event_name) > 5)\n",
      "CANNOT FETCHING DATA\n",
      "\n",
      "COMPLETE 80 from 106\n",
      "SELECT COUNT(DISTINCT user_pseudo_id) FROM   (SELECT user_pseudo_id, MAX(event_timestamp) as max_time FROM pointx_fbs_rpt_dly   WHERE event_name = \"session_start\"   GROUP BY user_pseudo_id   HAVING  DATEDIFF(WEEK, max_time, GETDATE()) > 2)\n",
      "CANNOT FETCHING DATA\n",
      "\n",
      "COMPLETE 81 from 106\n",
      "SELECT COUNT(DISTINCT user_pseudo_id) FROM   (SELECT user_pseudo_id, MAX(event_timestamp) as max_time FROM pointx_fbs_rpt_dly   WHERE event_name = \"session_start\"   GROUP BY user_pseudo_id   HAVING  DATEDIFF(MONTH, max_time, GETDATE()) > 1)\n",
      "CANNOT FETCHING DATA\n",
      "\n",
      "COMPLETE 82 from 106\n",
      "SELECT COUNT(DISTINCT user_pseudo_id) FROM   (SELECT user_pseudo_id, MAX(event_timestamp) as max_time FROM pointx_fbs_rpt_dly   WHERE event_name = \"\"session_start\"\"   GROUP BY user_pseudo_id   HAVING  DATEDIFF(MONTH, max_time, GETDATE()) > 3)\n",
      "CANNOT FETCHING DATA\n",
      "\n",
      "COMPLETE 83 from 106\n",
      "WITH users AS (   SELECT user_pseudo_id, event_timestamp    FROM pointx_fbs_rpt_dly   WHERE event_name = \"session_start\"    AND event_timestamp >= DATEADD(DAY, -60, GETDATE()) ), users_start AS ( SELECT COUNT(DISTINCT user_pseudo_id) as user FROM users ), users_end AS ( SELECT COUNT(DISTINCT user_pseudo_id) as user FROM users WHERE event_timestamp >= DATEADD(DAY, -30, GETDATE()) )  SELECT users_start.user - users_end.user FROM users_start JOIN users_end\n",
      "CANNOT FETCHING DATA\n",
      "\n",
      "COMPLETE 84 from 106\n",
      "WITH users AS (   SELECT user_pseudo_id, event_timestamp    FROM pointx_fbs_rpt_dly   WHERE event_name = \"session_start\"    AND event_timestamp >= DATEADD(DAY, -60, GETDATE()) ), users_start AS ( SELECT COUNT(DISTINCT user_pseudo_id) as user FROM users ), users_end AS ( SELECT COUNT(DISTINCT user_pseudo_id) as user FROM users WHERE event_timestamp >= DATEADD(DAY, -30, GETDATE()) )  SELECT (users_start.user - users_end.user)/(users_start.user) FROM users_start JOIN users_end\n",
      "CANNOT FETCHING DATA\n",
      "\n",
      "COMPLETE 85 from 106\n",
      "SELECT COUNT(DISTINCT user_pseudo_id) FROM pointx_fbs_rpt_dly WHERE DATEDIFF(DAY, user_first_touch_timestamp, GETDATE()) <= 7\n",
      "CANNOT FETCHING DATA\n",
      "\n",
      "COMPLETE 86 from 106\n",
      "SELECT COUNT(DISTINCT user_pseudo_id) FROM pointx_fbs_rpt_dly WHERE DATEDIFF(DAY, user_first_touch_timestamp, GETDATE()) <= 30\n",
      "CANNOT FETCHING DATA\n",
      "\n",
      "COMPLETE 87 from 106\n",
      "WITH users AS (   SELECT user_pseudo_id, event_timestamp    FROM pointx_fbs_rpt_dly   WHERE event_name = \"session_start\"    AND event_timestamp >= DATEADD(DAY, -60, GETDATE())   AND DATEDIFF(DAY, user_first_touch_timestamp, GETDATE()) > 60 ), users_start AS ( SELECT user_pseudo_id as user FROM users WHERE event_timestamp >= DATEADD(DAY, -60, GETDATE()) AND event_timestamp < DATEADD(DAY, -30, GETDATE()) ), users_end AS ( SELECT user_pseudo_id as user FROM users WHERE event_timestamp >= DATEADD(DAY, -30, GETDATE()) )  SELECT COUNT(users_end.user) FROM users_end LEFT JOIN users_start ON users_end.user = users_start.user WHERE users_start.user IS NULL\n",
      "CANNOT FETCHING DATA\n",
      "\n",
      "COMPLETE 88 from 106\n",
      "WITH users AS (   SELECT user_pseudo_id, event_timestamp    FROM pointx_fbs_rpt_dly   WHERE event_name = \"session_start\"    AND event_timestamp >= DATEADD(DAY, -180, GETDATE())   AND DATEDIFF(DAY, user_first_touch_timestamp, GETDATE()) > 180 ), users_start AS ( SELECT user_pseudo_id as user FROM users WHERE event_timestamp >= DATEADD(DAY, -180, GETDATE()) AND event_timestamp < DATEADD(DAY, -90, GETDATE()) ), users_end AS ( SELECT user_pseudo_id as user FROM users WHERE event_timestamp >= DATEADD(DAY, -90, GETDATE()) )  SELECT COUNT(users_end.user) FROM users_end LEFT JOIN users_start ON users_end.user = users_start.user WHERE users_start.user IS NULL\n",
      "CANNOT FETCHING DATA\n",
      "\n",
      "COMPLETE 89 from 106\n",
      "SELECT COUNT(DISTINCT user_pseudo_id) AS users_with_daily_events FROM pointx_fbs_rpt_dly WHERE YEAR(event_date) = 2022 AND MONTH(event_date) = 5 GROUP BY event_date HAVING COUNT(DISTINCT event_date) = DATEDIFF('2022-05-01', '2022-05-31') + 1;\n",
      "CANNOT FETCHING DATA\n",
      "\n",
      "COMPLETE 90 from 106\n",
      "SELECT  ((all_user_may.count_distinct_user_pseudo_id - all_users_april.count_distinct_user_pseudo_id )/ all_users_april.count_distinct_user_pseudo_id)*100 AS float_percentage FROM (   SELECT COUNT(DISTINCT user_pseudo_id) AS count_distinct_user_pseudo_id   FROM pointx_fbs_rpt_dly   WHERE MONTH(event_date) = 5 ) AS all_user_may CROSS JOIN (   SELECT COUNT(DISTINCT user_pseudo_id) AS count_distinct_user_pseudo_id   FROM pointx_fbs_rpt_dly   WHERE MONTH(event_date) = 4 ) AS all_users_april;\n",
      "CANNOT FETCHING DATA\n",
      "\n",
      "COMPLETE 91 from 106\n",
      "SELECT AVG(total_engagement_sec) FROM( SELECT user_pseudo_id, SUM(engagement_time_msec)/1000 AS total_engagement_sec   FROM pointx_fbs_rpt_dly   GROUP BY user_pseudo_id   HAVING(total_engagement_sec >= 10)   ORDER BY total_engagement_sec DESC   LIMIT 10)\n",
      "[(307.6,)]\n",
      "\n",
      "COMPLETE 92 from 106\n",
      "SELECT device_mobile_model_name, (COUNT(DISTINCT user_pseudo_id)/SUM(COUNT(DISTINCT user_pseudo_id)) OVER ())*100 AS total_devices_percentage FROM pointx_fbs_rpt_dly GROUP BY device_mobile_model_name\n",
      "[('1', 0), ('Pixel 6', 0), ('Redmi Note 5', 0), ('SH-01L', 0), ('SM-A526B', 0), ('SM-F711B', 0), ('SM-G985F', 0), ('SM-G998B', 0), ('SM-S908E', 0), ('iPhone', 0), ('iPhone 11', 0), ('iPhone 11 Pro Max', 0), ('iPhone 12 Pro', 0), ('iPhone 13 Pro', 0), ('iPhone 13 Pro Max', 0), ('iPhone 8 Plus', 0), ('iPhone XR', 0), ('iPhone XS', 0)]\n",
      "\n",
      "COMPLETE 93 from 106\n",
      "SELECT AVG(total_engagement_sec) FROM(SELECT user_pseudo_id, SUM(engagement_time_msec)/1000 AS total_engagement_sec   FROM pointx_fbs_rpt_dly   WHERE WEEKDAY(event_date) IN (0,1,2,3,4) AND WEEKDAY(event_date) IN (5,6)   GROUP BY user_pseudo_id   HAVING(total_engagement_sec >= 10)   ORDER BY total_engagement_sec DESC)\n",
      "CANNOT FETCHING DATA\n",
      "\n",
      "COMPLETE 94 from 106\n",
      "SELECT ((count_distinct_user_pseudo_id_this_year -  count_distinct_user_pseudo_id_last_year)/ count_distinct_user_pseudo_id_last_year) * 100 FROM(      SELECT COUNT(DISTINCT user_pseudo_id) AS count_distinct_user_pseudo_id_this_year     FROM pointx_fbs_rpt_dly     WHERE  YEAR(event_date) = YEAR(CURRENT_DATE) ) AS total_users_this_year, (     SELECT COUNT(DISTINCT user_pseudo_id) AS count_distinct_user_pseudo_id_last_year     FROM pointx_fbs_rpt_dly     WHERE  YEAR(event_date) = YEAR(CURRENT_DATE) - 1 ) AS total_users_last_year;\n",
      "CANNOT FETCHING DATA\n",
      "\n",
      "COMPLETE 95 from 106\n",
      "SELECT (CAST(active_user_everyday AS FLOAT) / all_users.count_distinct_user_pseudo_id) * 100 FROM(    SELECT COUNT(DISTINCT CASE WHEN event_date BETWEEN '2022-05-23' AND '2022-05-25' THEN user_pseudo_id END) AS active_user_everyday   FROM pointx_fbs_rpt_dly   WHERE event_date BETWEEN '2022-05-23' AND '2022-05-25'   GROUP BY user_pseudo_id   HAVING COUNT(DISTINCT event_date) = DATEDIFF('2022-05-24', '2022-05-23') + 1 ) AS active_user_everyday, (     SELECT COUNT(DISTINCT user_pseudo_id) AS count_distinct_user_pseudo_id     FROM pointx_fbs_rpt_dly ) AS all_users;\n",
      "CANNOT FETCHING DATA\n",
      "\n",
      "COMPLETE 96 from 106\n",
      "SELECT COUNT(DISTINCT user_pseudo_id) FROM ( SELECT DISTINCT user_pseudo_id, COUNT(event_name)   FROM pointx_fbs_rpt_dly   WHERE MONTH(event_date) = \"5\" AND event_name = 'user_engagement'   GROUP BY event_date, user_pseudo_id   HAVING SUM(engagement_time_msec)/1000 >= 10 AND COUNT(event_name) >= 2 )\n",
      "CANNOT FETCHING DATA\n",
      "\n",
      "COMPLETE 97 from 106\n",
      "SELECT AVG(time_difference) FROM(SELECT  user_pseudo_id,         TIMESTAMPDIFF(MINUTE,           MIN(CASE WHEN event_name = 'first_open' THEN event_timestamp END),           MIN(CASE WHEN event_name = 'payatmcht_select_product' THEN event_timestamp END)         ) AS time_difference     FROM (SELECT user_pseudo_id, event_name, event_timestamp           FROM pointx_fbs_rpt_dly           WHERE user_pseudo_id IN (             SELECT DISTINCT user_pseudo_id             FROM pointx_fbs_rpt_dly             WHERE event_name IN (\"payatmcht_select_product\",\"first_open\")     )     AND (event_name IN (\"payatmcht_select_product\",\"first_open\"))     ORDER BY user_pseudo_id, event_timestamp)     GROUP BY user_pseudo_id     HAVING time_difference IS NOT NULL)\n",
      "CANNOT FETCHING DATA\n",
      "\n",
      "COMPLETE 98 from 106\n",
      "SELECT geo_region, COUNT(DISTINCT user_pseudo_id) AS total_users FROM pointx_fbs_rpt_dly WHERE geo_country = \"Thailand\" GROUP BY geo_region\n",
      "[('Bangkok', 19), ('Chon Buri', 1), ('Nonthaburi', 2), ('Phra Nakhon Si Ayutthaya', 1)]\n",
      "\n",
      "COMPLETE 99 from 106\n",
      "SELECT (COUNT(DISTINCT active_user_weekday.user_pseudo_id) * 100)/ COUNT(count_distinct_user_pseudo_id) AS percentage_active_user_weekday FROM (   SELECT event_date, user_pseudo_id   FROM pointx_fbs_rpt_dly   WHERE WEEKDAY(event_date) IN (0,1,2,3,4) AND MONTH(event_date) = \"5\" AND YEAR(event_date) = \"2022\"   GROUP BY event_date, user_pseudo_id   HAVING SUM(engagement_time_msec)/1000 >= 10 ) AS active_user_weekday CROSS JOIN(     SELECT COUNT(DISTINCT user_pseudo_id) AS count_distinct_user_pseudo_id     FROM pointx_fbs_rpt_dly ) AS all_users;\n",
      "CANNOT FETCHING DATA\n",
      "\n",
      "COMPLETE 100 from 106\n",
      "SELECT (CAST(users_use_on_weekends AS FLOAT) / all_users.count_distinct_user_pseudo_id) * 100 AS percentage_only_weekends FROM (     SELECT COUNT(DISTINCT user_pseudo_id) AS users_use_on_weekends     FROM pointx_fbs_rpt_dly     WHERE WEEKDAY(event_date) IN (5, 6) ) AS users_only_weekends, (     SELECT COUNT(DISTINCT user_pseudo_id) AS count_distinct_user_pseudo_id     FROM pointx_fbs_rpt_dly ) AS all_users;\n",
      "CANNOT FETCHING DATA\n",
      "\n",
      "COMPLETE 101 from 106\n",
      "SELECT  event_date, AVG(time_difference_seconds) AS avg_time_diff_sec FROM (SELECT  event_date,                  session_id,                  COUNT(event_name) AS total_event,                 TIMESTAMPDIFF(                         SECOND,                         MIN(event_timestamp),                         MAX(event_timestamp)                 ) AS time_difference_seconds         FROM (         SELECT  user_pseudo_id,                  event_name,                  event_date,                  event_timestamp,                 SUM (   CASE WHEN event_name = 'session_start' THEN 1 ELSE 0 END)                          OVER (PARTITION BY event_date ORDER BY event_timestamp) AS session_id                  FROM pointx_fbs_rpt_dly)         WHERE event_name != \"app_remove\"         GROUP BY event_date, session_id) GROUP BY event_date\n",
      "CANNOT FETCHING DATA\n",
      "\n",
      "COMPLETE 102 from 106\n",
      "SELECT   (COUNT(DISTINCT CASE WHEN WEEKDAY(event_date) IN (5, 6) THEN user_pseudo_id END) / COUNT(DISTINCT user_pseudo_id)) * 100  FROM pointx_fbs_rpt_dly;\n",
      "CANNOT FETCHING DATA\n",
      "\n",
      "COMPLETE 103 from 106\n",
      "SELECT COUNT(DISTINCT user_pseudo_id) FROM pointx_fbs_rpt_dly WHERE MONTH(event_date) = 5\n",
      "CANNOT FETCHING DATA\n",
      "\n",
      "COMPLETE 104 from 106\n",
      "SELECT SUM(user_with_multiple_devices) AS total_users_with_multiple_devices FROM (SELECT COUNT(DISTINCT user_pseudo_id) as user_with_multiple_devices         FROM pointx_fbs_rpt_dly         GROUP BY user_pseudo_id         HAVING COUNT(DISTINCT device_category) >= 1     )\n",
      "[(25,)]\n",
      "\n",
      "COMPLETE 105 from 106\n",
      "SELECT event_name FROM pointx_fbs_rpt_dly GROUP BY event_name ORDER BY COUNT(DISTINCT user_pseudo_id) DESC LIMIT 10\n",
      "[('session_start',), ('screen_view',), ('user_engagement',), ('pointx_dealoftheday_landing',), ('mypointx_landing',), ('pointx_home_bottom_bar',), ('pointx_view_mypoint',), ('payatmcht_landing',), ('virtualmall_landing',), ('pointx_virtualmall_bottom_bar',)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "measurement_data = {\n",
    "    \"Question\" : [],\n",
    "    \"Actual result\" : [],\n",
    "    \"NSQL sql\" : [],\n",
    "    \"NSQL result\" : [],\n",
    "    \"LLM [MASK] sql\" : [],\n",
    "    \"LLM [MASK] result\": [],\n",
    "    \"LLM sql\" : [],\n",
    "    \"LLM result\" : []\n",
    "}\n",
    "\n",
    "for i, row in pointx_nlqsql_df.iterrows():\n",
    "    actual_sql = row['Actual SQL'].replace('\\xa0', \"\").replace(\"pointx_fbs_txn_rpt_dly\", \"pointx_fbs_rpt_dly\").strip()\n",
    "    llm_columns = ast.literal_eval(row['LLM result'])\n",
    "    question = row['Question']\n",
    "    try:\n",
    "        tables = ast.literal_eval(row['Table'])\n",
    "    except ValueError:\n",
    "        tables = [row['Table']]\n",
    "    used_schema = gen_used_schema(tables, llm_columns)\n",
    "    full_prompt = create_prompt(question, used_schema)\n",
    "    nsql_result = gen_sql(full_prompt)\n",
    "    nsql_mask = mask_column(nsql_result, llm_columns)\n",
    "    used_schema_description = gen_schema_description(used_schema)\n",
    "    llm_sql = llm_gensql_chain({\"domain_knowledge\": used_schema_description, \"question\": question})['result']\n",
    "    llm_fill_columns = llm_fill_columns_chain({\"domain_knowledge\": used_schema_description, \n",
    "                                               \"mask_column\": nsql_mask,\n",
    "                                               \"question\": question})['result']\n",
    "    \n",
    "    measurement_data['Question'].append(question)\n",
    "    measurement_data[\"Actual result\"].append(query_pointx_db(actual_sql))\n",
    "    measurement_data[\"NSQL sql\"].append(nsql_result)\n",
    "    measurement_data[\"NSQL result\"].append(query_pointx_db(nsql_result))\n",
    "    measurement_data[\"LLM [MASK] sql\"].append(llm_fill_columns)\n",
    "    measurement_data[\"LLM [MASK] result\"].append(query_pointx_db(llm_fill_columns))\n",
    "    measurement_data[\"LLM sql\"].append(llm_sql)\n",
    "    measurement_data['LLM result'].append(query_pointx_db(llm_sql))\n",
    "\n",
    "    print(f\"COMPLETE {i+1} from {pointx_nlqsql_df.shape[0] + 1}\")\n",
    "    print(actual_sql)\n",
    "    print(query_pointx_db(actual_sql))\n",
    "    print(llm_sql)\n",
    "    print(nsql_result)\n",
    "    print(nsql_mask)\n",
    "    print(llm_fill_columns)\n",
    "    print()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "measurement_df = pd.DataFrame(measurement_data)\n",
    "merged_df = pd.merge(pointx_nlqsql_df, measurement_df, on='Question', how='outer')\n",
    "merged_df.to_excel('EXP_LLM_SQLresult.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'CANNOT FETCHING DATA'"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_pointx_db('SELECT COUNT(DISTINCT user_pseudo_id) FROM pointx_fbs_rpt_dly WHERE MONTH(event_date) = 5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'SELECT event_name FROM pointx_fbs_txn_rpt_dly GROUP BY event_name ORDER BY COUNT(DISTINCT user_pseudo_id) DESC LIMIT 10'"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "row['Actual SQL'].replace('\\xa0', \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
