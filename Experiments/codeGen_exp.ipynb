{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare models experiments with PointX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os, json, sqlite3, asyncio, random, re\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "\n",
    "from SchemaLinking import SchemaLinking\n",
    "import warnings\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import google.generativeai as genai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv('../.env')\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"../models/nsql-350M\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"../models/nsql-350M\")\n",
    "\n",
    "GOOGLE_API_KEY = os.environ.get('GOOGLE_API_KEY')\n",
    "DEEPSEEK_API_KEY = os.environ.get('DEEPSEEK_API_KEY')\n",
    "OPENAI_API_KEY = os.environ.get('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_content_puresql = \"\"\"You are a helpful assistant for generate SQL query from user-specified questions. \n",
    "please return only answer of sql string query result !!! \n",
    "Do not return any other format the user has provided to you.\n",
    "This is example of output format which user expect from you\n",
    "query : 'SELECT...'\n",
    "\"\"\"\n",
    "\n",
    "system_content_schemaprovide = \"\"\"You are a helpful assistant for generate SQL query from user-specified questions and schema.\n",
    "User provides you with a question.\n",
    "Please return only a sql string query results.\n",
    "Do not return any other format the user has provided to you.\n",
    "This is example of output format which user expect from you\n",
    "query : 'SELECT...'\n",
    "\"\"\"\n",
    "\n",
    "system_content_fillmask = \"\"\"You are a helpful assistant for generate SQL query from user-specified questions and schema. \n",
    "User has some SQL where the [MASK] columns, condition values and tables are syntaxed and User wants you to respond to output that populates the [MASK] column of the SQL input followed by the question and schema description (name - description - data type).\n",
    "If you don't know which column to fill in Do not include columns that you have created yourself. And only columns defined from the schema must be used. \n",
    "Do not use columns from other tables or schema. must also be used from the same table defined in the input.\n",
    "If you must enter conditional values Please decide the format or value based on the sample values of that column.\n",
    "If that column has many too long category value please decide base on column description.\n",
    "please return only the answer of sql string query result!!! ('SELECT...')\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_shot_prompt_mask = \"\"\"For example:\n",
    "table :     cat - this table contain cat information \n",
    "columns :    id - number for identify cat | number\n",
    "            name - name of cat | text\n",
    "            age - age of cat | number\n",
    "            birth_date - pet birthday in format 'YYYY-MM-DD' | datetime\n",
    "            gender - gender of cat (male, female) | text\n",
    "\n",
    "question : Show me number of cat for each gender which born before March 23, 2011.\n",
    "input : SELECT [MASK], COUNT([MASK]) FROM [MASK] WHERE [MASK] < [MASK] GROUP BY [MASK] ;\n",
    "query : SELECT gender, COUNT(*) FROM cat WHERE birth_date < '2011-03-23' GROUP BY gender;\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "zero_shot_prompt = \"\"\"For example:\n",
    "table :     cat - this table contain cat information \n",
    "columns :    id - number for identify cat | number\n",
    "            name - name of cat | text\n",
    "            age - age of cat | number\n",
    "            birth_date - pet birthday in format 'YYYY-MM-DD' | datetime\n",
    "            gender - gender of cat (male, female) | text\n",
    "\n",
    "question : 'Show me number of cat for each gender which born before March 23, 2011.'\n",
    "query : 'SELECT gender, COUNT(*) FROM cat WHERE birth_date < '2011-03-23' GROUP BY gender;'\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_nsql_prompt(schema_link:object, question:str, used_schema:dict) -> str:\n",
    "    \"\"\"\n",
    "    Generate a prompt for applying into SQL generation model based on the question and schema.\n",
    "\n",
    "    Parameters:\n",
    "    schema_link (object): The instance of the class containing schema information.\n",
    "    question (str): The question for which the prompt is generated.\n",
    "    used_schema (dict): A dictionary containing tables as keys and lists of columns as values after filtering the schema.\n",
    "\n",
    "    Returns:\n",
    "    str: A prompt for applying into SQL generation model.\n",
    "\n",
    "    Example:\n",
    "    prompt = create_prompt(schema_instance, \"What are the total sales?\", \n",
    "                          { 'sales': {'date' : 0.3, 'amount' : 0.61}, \n",
    "                            'products': {'name' : 0.23, 'price' : 0.57}})\n",
    "    print(prompt)\n",
    "\n",
    "    CREATE TABLE sales ( date DATE, amount INT,PRIMARY KEY (\"date\") )\n",
    "    -- Using valid SQLite, answer the following questions for the tables provided above.\n",
    "    -- What are the total sales?\n",
    "    SELECT\n",
    "    \"\"\"\n",
    "    full_sql = \"\"\n",
    "    for table, columns in used_schema.items():\n",
    "        if not len(columns): continue       # pass this table when no column\n",
    "        primary_keys = schema_link.schema_datatypes[table][\"JOIN_KEY\"][\"PK\"]\n",
    "        foreign_keys = list(schema_link.schema_datatypes[table][\"JOIN_KEY\"][\"FK\"].keys())\n",
    "        join_table_key = primary_keys + foreign_keys\n",
    "        \n",
    "        sql = f\"CREATE TABLE {table} (\"\n",
    "        for column in columns:\n",
    "            if column in join_table_key and len(join_table_key): join_table_key.remove(column)\n",
    "            try:\n",
    "                sql += f' {column} {schema_link.schema_datatypes[table][\"COLUMNS\"][column]},'\n",
    "            except KeyError: \n",
    "                print(f\"KeyError :{column}\")\n",
    "                \n",
    "        if len(join_table_key): # key for join of table are remaining\n",
    "            for column in join_table_key:\n",
    "                sql += f' {column} {schema_link.schema_datatypes[table][\"COLUMNS\"][column]},'\n",
    "\n",
    "        # All table contain PK (maybe)\n",
    "        if len(primary_keys):\n",
    "            sql += 'PRIMARY KEY ('\n",
    "            for pk_type in primary_keys: sql += f'\"{pk_type}\" ,'\n",
    "            sql = sql[:-1] + \"),\"\n",
    "\n",
    "        if len(foreign_keys):\n",
    "            for fk, ref_table_column in schema_link.schema_datatypes[table][\"JOIN_KEY\"][\"FK\"].items():\n",
    "                sql += f' FOREIGN KEY (\"{fk}\") REFERENCES \"{list(ref_table_column.keys())[0]}\" (\"{list(ref_table_column.values())[0]}\"),'\n",
    "\n",
    "        sql = sql[:-1] + \" )\\n\\n\"\n",
    "        full_sql += sql\n",
    "    prompt = full_sql + \"-- Using valid SQLite, answer the following questions for the tables provided above.\"\n",
    "    prompt = prompt + '\\n' + '-- ' + question\n",
    "    prompt = prompt + '\\n' + \"SELECT\"\n",
    "\n",
    "    return prompt\n",
    "\n",
    "def generate_nsql_sql(prompt):\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "        generated_ids = model.generate(input_ids, max_length=1000)\n",
    "        sql = tokenizer.decode(generated_ids[0], skip_special_tokens=True).split('\\n')[-1]\n",
    "    return sql\n",
    "\n",
    "async def LLM_gensql(full_prompt:str, system_content:str, llm_model:str) -> str:\n",
    "        \"\"\"\n",
    "        Generate SQL query followed by prompt\n",
    "\n",
    "        Parameters:\n",
    "        prompt (str): prompt for generate result\n",
    "        llm_model (str): model-service name for generate result\n",
    "\n",
    "        Returns:\n",
    "        str: The complete SQL query.\n",
    "        \"\"\"\n",
    "        if llm_model in ['gemini-pro']:\n",
    "            try:\n",
    "                gemini_prompt = system_content + full_prompt\n",
    "                genai.configure(api_key=GOOGLE_API_KEY)\n",
    "                gemini_model = genai.GenerativeModel(llm_model)\n",
    "                gemini_model.temperature = 0\n",
    "                response = gemini_model.generate_content(gemini_prompt)\n",
    "                return response.text\n",
    "            \n",
    "            except Exception as e:\n",
    "                 return f\"Google AI Error : {e}\"\n",
    "            \n",
    "        elif llm_model in ['gpt-3.5-turbo', 'gpt-4-0125-preview']:\n",
    "            \n",
    "            API_KEY = OPENAI_API_KEY\n",
    "            base_url = \"https://api.openai.com/v1\"\n",
    "            # return None\n",
    "        \n",
    "        elif llm_model in ['deepseek-coder', 'deepseek-chat']:\n",
    "            API_KEY = DEEPSEEK_API_KEY\n",
    "            base_url = \"https://api.deepseek.com/v1\"\n",
    "        try:\n",
    "            print(llm_model)\n",
    "            \n",
    "            client = OpenAI(api_key=API_KEY, base_url=base_url)\n",
    "            response = client.chat.completions.create(\n",
    "                model=llm_model,\n",
    "                messages=[\n",
    "                        {\"role\": \"system\",\n",
    "                            \"content\": system_content},\n",
    "                        {\"role\": \"user\", \n",
    "                            \"content\": full_prompt},\n",
    "                        ],\n",
    "                stop=['\\n'],\n",
    "                temperature=0\n",
    "            )\n",
    "            return response.choices[0].message.content\n",
    "        \n",
    "        except Exception as e:\n",
    "            return f\"API Error :{e}\"\n",
    "            \n",
    "def get_reason(schema_link:object, sql_result:str) -> str:\n",
    "    \"\"\"\n",
    "    Get the reason message related to the selected columns and tables from the schema based on the SQL query.\n",
    "\n",
    "    Parameters:\n",
    "    schema_link (object): The instance of the class containing schema information.\n",
    "    sql_result (str): The SQL query result for which the reason message is generated.\n",
    "\n",
    "    Returns:\n",
    "    str: The reason message explaining the selection of columns and tables from the schema.\n",
    "\n",
    "    Example:\n",
    "    get_reason(schema_instance, \"SELECT column1, column2 FROM table1 WHERE column3 = 'value'\")\n",
    "\n",
    "    Table - table1 : Description of table1\n",
    "        Column - column1 : Description of column1\n",
    "        Column - column2 : Description of column2\n",
    "        Column - column3 : Description of column3\n",
    "    \"\"\"\n",
    "\n",
    "    table_col_sql = schema_link.table_col_of_sql(sql_result)\n",
    "    reason = \"\"\n",
    "\n",
    "    for table, cols in table_col_sql.items():\n",
    "        _df = schema_link.column_info_df[schema_link.column_info_df['Table'] == table][['Column', 'Description']].drop_duplicates()\n",
    "        table_reason = f\"Table - {table}\\t: {schema_link.table_descriptions[table]['text']}\\n\"\n",
    "        if len(cols):       # have columns of table\n",
    "            col_reason = \"\\n\".join([f\"\\tColumn - {c}\\t: {_df.loc[_df['Column'] == c, 'Description'].values[0]}\" for c in cols])\n",
    "        else: col_reason = \"\"\n",
    "        reason += str(table_reason + col_reason + \"\\n\\n\")\n",
    "\n",
    "    return reason"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_llm_prompt(schema_link:object, used_schema:dict, question:str, masked_query:str, \n",
    "                      few_shot:str=zero_shot_prompt_mask, is_marked:bool=True, is_fewshot:bool=True) -> str:\n",
    "\n",
    "    full_prompt = \"\"\n",
    "    for table_name, column_score in used_schema.items():\n",
    "        _df = schema_link.column_info_df[schema_link.column_info_df['Table'] == table_name][['Column', 'Description']].drop_duplicates()\n",
    "        full_prompt += f\"\\ntable : {table_name} - {schema_link.table_descriptions[table_name]['text']}\\ncolumns:\"\n",
    "\n",
    "        for column_name in column_score:\n",
    "            full_prompt += f\"\\t{column_name} - {_df[_df['Column'] == column_name]['Description'].values[0]}\"\n",
    "            full_prompt += f\" | {schema_link.schema_datatypes[table_name]['COLUMNS'][column_name]}\\n\"\n",
    "\n",
    "    full_prompt += f\"question : {question}\\n\"\n",
    "    if is_marked: full_prompt += f\"input : {masked_query}\"\n",
    "    if is_fewshot: full_prompt = few_shot + full_prompt\n",
    "    \n",
    "    return full_prompt + \"\\nquery : \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_pointx_db(sql_query):\n",
    "    try:\n",
    "        conn = sqlite3.connect(f'../src/pointx/database/pointx.db')\n",
    "        cursor = conn.cursor()\n",
    "    except:\n",
    "        return \"CANNOT CONNECT DATABASE\"\n",
    "    try:\n",
    "        cursor.execute(sql_query)\n",
    "        results = cursor.fetchall()\n",
    "    except:\n",
    "        return \"CANNOT FETCHING DATA\"\n",
    "    conn.close()\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def table_col_of_sql(schema_link, sql_query:str) -> dict:\n",
    "        \"\"\"\n",
    "        Extract tables and their corresponding columns from the given SQL query.\n",
    "\n",
    "        Parameters:\n",
    "        sql_query (str): The SQL query from which tables and columns need to be extracted.\n",
    "\n",
    "        Returns:\n",
    "        dict: A dictionary containing tables as keys and lists of columns as values.\n",
    "\n",
    "        Example:\n",
    "        SchemaLinking.table_col_of_sql(\"SELECT column1, column2 FROM table1 WHERE column3 = 'value'\")\n",
    "        {'table1': ['column1', 'column2', 'column3']}\n",
    "        \"\"\"\n",
    "        \n",
    "        selected_schema = {}\n",
    "        query_split = re.split(schema_link.split_pattern, sql_query)\n",
    "        for table in schema_link.schema_datatypes.keys():\n",
    "            if table in query_split:\n",
    "                selected_col = []\n",
    "                for col in schema_link.schema_datatypes[table]['COLUMNS'].keys():\n",
    "                    if col in query_split: selected_col.append(col)\n",
    "                selected_schema[table] = selected_col\n",
    "\n",
    "        return selected_schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"../src/src_dev/pointx/embedded_data.json\", \"r\") as f:\n",
    "    domain = json.load(f)\n",
    "\n",
    "schema_link = SchemaLinking(domain)\n",
    "\n",
    "async def ChatQ_pipeline(question:str, domain_tables:list, llm_model_name:str, \n",
    "                         max_n:int=10,verbose:bool=True, get_final_prompt:bool=False):\n",
    "\n",
    "    if not domain_tables: domain_tables = list(domain['tables'].keys())\n",
    "    used_schema = schema_link.filter_schema(question, domain_tables, max_n=max_n)\n",
    "    nsql_prompt = create_nsql_prompt(schema_link, question, used_schema)\n",
    "    nsql_sql_result = generate_nsql_sql(nsql_prompt)\n",
    "    masked_query = schema_link.masking_query(nsql_sql_result)\n",
    "    llm_prompt = create_llm_prompt(schema_link, used_schema, question, masked_query)\n",
    "    if get_final_prompt: return llm_prompt\n",
    "    llm_result = await LLM_gensql(llm_prompt, system_content_fillmask, llm_model_name)\n",
    "    if verbose:\n",
    "        reason = get_reason(schema_link, llm_result)\n",
    "        print(\"========= QUESTION =========\")\n",
    "        print(question)\n",
    "        print()\n",
    "        print(\"========= NSQL SQL =========\")\n",
    "        print(nsql_sql_result)\n",
    "        print()\n",
    "        print(\"========= LLM SQL =========\")\n",
    "        print(llm_result)\n",
    "        print()\n",
    "        print(\"========= REASON =========\")\n",
    "        print(reason)\n",
    "        print()\n",
    "        print(\"========= SCHEMA =========\")\n",
    "        print(used_schema)\n",
    "        print()\n",
    "\n",
    "    return llm_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_pair_df = pd.read_csv(\"../src/pointx/PointX_questionpair.csv\")[['Table', 'Question', 'Actual SQL']]\n",
    "print(q_pair_df.shape)\n",
    "q_pair_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # delete duplicate experiment record\n",
    "\n",
    "# _df = pd.read_excel('results/temp_model_description_experiments.xlsx')\n",
    "# print(_df.shape)\n",
    "# _df.drop_duplicates(subset=['Question'], inplace=True)\n",
    "# print(_df.shape)\n",
    "# _df.to_excel('results/temp_model_description_experiments.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = await ChatQ_pipeline(\"How many users have not used the app for more than 3 months?\",\n",
    "                            ['pointx_fbs_rpt_dly'], \"gpt-3.5-turbo\", verbose=True)\n",
    "query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GenAI Model with provide schema-description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_list(input_list, chunk_size):\n",
    "    for i in range(0, len(input_list), chunk_size):\n",
    "        yield input_list[i:i + chunk_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_columns(schema_link:object, table_name:str, used_columns:dict, noise:int=20) -> dict:\n",
    "    table_columns = list(schema_link.column_info_df[schema_link.column_info_df['Table'] == table_name]['Column'].unique())\n",
    "    remaining_values = [value for value in table_columns if value not in used_columns[table_name]]\n",
    "    remaining_count = noise - len(used_columns[table_name])\n",
    "    random_selected_values = random.sample(remaining_values, remaining_count)\n",
    "    result = list(used_columns[table_name]) + random_selected_values\n",
    "    used_schema = { table_name : result}\n",
    "    return used_schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def yeild_columns(schema_link, sql_queries:list) -> list:\n",
    "\n",
    "    selected_schema = {}\n",
    "    for sql_query in sql_queries:\n",
    "        \n",
    "        query_split = re.split(schema_link.split_pattern, sql_query)\n",
    "        \n",
    "        for table in schema_link.schema_datatypes.keys():\n",
    "            if table in query_split:\n",
    "                selected_col = []\n",
    "                for col in schema_link.schema_datatypes[table]['COLUMNS'].keys():\n",
    "                    if col in query_split:\n",
    "                        selected_col.append(col)\n",
    "                selected_schema[table] = selected_col\n",
    "    \n",
    "    return selected_schema\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_columns(schema_link, 'pointx_fbs_rpt_dly', { \"pointx_fbs_rpt_dly\":{'month_id':0.2}}, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_data = {\n",
    "    \"Question\" : [],\n",
    "    \"Desc DeepSeek\" : [],\n",
    "    \"Desc GPT3.5\" : [],\n",
    "    \"Desc GPT4\" : [],\n",
    "    \"Desc Gemini\" : []\n",
    "}\n",
    "\n",
    "\n",
    "i = 0\n",
    "n_chunk = 5\n",
    "\n",
    "temp_result_file = \"results/temp_model_description_experiments.xlsx\"\n",
    "\n",
    "if os.path.exists(temp_result_file):\n",
    "    print(\"File exist\")\n",
    "    _df = pd.read_excel(temp_result_file)\n",
    "    i = _df.shape[0]\n",
    "    for key in predict_data:\n",
    "        if key in _df.columns:\n",
    "            predict_data[key] = _df[key].tolist()\n",
    "\n",
    "\n",
    "for table_name in q_pair_df['Table'].unique():\n",
    "    table_df = q_pair_df[q_pair_df['Table'] == table_name]\n",
    "    table_questions = table_df['Question'].to_list()\n",
    "    table_actualSQL = table_df['Actual SQL'].to_list()\n",
    "    # list_chunk_questions = list(split_list(table_questions, n_chunk))\n",
    "    # list_chunk_actualSQL = list(split_list(table_actualSQL, n_chunk))\n",
    "    # cannot used full schema because context in larger than handle\n",
    "    # used_schema = { table_name : list(schema_link.column_info_df[schema_link.column_info_df['Table'] == table_name]['Column'].unique())}\n",
    "    \n",
    "    \n",
    "    for question, sql in zip(table_questions, table_actualSQL):\n",
    "        if question in predict_data['Question']: continue\n",
    "        # used_columns = yeild_columns(schema_link,sql)\n",
    "        used_columns = table_col_of_sql(schema_link, sql)\n",
    "        used_schema = sample_columns(schema_link, table_name, used_columns)\n",
    "        table_prompt = create_llm_prompt(schema_link, used_schema, question, zero_shot_prompt, is_marked=False)\n",
    "\n",
    "        gemini_results = LLM_gensql(table_prompt, system_content_schemaprovide, 'gemini-pro')\n",
    "        gpt3_5_results = LLM_gensql(table_prompt, system_content_schemaprovide, 'gpt-3.5-turbo')\n",
    "        gpt4_results = LLM_gensql(table_prompt, system_content_schemaprovide, 'gpt-4-0125-preview')\n",
    "        deepseek_result = LLM_gensql(table_prompt, system_content_schemaprovide, 'deepseek-coder')\n",
    "\n",
    "        print(\"Generating SQL...\")\n",
    "        gemini_results, deepseek_result, gpt3_5_results, gpt4_results = await asyncio.gather(gemini_results, deepseek_result, gpt3_5_results, gpt4_results)\n",
    "\n",
    "        print(question)\n",
    "        print(gemini_results, deepseek_result, gpt3_5_results, gpt4_results, sep='\\n')\n",
    "\n",
    "        predict_data['Question'].append(question)\n",
    "        predict_data['Desc DeepSeek'].append(deepseek_result)\n",
    "        predict_data['Desc GPT3.5'].append(gpt3_5_results)\n",
    "        predict_data['Desc GPT4'].append(gpt4_results)\n",
    "        predict_data['Desc Gemini'].append(gemini_results)\n",
    "\n",
    "        i += 1\n",
    "        if not i % 5:\n",
    "            save_df = pd.DataFrame(predict_data)\n",
    "            save_df.to_excel(temp_result_file, index=False)\n",
    "            print(\"SAVE TEMP COMPLETE\", i)\n",
    "        \n",
    "    save_df = pd.DataFrame(predict_data)\n",
    "    save_df.to_excel(temp_result_file, index=False)\n",
    "    print(\"SAVE TEMP COMPLETE\", i)\n",
    "\n",
    "save_df = pd.DataFrame(predict_data)\n",
    "save_df.to_excel(\"results/model_description_experiments.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GenAI Model with Framework pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_data = {\n",
    "    \"Question\" : [],\n",
    "    \"Desc DeepSeek\" : [],\n",
    "    \"Desc GPT3.5\" : [],\n",
    "    \"Desc GPT4\" : [],\n",
    "    \"Desc Gemini\" : []\n",
    "}\n",
    "\n",
    "\n",
    "i = 0\n",
    "n_chunk = 5\n",
    "\n",
    "temp_result_file = \"results/temp_model_chatq_experiments.xlsx\"\n",
    "\n",
    "if os.path.exists(temp_result_file):\n",
    "    print(\"File exist\")\n",
    "    _df = pd.read_excel(temp_result_file)\n",
    "    i = _df.shape[0]\n",
    "    for key in predict_data:\n",
    "        if key in _df.columns:\n",
    "            predict_data[key] = _df[key].tolist()\n",
    "\n",
    "\n",
    "for table_name in q_pair_df['Table'].unique():\n",
    "    table_df = q_pair_df[q_pair_df['Table'] == table_name]\n",
    "    table_questions = table_df['Question'].to_list()\n",
    "    table_actualSQL = table_df['Actual SQL'].to_list()\n",
    "    \n",
    "    \n",
    "    for question, sql in zip(table_questions, table_actualSQL):\n",
    "        if question in predict_data['Question']: continue\n",
    "        \n",
    "        table_prompt = await ChatQ_pipeline(question, domain_tables=[table_name], \n",
    "                                            llm_model_name=None, max_n=10,get_final_prompt=True)\n",
    "        gemini_results = LLM_gensql(table_prompt, system_content_fillmask, 'gemini-pro')\n",
    "        gpt3_5_results = LLM_gensql(table_prompt, system_content_fillmask, 'gpt-3.5-turbo')\n",
    "        gpt4_results = LLM_gensql(table_prompt, system_content_fillmask, 'gpt-4-0125-preview')\n",
    "        deepseek_result = LLM_gensql(table_prompt, system_content_fillmask, 'deepseek-coder')\n",
    "\n",
    "        print(\"Generating SQL...\")\n",
    "        gemini_results, deepseek_result, gpt3_5_results, gpt4_results = await asyncio.gather(gemini_results, deepseek_result, gpt3_5_results, gpt4_results)\n",
    "\n",
    "        print(question)\n",
    "        print(gemini_results, deepseek_result, gpt3_5_results, gpt4_results, sep='\\n')\n",
    "\n",
    "        predict_data['Question'].append(question)\n",
    "        predict_data['Desc DeepSeek'].append(deepseek_result)\n",
    "        predict_data['Desc GPT3.5'].append(gpt3_5_results)\n",
    "        predict_data['Desc GPT4'].append(gpt4_results)\n",
    "        predict_data['Desc Gemini'].append(gemini_results)\n",
    "\n",
    "        i += 1\n",
    "        if not i % 5:\n",
    "            save_df = pd.DataFrame(predict_data)\n",
    "            save_df.to_excel(temp_result_file, index=False)\n",
    "            print(\"SAVE TEMP COMPLETE\", i)\n",
    "        \n",
    "    save_df = pd.DataFrame(predict_data)\n",
    "    save_df.to_excel(temp_result_file, index=False)\n",
    "    print(\"SAVE TEMP COMPLETE\", i)\n",
    "\n",
    "save_df = pd.DataFrame(predict_data)\n",
    "save_df.to_excel(\"results/model_chatq_experiments.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pure question without providing schema and description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_data = {\n",
    "    \"Question\" : [],\n",
    "    \"Desc DeepSeek\" : [],\n",
    "    \"Desc GPT3.5\" : [],\n",
    "    \"Desc GPT4\" : [],\n",
    "    \"Desc Gemini\" : []\n",
    "}\n",
    "\n",
    "\n",
    "i = 0\n",
    "n_chunk = 5\n",
    "\n",
    "temp_result_file = \"results/temp_model_pureQ_experiments.xlsx\"\n",
    "\n",
    "if os.path.exists(temp_result_file):\n",
    "    print(\"File exist\")\n",
    "    _df = pd.read_excel(temp_result_file)\n",
    "    i = _df.shape[0]\n",
    "    for key in predict_data:\n",
    "        if key in _df.columns:\n",
    "            predict_data[key] = _df[key].tolist()\n",
    "\n",
    "\n",
    "for table_name in q_pair_df['Table'].unique():\n",
    "    table_df = q_pair_df[q_pair_df['Table'] == table_name]\n",
    "    table_questions = table_df['Question'].to_list()\n",
    "    table_actualSQL = table_df['Actual SQL'].to_list()\n",
    "    \n",
    "    \n",
    "    for question, sql in zip(table_questions, table_actualSQL):\n",
    "        if question in predict_data['Question']: continue\n",
    "\n",
    "        gemini_results = LLM_gensql(question, system_content_puresql, 'gemini-pro')\n",
    "        gpt3_5_results = LLM_gensql(question, system_content_puresql, 'gpt-3.5-turbo')\n",
    "        gpt4_results = LLM_gensql(question, system_content_puresql, 'gpt-4-0125-preview')\n",
    "        deepseek_result = LLM_gensql(question, system_content_puresql, 'deepseek-coder')\n",
    "\n",
    "        print(\"Generating SQL...\")\n",
    "        gemini_results, deepseek_result, gpt3_5_results, gpt4_results = await asyncio.gather(gemini_results, deepseek_result, gpt3_5_results, gpt4_results)\n",
    "\n",
    "        print(question)\n",
    "        print(gemini_results, deepseek_result, gpt3_5_results, gpt4_results, sep='\\n')\n",
    "\n",
    "        predict_data['Question'].append(question)\n",
    "        predict_data['Desc DeepSeek'].append(deepseek_result)\n",
    "        predict_data['Desc GPT3.5'].append(gpt3_5_results)\n",
    "        predict_data['Desc GPT4'].append(gpt4_results)\n",
    "        predict_data['Desc Gemini'].append(gemini_results)\n",
    "\n",
    "        i += 1\n",
    "        if not i % 5:\n",
    "            save_df = pd.DataFrame(predict_data)\n",
    "            save_df.to_excel(temp_result_file, index=False)\n",
    "            print(\"SAVE TEMP COMPLETE\", i)\n",
    "        \n",
    "    save_df = pd.DataFrame(predict_data)\n",
    "    save_df.to_excel(temp_result_file, index=False)\n",
    "    print(\"SAVE TEMP COMPLETE\", i)\n",
    "\n",
    "save_df = pd.DataFrame(predict_data)\n",
    "save_df.to_excel(\"results/model_pureQ_experiments.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pure SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_data = {\n",
    "    \"Question\" : [],\n",
    "    \"ChatQ - NSQL\" : []\n",
    "}\n",
    "\n",
    "\n",
    "i = 0\n",
    "n_chunk = 5\n",
    "\n",
    "temp_result_file = \"results/temp_model_onlyNSQL_experiments.xlsx\"\n",
    "\n",
    "if os.path.exists(temp_result_file):\n",
    "    print(\"File exist\")\n",
    "    _df = pd.read_excel(temp_result_file)\n",
    "    i = _df.shape[0]\n",
    "    for key in predict_data:\n",
    "        if key in _df.columns:\n",
    "            predict_data[key] = _df[key].tolist()\n",
    "\n",
    "\n",
    "for table_name in q_pair_df['Table'].unique():\n",
    "    table_df = q_pair_df[q_pair_df['Table'] == table_name]\n",
    "    table_questions = table_df['Question'].to_list()\n",
    "    table_actualSQL = table_df['Actual SQL'].to_list()\n",
    "    \n",
    "    \n",
    "    for question, sql in zip(table_questions, table_actualSQL):\n",
    "        if question in predict_data['Question']: continue\n",
    "\n",
    "        gemini_results = LLM_gensql(question, system_content_puresql, 'gemini-pro')\n",
    "        gpt3_5_results = LLM_gensql(question, system_content_puresql, 'gpt-3.5-turbo')\n",
    "        gpt4_results = LLM_gensql(question, system_content_puresql, 'gpt-4-0125-preview')\n",
    "        deepseek_result = LLM_gensql(question, system_content_puresql, 'deepseek-coder')\n",
    "\n",
    "        print(\"Generating SQL...\")\n",
    "        gemini_results, deepseek_result, gpt3_5_results, gpt4_results = await asyncio.gather(gemini_results, deepseek_result, gpt3_5_results, gpt4_results)\n",
    "\n",
    "        print(question)\n",
    "        print(gemini_results, deepseek_result, gpt3_5_results, gpt4_results, sep='\\n')\n",
    "\n",
    "        predict_data['Question'].append(question)\n",
    "        predict_data['Desc DeepSeek'].append(deepseek_result)\n",
    "        predict_data['Desc GPT3.5'].append(gpt3_5_results)\n",
    "        predict_data['Desc GPT4'].append(gpt4_results)\n",
    "        predict_data['Desc Gemini'].append(gemini_results)\n",
    "\n",
    "        i += 1\n",
    "        if not i % 5:\n",
    "            save_df = pd.DataFrame(predict_data)\n",
    "            save_df.to_excel(temp_result_file, index=False)\n",
    "            print(\"SAVE TEMP COMPLETE\", i)\n",
    "        \n",
    "    save_df = pd.DataFrame(predict_data)\n",
    "    save_df.to_excel(temp_result_file, index=False)\n",
    "    print(\"SAVE TEMP COMPLETE\", i)\n",
    "\n",
    "save_df = pd.DataFrame(predict_data)\n",
    "save_df.to_excel(\"results/model_pureQ_experiments.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NSQL only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_data = {\n",
    "    \"Question\" : [],\n",
    "    \"pure NSQL\" : []\n",
    "}\n",
    "\n",
    "\n",
    "i = 0\n",
    "n_chunk = 5\n",
    "\n",
    "temp_result_file = \"results/temp_model_pureNSQL_experiments.xlsx\"\n",
    "\n",
    "if os.path.exists(temp_result_file):\n",
    "    print(\"File exist\")\n",
    "    _df = pd.read_excel(temp_result_file)\n",
    "    i = _df.shape[0]\n",
    "    for key in predict_data:\n",
    "        if key in _df.columns:\n",
    "            predict_data[key] = _df[key].tolist()\n",
    "\n",
    "\n",
    "for table_name in q_pair_df['Table'].unique():\n",
    "    table_df = q_pair_df[q_pair_df['Table'] == table_name]\n",
    "    table_questions = table_df['Question'].to_list()\n",
    "    table_actualSQL = table_df['Actual SQL'].to_list()\n",
    "    \n",
    "    \n",
    "    for question, sql in zip(table_questions, table_actualSQL):\n",
    "        if question in predict_data['Question']: continue\n",
    "        \n",
    "        used_schema = schema_link.filter_schema(question, [table_name], max_n=50)\n",
    "        nsql_prompt = create_nsql_prompt(schema_link, question, used_schema)\n",
    "        nsql_sql_result = generate_nsql_sql(nsql_prompt)\n",
    "\n",
    "        predict_data['Question'].append(question)\n",
    "        predict_data['pure NSQL'].append(nsql_sql_result)\n",
    "\n",
    "        i += 1\n",
    "        if not i % 5:\n",
    "            save_df = pd.DataFrame(predict_data)\n",
    "            save_df.to_excel(temp_result_file, index=False)\n",
    "            print(\"SAVE TEMP COMPLETE\", i)\n",
    "        \n",
    "    save_df = pd.DataFrame(predict_data)\n",
    "    save_df.to_excel(temp_result_file, index=False)\n",
    "    print(\"SAVE TEMP COMPLETE\", i)\n",
    "\n",
    "save_df = pd.DataFrame(predict_data)\n",
    "save_df.to_excel(\"results/model_pureNSQL_experiments.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Structure by masking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from SchemaLinking import SchemaLinking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def extract_sql_query(text):\n",
    "    \n",
    "    sql_patterns = [r'```sql(.*?)```', r'```(.*?)```', r'(SELECT.*?;)', r'(SELECT.*)']\n",
    "    \n",
    "    for pattern in sql_patterns:\n",
    "        match = re.search(pattern, text, re.DOTALL)\n",
    "        if match:\n",
    "            return match.group(1).strip()\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"../src/src_dev/pointx/embedded_data.json\", \"r\") as f:\n",
    "    domain = json.load(f)\n",
    "\n",
    "schema_link = SchemaLinking(domain)\n",
    "\n",
    "result_file = pd.read_excel('../src/pointx/NLQ2SQL model exp result.xlsx').iloc[:,3:]\n",
    "result_file = result_file.applymap(str)\n",
    "for col in result_file.columns:\n",
    "    result_file[col] = result_file[col].apply(extract_sql_query)\n",
    "result_file.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ncols = len(result_file.columns) // 2\n",
    "sql_df = result_file.iloc[:,:ncols]\n",
    "sql_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema_link.masking_query(\"SELECT customer_type, SUM(ntx) FROM pointx_cust_mly GROUP BY customer_type;\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_query(sql):\n",
    "    masked_sql = schema_link.masking_query(sql).replace('\\n',' ').strip()\n",
    "    cleaned_query = re.sub(r'\\b(?:AS|as)\\s+\\w+\\b', '', masked_sql)\n",
    "    return cleaned_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = \"\"\"SELECT customer_type as eiei, SUM(ntx) AS TOTAL\n",
    "FROM pointx_cust_mly \n",
    "GROUP BY customer_type;\"\"\"\n",
    "mask_query(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_df_copy = sql_df.copy()\n",
    "columns = sql_df_copy.columns\n",
    "\n",
    "for col in columns:\n",
    "    print(col)\n",
    "    new_col = f\"MASK {col}\"\n",
    "    sql_df_copy[new_col] = sql_df_copy[col].apply(mask_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_df_copy.to_excel(\"results/temp_experiments.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os, json, sqlite3, re\n",
    "from dotenv import load_dotenv\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = \"../src/spider/database\"\n",
    "\n",
    "db = dict()\n",
    "\n",
    "if os.path.exists(folder_path) and os.path.isdir(folder_path):\n",
    "    files = os.listdir(folder_path)\n",
    "    for db_id in files:\n",
    "        db_path = os.path.join(folder_path, db_id)\n",
    "        sqlite_db = [os.path.join(db_path, sql) for sql in os.listdir(db_path) if \".sqlite\" in sql]\n",
    "        assert len(sqlite_db) == 1\n",
    "        db[db_id] = sqlite_db[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_schema(sqlite_db_path):\n",
    "    connection = sqlite3.connect(sqlite_db_path)\n",
    "    cursor = connection.cursor()\n",
    "\n",
    "    cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n",
    "    tables = cursor.fetchall()\n",
    "    full_sql = \"\"\n",
    "    for table in tables:\n",
    "        table_name = table[0]\n",
    "        cursor.execute(f\"PRAGMA table_info({table_name});\")\n",
    "        columns = cursor.fetchall()\n",
    "\n",
    "        sql = f\"CREATE TABLE {table_name} (\"\n",
    "        for column in columns:\n",
    "            column_name = column[1]\n",
    "            column_datatype = column[2].lower()\n",
    "            sql += f\"{column_name} {column_datatype}, \"\n",
    "        sql = sql[:-2] + \");\"\n",
    "        full_sql += sql\n",
    "    \n",
    "    cursor.close()\n",
    "    connection.close()\n",
    "    return full_sql\n",
    "\n",
    "def create_prompt(question, schema):\n",
    "    full_prompt = \"\"\n",
    "    full_prompt += f\"{str(schema)}\\n\\n\"\n",
    "    full_prompt += \"-- Using valid SQLite, answer the following questions for the tables provided above.\\n\\n\"\n",
    "    full_prompt += f\"--{question}\\n\\nSELECT\"\n",
    "    return full_prompt\n",
    "\n",
    "def extract_table_and_columns(sql_statements):\n",
    "    # Regular expression pattern to match the table name and column names\n",
    "    pattern = r'CREATE\\s+TABLE\\s+(\\w+)\\s*\\((.+?)\\);?'\n",
    "\n",
    "    table_column_pairs = {}\n",
    "    matches = re.finditer(pattern, sql_statements, re.IGNORECASE)\n",
    "    for match in matches:\n",
    "        table_name = match.group(1)\n",
    "        columns = [col.strip().split()[0] for col in match.group(2).split(',')]\n",
    "        table_column_pairs[table_name] = columns\n",
    "\n",
    "    return table_column_pairs\n",
    "\n",
    "def table_column_of_create_table(query):\n",
    "    lines = query.splitlines()\n",
    "    columns = []\n",
    "    table_names = []\n",
    "\n",
    "    # Look for \"CREATE TABLE\" and start capturing columns\n",
    "    capture = False\n",
    "    for line in lines:\n",
    "        if \"CREATE TABLE\" in line:\n",
    "            capture = True\n",
    "            table_names.append(line.split()[-2])\n",
    "        elif line.strip().endswith(')') or line.strip().endswith(');'):\n",
    "            capture = False\n",
    "        elif capture:\n",
    "            column_name = line.strip().split()[0]\n",
    "            if column_name in [\"CONSTRAINT\", \"PRIMARY\"]: continue\n",
    "            columns.append(column_name)\n",
    "    return table_names, columns\n",
    "\n",
    "def query_db(sql_query, db_name):\n",
    "    try:\n",
    "        conn = sqlite3.connect(f'../src/spider/database/{db_name}/{db_name}.sqlite')\n",
    "        cursor = conn.cursor()\n",
    "    except:\n",
    "        return \"CANNOT CONNECT DATABASE\"\n",
    "    try:\n",
    "        cursor.execute(sql_query)\n",
    "        results = cursor.fetchall()\n",
    "    except:\n",
    "        return \"CANNOT FETCHING DATA\"\n",
    "    conn.close()\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spider_masking_query(sql_query:str, tab_columns:list, condition_value_mask:bool=True) -> str:\n",
    "\n",
    "        tab_columns_lower = [t.lower() for t in tab_columns]\n",
    "        if '*' in sql_query: sql_query = sql_query.replace('*', \"[MASK]\")\n",
    "        query_split = re.split(r'(?<=[() .,;])|(?=[() .,;])', sql_query)\n",
    "        mask_next = False\n",
    "\n",
    "        for i in range(len(query_split)):\n",
    "            token = query_split[i].lower()\n",
    "            # prepare mask condition value\n",
    "            if token.lower() == 'where': mask_next = True\n",
    "            if condition_value_mask and mask_next and (token in {'=', '>', '<', '>=', '<=', '<>', '!='} and i + 1 < len(query_split)):\n",
    "                step_mask_next = 1\n",
    "                # find the condition value\n",
    "                while query_split[i + step_mask_next] == ' ': step_mask_next += 1\n",
    "                query_split[i + step_mask_next] = \"[MASK]\"\n",
    "            \n",
    "            if token in tab_columns_lower:\n",
    "                query_split[i] = \"[MASK]\"\n",
    "\n",
    "        return \"\".join(query_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv('../.env')\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"../models/nsql-350M\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"../models/nsql-350M\")\n",
    "\n",
    "GOOGLE_API_KEY = os.environ.get('GOOGLE_API_KEY')\n",
    "DEEPSEEK_API_KEY = os.environ.get('DEEPSEEK_API_KEY')\n",
    "OPENAI_API_KEY = os.environ.get('OPENAI_API_KEY')\n",
    "\n",
    "system_content_schemaprovide = \"\"\"You are a helpful assistant for generate SQL query from user-specified questions and schema.\n",
    "User provides you with a question.\n",
    "Please return only a sql string query results.\n",
    "Do not return any other format the user has provided to you.\n",
    "This is example of output format which user expect from you\n",
    "query : 'SELECT...'\n",
    "\"\"\"\n",
    "\n",
    "system_content_fillmask = \"\"\"You are a helpful assistant for generate SQL query from user-specified questions and schema. \n",
    "User has some SQL where the [MASK] columns and condition values are syntaxed and User wants you to respond to output that populates the [MASK] column of the SQL input followed by the question and schema description (name - description).\n",
    "If you don't know which column to fill in Do not include columns that you have created yourself. And only columns defined from the schema must be used. \n",
    "Do not use columns from other tables or schema. must also be used from the same table defined in the input.\n",
    "If you must enter conditional values Please decide the format or value based on the sample values of that column.\n",
    "If that column has many too long category value please decide base on column description.\n",
    "please return only the answer of sql string query result!!! ('SELECT...')\n",
    "\"\"\"\n",
    "\n",
    "zero_shot_prompt = \"\"\"For example:\n",
    "table :     cat - this table contain cat information \n",
    "columns :    id - number for identify cat\n",
    "            name - name of cat \n",
    "            age - age of cat \n",
    "            birth_date - pet birthday in format 'YYYY-MM-DD'\n",
    "            gender - gender of cat (male, female)\n",
    "\n",
    "question : 'Show me number of cat for each gender which born before March 23, 2011.'\n",
    "query : 'SELECT gender, COUNT(*) FROM cat WHERE birth_date < '2011-03-23' GROUP BY gender;'\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "zero_shot_prompt_mask = \"\"\"For example:\n",
    "table :     cat - this table contain cat information \n",
    "columns :    id - number for identify cat\n",
    "            name - name of cat \n",
    "            age - age of cat \n",
    "            birth_date - pet birthday in format 'YYYY-MM-DD'\n",
    "            gender - gender of cat (male, female)\n",
    "\n",
    "question : Show me number of cat for each gender which born before March 23, 2011.\n",
    "input : SELECT [MASK], COUNT([MASK]) FROM cat WHERE [MASK] < [MASK] GROUP BY [MASK] ;\n",
    "query : SELECT gender, COUNT(*) FROM cat WHERE birth_date < '2011-03-23' GROUP BY gender;\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_nsql_sql(prompt):\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "        generated_ids = model.generate(input_ids, max_length=1000)\n",
    "        sql = tokenizer.decode(generated_ids[0], skip_special_tokens=True).split('\\n')[-1]\n",
    "    return sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../src/src_dev/spider/mockup_schema_description.json\") as f:\n",
    "    spider_description = json.load(f)\n",
    "    exists_tables = [tab['table'] for tab in spider_description]\n",
    "\n",
    "with open(\"../src/src_dev/spider/table_database_map.json\") as f:\n",
    "    table_map_db = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spider_df = pd.read_csv('../src/NSText2SQL/train_spider.csv')\n",
    "spider_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_data = {\n",
    "    \"Question\" : [],\n",
    "    \"Desc DeepSeek\" : [],\n",
    "    \"Desc GPT3.5\" : [],\n",
    "    \"Desc GPT4\" : [],\n",
    "    \"Desc Gemini\" : [],\n",
    "    \"ChatQ NSQL\" : [],\n",
    "    \"ChatQ DeepSeek\" : [],\n",
    "    \"ChatQ GPT3.5\" : [],\n",
    "    \"ChatQ GPT4\" : [],\n",
    "    \"ChatQ Gemini\" : []\n",
    "}\n",
    "\n",
    "temp_result_file = \"results/temp_spider_experiments.xlsx\"\n",
    "\n",
    "if os.path.exists(temp_result_file):\n",
    "    print(\"File exist\")\n",
    "    _df = pd.read_excel(temp_result_file)\n",
    "    i = _df.shape[0]\n",
    "    for key in predict_data:\n",
    "        if key in _df.columns:\n",
    "            predict_data[key] = _df[key].tolist()\n",
    "\n",
    "for i in range(spider_df.shape[0]):\n",
    "    tabs, cols = table_column_of_create_table(spider_df.iloc[i,1])\n",
    "    if len(set(tabs).intersection(set(exists_tables))) == len(tabs) :\n",
    "\n",
    "        question = spider_df.iloc[i,0]\n",
    "        if question in predict_data['Question']: continue\n",
    "        nsql_prompt = create_prompt(question, spider_df.iloc[i,1])\n",
    "        actual_sql = spider_df.iloc[i,2]\n",
    "        \n",
    "        schema_desc_prompt = \"\"\n",
    "        for use_tabl in tabs:\n",
    "            used_schema = spider_description[exists_tables.index(use_tabl)]\n",
    "            schema_desc_prompt += f\"{used_schema['table']} - {used_schema['description']}\\n\"\n",
    "            for col_name, col_desc in used_schema['columns'].items():\n",
    "                schema_desc_prompt += f\"\\t{col_name} - {col_desc}\\n\"\n",
    "\n",
    "        schema_desc_prompt += f\"question: {question}\\n\"\n",
    "\n",
    "        schema_provide_prompt = zero_shot_prompt + schema_desc_prompt + \"query: \"\n",
    "\n",
    "        schema_gemini_results = LLM_gensql(schema_provide_prompt, system_content_schemaprovide, 'gemini-pro')\n",
    "        schema_gpt3_5_results = LLM_gensql(schema_provide_prompt, system_content_schemaprovide, 'gpt-3.5-turbo')\n",
    "        schema_gpt4_results = LLM_gensql(schema_provide_prompt, system_content_schemaprovide, 'gpt-4-0125-preview')\n",
    "        schema_deepseek_result = LLM_gensql(schema_provide_prompt, system_content_schemaprovide, 'deepseek-coder')\n",
    "        \n",
    "        nsql_result = generate_nsql_sql(nsql_prompt)\n",
    "        masked_nsql_result = spider_masking_query(nsql_result, cols)\n",
    "        \n",
    "        chatq_prompt = zero_shot_prompt_mask + schema_desc_prompt + f\"input : {masked_nsql_result}\\nquery: \"\n",
    "\n",
    "        chatq_gemini_results = LLM_gensql(chatq_prompt, system_content_fillmask, 'gemini-pro')\n",
    "        chatq_gpt3_5_results = LLM_gensql(chatq_prompt, system_content_fillmask, 'gpt-3.5-turbo')\n",
    "        chatq_gpt4_results = LLM_gensql(chatq_prompt, system_content_fillmask, 'gpt-4-0125-preview')\n",
    "        chatq_deepseek_result = LLM_gensql(chatq_prompt, system_content_fillmask, 'deepseek-coder')\n",
    "\n",
    "        print(\"Generating SQL...\")\n",
    "        schema_gemini_results, schema_deepseek_result, schema_gpt3_5_results, schema_gpt4_results = await asyncio.gather(schema_gemini_results, schema_deepseek_result, schema_gpt3_5_results, schema_gpt4_results)\n",
    "        chatq_gemini_results, chatq_gpt3_5_results, chatq_gpt4_results, chatq_deepseek_result = await asyncio.gather(chatq_gemini_results, chatq_gpt3_5_results, chatq_gpt4_results, chatq_deepseek_result)\n",
    "\n",
    "        print(question)\n",
    "        print(schema_gemini_results, schema_deepseek_result, schema_gpt3_5_results, schema_gpt4_results, chatq_gemini_results, chatq_gpt3_5_results, chatq_gpt4_results, chatq_deepseek_result, sep='\\n')\n",
    "\n",
    "        predict_data['Question'].append(question)\n",
    "        predict_data['Desc DeepSeek'].append(schema_deepseek_result)\n",
    "        predict_data['Desc GPT3.5'].append(schema_gpt3_5_results)\n",
    "        predict_data['Desc GPT4'].append(schema_gpt4_results)\n",
    "        predict_data['Desc Gemini'].append(schema_gemini_results)\n",
    "        predict_data['ChatQ NSQL'].append(nsql_result)\n",
    "        predict_data['ChatQ DeepSeek'].append(chatq_deepseek_result)\n",
    "        predict_data['ChatQ GPT3.5'].append(chatq_gpt3_5_results)\n",
    "        predict_data['ChatQ GPT4'].append(chatq_gpt4_results)\n",
    "        predict_data['ChatQ Gemini'].append(chatq_gemini_results)\n",
    "\n",
    "        save_df = pd.DataFrame(predict_data)\n",
    "        save_df.to_excel(temp_result_file, index=False)\n",
    "        print(\"SAVE TEMP COMPLETE\", i)\n",
    "\n",
    "save_df = pd.DataFrame(predict_data)\n",
    "save_df.to_excel(\"results/model_spider_experiments.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(chatq_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "RateLimitError",
     "evalue": "Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRateLimitError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m load_dotenv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../.env\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      6\u001b[0m client \u001b[38;5;241m=\u001b[39m OpenAI(api_key\u001b[38;5;241m=\u001b[39mos\u001b[38;5;241m.\u001b[39menviron\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOPENAI_API_KEY\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m----> 8\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompletions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m  \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgpt-3.5-turbo\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m  \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrole\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msystem\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcontent\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mYou are a helpful assistant.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrole\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcontent\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mWho won the world series in 2020?\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrole\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43massistant\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcontent\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mThe Los Angeles Dodgers won the World Series in 2020.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrole\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcontent\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mWhere was it played?\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m  \u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m response\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/openai/_utils/_utils.py:271\u001b[0m, in \u001b[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    269\u001b[0m             msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    270\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[0;32m--> 271\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/openai/resources/chat/completions.py:659\u001b[0m, in \u001b[0;36mCompletions.create\u001b[0;34m(self, messages, model, frequency_penalty, function_call, functions, logit_bias, logprobs, max_tokens, n, presence_penalty, response_format, seed, stop, stream, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    608\u001b[0m \u001b[38;5;129m@required_args\u001b[39m([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m], [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    609\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate\u001b[39m(\n\u001b[1;32m    610\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    657\u001b[0m     timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m httpx\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m NotGiven \u001b[38;5;241m=\u001b[39m NOT_GIVEN,\n\u001b[1;32m    658\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatCompletion \u001b[38;5;241m|\u001b[39m Stream[ChatCompletionChunk]:\n\u001b[0;32m--> 659\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    660\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/chat/completions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    661\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    662\u001b[0m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m    663\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmessages\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    664\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    665\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfrequency_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    666\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunction_call\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    667\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunctions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    668\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogit_bias\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    669\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    670\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    671\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mn\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    672\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpresence_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    673\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mresponse_format\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    674\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mseed\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    675\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstop\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    676\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    677\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtemperature\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    678\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtool_choice\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    679\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtools\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    680\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_logprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    681\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_p\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    682\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    683\u001b[0m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    684\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCompletionCreateParams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    685\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    686\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    687\u001b[0m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[1;32m    688\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    689\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mChatCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    690\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    691\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatCompletionChunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    692\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/openai/_base_client.py:1200\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(\n\u001b[1;32m   1187\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1188\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1195\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1196\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[1;32m   1197\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[1;32m   1198\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[1;32m   1199\u001b[0m     )\n\u001b[0;32m-> 1200\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/openai/_base_client.py:889\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    880\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrequest\u001b[39m(\n\u001b[1;32m    881\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    882\u001b[0m     cast_to: Type[ResponseT],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    887\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    888\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[0;32m--> 889\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    890\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    891\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    892\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    893\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    894\u001b[0m \u001b[43m        \u001b[49m\u001b[43mremaining_retries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremaining_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    895\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/openai/_base_client.py:965\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    963\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m retries \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_retry(err\u001b[38;5;241m.\u001b[39mresponse):\n\u001b[1;32m    964\u001b[0m     err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mclose()\n\u001b[0;32m--> 965\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_retry_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    966\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    967\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    968\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    969\u001b[0m \u001b[43m        \u001b[49m\u001b[43merr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    970\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    971\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    972\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    974\u001b[0m \u001b[38;5;66;03m# If the response is streamed then we need to explicitly read the response\u001b[39;00m\n\u001b[1;32m    975\u001b[0m \u001b[38;5;66;03m# to completion before attempting to access the response text.\u001b[39;00m\n\u001b[1;32m    976\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mis_closed:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/openai/_base_client.py:1013\u001b[0m, in \u001b[0;36mSyncAPIClient._retry_request\u001b[0;34m(self, options, cast_to, remaining_retries, response_headers, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1009\u001b[0m \u001b[38;5;66;03m# In a synchronous context we are blocking the entire thread. Up to the library user to run the client in a\u001b[39;00m\n\u001b[1;32m   1010\u001b[0m \u001b[38;5;66;03m# different thread if necessary.\u001b[39;00m\n\u001b[1;32m   1011\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(timeout)\n\u001b[0;32m-> 1013\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1014\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1015\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1016\u001b[0m \u001b[43m    \u001b[49m\u001b[43mremaining_retries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremaining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1017\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1018\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1019\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/openai/_base_client.py:965\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    963\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m retries \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_retry(err\u001b[38;5;241m.\u001b[39mresponse):\n\u001b[1;32m    964\u001b[0m     err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mclose()\n\u001b[0;32m--> 965\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_retry_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    966\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    967\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    968\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    969\u001b[0m \u001b[43m        \u001b[49m\u001b[43merr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    970\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    971\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    972\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    974\u001b[0m \u001b[38;5;66;03m# If the response is streamed then we need to explicitly read the response\u001b[39;00m\n\u001b[1;32m    975\u001b[0m \u001b[38;5;66;03m# to completion before attempting to access the response text.\u001b[39;00m\n\u001b[1;32m    976\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mis_closed:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/openai/_base_client.py:1013\u001b[0m, in \u001b[0;36mSyncAPIClient._retry_request\u001b[0;34m(self, options, cast_to, remaining_retries, response_headers, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1009\u001b[0m \u001b[38;5;66;03m# In a synchronous context we are blocking the entire thread. Up to the library user to run the client in a\u001b[39;00m\n\u001b[1;32m   1010\u001b[0m \u001b[38;5;66;03m# different thread if necessary.\u001b[39;00m\n\u001b[1;32m   1011\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(timeout)\n\u001b[0;32m-> 1013\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1014\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1015\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1016\u001b[0m \u001b[43m    \u001b[49m\u001b[43mremaining_retries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremaining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1017\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1018\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1019\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/openai/_base_client.py:980\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    977\u001b[0m         err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m    979\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRe-raising status error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 980\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    982\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_response(\n\u001b[1;32m    983\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[1;32m    984\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    987\u001b[0m     stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[1;32m    988\u001b[0m )\n",
      "\u001b[0;31mRateLimitError\u001b[0m: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv('../.env')\n",
    "client = OpenAI(api_key=os.environ.get('OPENAI_API_KEY'))\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "  model=\"gpt-3.5-turbo\",\n",
    "  messages=[\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Who won the world series in 2020?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"The Los Angeles Dodgers won the World Series in 2020.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Where was it played?\"}\n",
    "  ]\n",
    ")\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
